{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fake_news_eda.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LSBxv9dopMZ"
      },
      "source": [
        "# Data validation between the data set in BigQuery and GraphDB\n",
        "\n",
        "The objectives are to:\n",
        "1. Ensure complete upload of data to BigQuery and GraphDB.\n",
        "2. Check for data qualtiy issues.\n",
        "3. Gain insights on the data.\n",
        "4. Engineer features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VzbNI9SGNVL"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSxr4qJ-oaTA"
      },
      "source": [
        "# Mount to Google Drive to save results\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/MSc/2020-21/Research\\ Project/Colab/\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdwNSXCTpyeG"
      },
      "source": [
        "# Connect to GCP Bucket\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvgk0LpPrHwC"
      },
      "source": [
        "# Set GCP project ID and region to Europe West 2 - London\n",
        "PROJECT = 'fake-news-bs-detector'\n",
        "!gcloud config set project $PROJECT\n",
        "REGION = 'europe-west2'\n",
        "CLUSTER = '{}-cluster'.format(PROJECT)\n",
        "!gcloud config set compute/region $REGION\n",
        "!gcloud config set dataproc/region $REGION\n",
        "\n",
        "!gcloud config list # show some information"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRD3VGI0Cdik"
      },
      "source": [
        "## Check the number of files in successive GCP cloud storage buckets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEkjkw1A9oU_"
      },
      "source": [
        "# Count the number of cleaned JSON files from the end of stage 1 in the pipeline\n",
        "!gsutil ls -l gs://fake_news_cleaned_json/*.json | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPVFxHpO-J6K"
      },
      "source": [
        "# Count the number of parsed JSON and TTL files into triples at the end of stage 2 in the pipeline\n",
        "!gsutil ls -l gs://fake_news_ttl_json/*.ttl | wc -l\n",
        "!gsutil ls -l gs://fake_news_ttl_json/*.json | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ-7JVyyCHXG"
      },
      "source": [
        "The variance between the 59,733 cleaned files to 27,590 turtle documents would suggest this is due to the raw data containing duplicating records for the same news web page, when the turtles are indexed by the hash value of the URLs and therefore would overwrite leading to small number of samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmZL7UeNrWoQ"
      },
      "source": [
        "# Based on https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries#bigquery_simple_app_client-python\n",
        "from google.cloud import bigquery\n",
        "client = bigquery.Client(PROJECT)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGkFRU4H6i-3"
      },
      "source": [
        "## Profile the data in its original form held in BigQuery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvjhBFsku0X_"
      },
      "source": [
        "# BIgQuery data row count\n",
        "query_job = client.query(\n",
        "    \"\"\"\n",
        "    SELECT COUNT(*) AS POPULATION_COUNT\n",
        "    FROM `detect-fake-news-313201.fake_news_sql.src_fake_news`\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "res_df = query_job.result().to_dataframe()  # Waits for job to complete.\n",
        "\n",
        "res_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH5MXB-7DzrY"
      },
      "source": [
        "Therefore deviation by one record compared to the number of files in `gs://fake_news_ttl_json`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFhbDKxLAO5F"
      },
      "source": [
        "# BIgQuery data row count\n",
        "query_job = client.query(\n",
        "    \"\"\"\n",
        "    WITH URL_LIST AS (\n",
        "      SELECT \n",
        "      URL\n",
        "      , COUNT(*) AS URL_COUNT\n",
        "      FROM `detect-fake-news-313201.fake_news_sql.src_fake_news`\n",
        "      GROUP BY URL\n",
        "    )\n",
        "    SELECT * FROM URL_LIST WHERE URL_COUNT > 1\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "res_df = query_job.result().to_dataframe()  # Waits for job to complete.\n",
        "\n",
        "res_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15-MVHq_DqNL"
      },
      "source": [
        "Therefore no samples found to have duplicating URL in the BigQuery table, and all articles have unique URLs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRQWKshbrteE"
      },
      "source": [
        "# BIgQuery data preview\n",
        "query_job = client.query(\n",
        "    \"\"\"\n",
        "    SELECT *\n",
        "    FROM `detect-fake-news-313201.fake_news_sql.src_fake_news`\n",
        "    LIMIT 10\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "res_df = query_job.result().to_dataframe()  # Waits for job to complete.\n",
        "\n",
        "res_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2gbwrmz82nf"
      },
      "source": [
        "# BigQuery count by domain\n",
        "query_job = client.query(\n",
        "    \"\"\"\n",
        "    SELECT\n",
        "    DOMAIN_HASH\n",
        "    , LABEL\n",
        "    , COUNT(*) AS ARTICLES_COUNT\n",
        "    FROM `detect-fake-news-313201.fake_news_sql.src_fake_news`\n",
        "    GROUP BY DOMAIN_HASH, LABEL\n",
        "    ORDER BY ARTICLES_COUNT DESC\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "res_df = query_job.result().to_dataframe()\n",
        "\n",
        "# Tally the domain hash to ensure each domain only has one label\n",
        "domain_tally_ls = []\n",
        "duplicate_domain_tally_ls = []\n",
        "for i, row in res_df.iterrows():\n",
        "  if row['DOMAIN_HASH'] in domain_tally_ls:\n",
        "    duplicate_domain_tally_ls += [True]\n",
        "  else:\n",
        "    duplicate_domain_tally_ls += [False]\n",
        "  \n",
        "  # Add domain hash to the list of domains already reviwed\n",
        "  domain_tally_ls += [row['DOMAIN_HASH']]\n",
        "\n",
        "res_df['DOMAIN_HAS_MULTIPLE_LABEL'] = duplicate_domain_tally_ls\n",
        "\n",
        "res_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12DM3kHjFZvX"
      },
      "source": [
        "# BIgQuery count by label\n",
        "query_job = client.query(\n",
        "    \"\"\"\n",
        "    SELECT\n",
        "    LABEL\n",
        "    , COUNT(*) AS LABEL_COUNT\n",
        "    FROM `detect-fake-news-313201.fake_news_sql.src_fake_news`\n",
        "    GROUP BY LABEL\n",
        "    ORDER BY LABEL_COUNT DESC\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "res_df = query_job.result().to_dataframe()\n",
        "\n",
        "print('Total: {}'.format(res_df['LABEL_COUNT'].sum()))\n",
        "\n",
        "res_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKoPGDQ_aoLh"
      },
      "source": [
        "# BIgQuery list of URLs\n",
        "query_job = client.query(\n",
        "    \"\"\"\n",
        "    SELECT DISTINCT\n",
        "    URL_HASH\n",
        "    FROM `detect-fake-news-313201.fake_news_sql.src_fake_news`\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "res_df = query_job.result().to_dataframe()\n",
        "\n",
        "print('Total: {}'.format(res_df.shape[0]))\n",
        "\n",
        "res_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEvUkSkoDSwj"
      },
      "source": [
        "Noted that there were no classification for 990 samples, and further 354 with unknown classifications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNUkn27VDE_q"
      },
      "source": [
        "## Profile the data in GraphDB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNWkJujiDIak"
      },
      "source": [
        "# Install the wrapper package\n",
        "# Source: https://github.com/RDFLib/sparqlwrapper\n",
        "!pip install sparqlwrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2dvaUQ2Yiu8"
      },
      "source": [
        "# Code based on: https://sparqlwrapper.readthedocs.io/en/latest/main.html\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "queryString = \"\"\"\n",
        "PREFIX aa: <http://www.city.ac.uk/ds/inm363/aaron_altrock#>\n",
        "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "\n",
        "select (count(?url_hash) as ?url_count) where {\n",
        "  ?url_hash rdf:type aa:urlHash .\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "sparql = SPARQLWrapper(\"http://35.246.120.165:7200/repositories/src_fake_news\")\n",
        "sparql.setReturnFormat(JSON)\n",
        "sparql.setQuery(queryString)\n",
        "\n",
        "try :\n",
        "   res_dct = sparql.query().convert()\n",
        "   print('OK')\n",
        "\n",
        "except Exception as e:\n",
        "   print('ERROR: {}'.format(e))\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmi38ivdb-lc"
      },
      "source": [
        "# No. of URL hash in GraphDB\n",
        "res_dct.get('results').get('bindings')[0].get('url_count').get('value')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swozazo7eyw7"
      },
      "source": [
        "Therefore noted that the number of news articles as URL hashes were completely uploaded when compared to BigQury count given both have the same number of articles `27598`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnE3vThpioL6"
      },
      "source": [
        "res_dct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCDNtvD5hKXy"
      },
      "source": [
        "### No. of URL hashes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GChKHQm4hNtN"
      },
      "source": [
        "# Code based on: https://sparqlwrapper.readthedocs.io/en/latest/main.html\n",
        "\n",
        "queryString = \"\"\"\n",
        "PREFIX aa: <http://www.city.ac.uk/ds/inm363/aaron_altrock#>\n",
        "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "\n",
        "select ?domain_hash ?label (count(?url_hash) as ?url_count) where {\n",
        "  ?domain_hash rdf:type aa:domainHash .\n",
        "  ?url_hash rdf:type aa:urlHash .\n",
        "  ?label rdf:type aa:newsLabel .\n",
        "  ?url_hash aa:has_domain_hash ?domain_hash .\n",
        "  ?url_hash aa:has_news_label ?label .\n",
        "}\n",
        "GROUP BY ?domain_hash ?label\n",
        "ORDER BY ?url_count\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "sparql = SPARQLWrapper(\"http://35.246.120.165:7200/repositories/src_fake_news\")\n",
        "sparql.setReturnFormat(JSON)\n",
        "sparql.setQuery(queryString)\n",
        "\n",
        "try :\n",
        "   res_dct = sparql.query().convert()\n",
        "   print('OK')\n",
        "\n",
        "except Exception as e:\n",
        "   print('ERROR: {}'.format(e))\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VTqPHeJibNr"
      },
      "source": [
        "res_dct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Pry0CwAifS3"
      },
      "source": [
        "import re\n",
        "res_ls = res_dct.get('results').get('bindings')\n",
        "\n",
        "# Helper func to transform SPARQLWrapper query result output (dict) to Pandas dataframe\n",
        "def parse_to_dataframe(res_ls):\n",
        "\n",
        "  # If query result has content then parse to data frame else return None\n",
        "  if len(res_ls) > 0:\n",
        "    # Get column names\n",
        "    col_nm_ls = list(res_ls[0].keys())\n",
        "\n",
        "    parsed_res_ls = []\n",
        "\n",
        "    for res_dct in res_ls:\n",
        "      __res_ls = []\n",
        "      for k, v in res_dct.items():\n",
        "        __res_ls += [v.get('value')]\n",
        "\n",
        "      # __res_ls = [res_dct]\n",
        "      parsed_res_ls += [__res_ls]\n",
        "    \n",
        "    res_df = pd.DataFrame.from_dict(parsed_res_ls)\n",
        "    res_df.columns = col_nm_ls\n",
        "    res_df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    return res_df\n",
        "\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "# Parse dict output from SPARQL to Pandas data frame\n",
        "domain_url_count_df = parse_to_dataframe(res_ls)\n",
        "\n",
        "# Remove name space prefix\n",
        "domain_url_count_df['domain_hash'] = domain_url_count_df['domain_hash'].map(lambda str: str[str.find('#') + 1:])\n",
        "domain_url_count_df['label'] = domain_url_count_df['label'].map(lambda str: str[str.find('#') + 1:])\n",
        "\n",
        "# Convert url_count to integer\n",
        "domain_url_count_df['url_count'] = domain_url_count_df['url_count'].map(int)\n",
        "\n",
        "# Count percentage\n",
        "domain_url_count_df['url_count_pct'] = domain_url_count_df['url_count'].map(lambda val: val / domain_url_count_df['url_count'].sum() * 100)\n",
        "\n",
        "domain_url_count_df.sort_values(by=['url_count'], ascending=False, inplace=True)\n",
        "\n",
        "domain_url_count_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQTQxEWrrs2b"
      },
      "source": [
        "# Summary of the classification by count of articles\n",
        "label_count_df = domain_url_count_df[['label', 'url_count']].groupby('label').sum().sort_values(by='url_count', ascending=False).reset_index(drop=False)\n",
        "total_rec_count = sum(label_count_df['url_count'].to_list())\n",
        "print('Total number of articles: {}'.format(total_rec_count))\n",
        "label_count_df['url_count_pct'] = label_count_df['url_count'].map(lambda val: val / total_rec_count * 100 if val is not None else 0)\n",
        "label_count_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntu96Yvjxl6Y"
      },
      "source": [
        "Per the https://github.com/several27/FakeNewsCorpus details on each news article classification noted no such classifications as `unknown` nor a classification of `None`.  These may be added by the author of the data set who performed scrapping to backfill classifications that could not be found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtGbL_bgtbdo"
      },
      "source": [
        "# Save summary to CSV\n",
        "label_count_df.to_csv(r'label_count_df.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76IyQ4LZlIeo"
      },
      "source": [
        "## No. of domains by classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhPOxr97lOOn"
      },
      "source": [
        "print('No. of domains in total: {}'.format(domain_url_count_df['domain_hash'].shape[0]))\n",
        "domain_count_df = domain_url_count_df[['domain_hash', 'label']].groupby(by='label').count().sort_values(by='domain_hash', ascending=False).reset_index()\n",
        "domain_count_df.rename(columns={'domain_hash': 'domain_count'}, inplace=True)\n",
        "domain_count_df['domain_count_pct'] = domain_count_df['domain_count'].map(lambda val: val / domain_count_df['domain_count'].sum() * 100)\n",
        "domain_count_df.to_csv('domain_count_df.csv', index=False)\n",
        "domain_count_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BqB4ZTftbzW"
      },
      "source": [
        "## Repeated publication of the same news articles by URL, domain and classifications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBwPs2lhtgkQ"
      },
      "source": [
        "# Code based on: https://sparqlwrapper.readthedocs.io/en/latest/main.html\n",
        "\n",
        "queryString = \"\"\"\n",
        "PREFIX aa: <http://www.city.ac.uk/ds/inm363/aaron_altrock#>\n",
        "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "\n",
        "select ?domain_hash ?url_hash ?body_hash ?label where {\n",
        "  ?domain_hash rdf:type aa:domainHash .\n",
        "  ?url_hash rdf:type aa:urlHash .\n",
        "  ?body_hash rdf:type aa:bodyHash .\n",
        "  ?label rdf:type aa:newsLabel .\n",
        "  ?url_hash aa:has_domain_hash ?domain_hash .\n",
        "  ?url_hash aa:has_news_label ?label .\n",
        "  ?url_hash aa:has_body_hash ?body_hash .\n",
        "}\n",
        "ORDER BY ?url_count\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "sparql = SPARQLWrapper(\"http://35.246.120.165:7200/repositories/src_fake_news\")\n",
        "sparql.setReturnFormat(JSON)\n",
        "sparql.setQuery(queryString)\n",
        "\n",
        "try :\n",
        "   res_dct = sparql.query().convert()\n",
        "   print('OK')\n",
        "\n",
        "except Exception as e:\n",
        "   print('ERROR: {}'.format(e))\n",
        "\n",
        "\n",
        "# Parse dict output from SPARQL to Pandas data frame\n",
        "res_ls = res_dct.get('results').get('bindings')\n",
        "domain_url_body_df = parse_to_dataframe(res_ls)\n",
        "\n",
        "# Remove name space prefix\n",
        "domain_url_body_df['domain_hash'] = domain_url_body_df['domain_hash'].map(lambda str: str[str.find('#') + 1:])\n",
        "domain_url_body_df['url_hash'] = domain_url_body_df['url_hash'].map(lambda str: str[str.find('#') + 1:])\n",
        "domain_url_body_df['body_hash'] = domain_url_body_df['body_hash'].map(lambda str: str[str.find('#') + 1:])\n",
        "domain_url_body_df['label'] = domain_url_body_df['label'].map(lambda str: str[str.find('#') + 1:])\n",
        "\n",
        "domain_url_body_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoCLHZ9_upoW"
      },
      "source": [
        "# Summarise the number of URLs with the same text body in news articles\n",
        "reuse_content_url_df = domain_url_body_df[['body_hash', 'label', 'url_hash']].drop_duplicates().groupby(['body_hash', 'label']).count().reset_index().sort_values(by='url_hash', ascending=False).head(50)\n",
        "reuse_content_url_df.to_csv('reuse_content_url_df.csv', index=False)\n",
        "print('No. of distinct text corpora: {}'.format(reuse_content_df.shape[0]))\n",
        "reuse_content_url_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS-7U0bowTjy"
      },
      "source": [
        "print('No. of text bodies re-used: {}'.format(reuse_content_url_df[reuse_content_url_df['url_hash'] >= 2].shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1zzXfp_HUOQ"
      },
      "source": [
        "Therefore noted all articles were re-used up to 410 times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RskiCEa2HSKm"
      },
      "source": [
        "# Summarise the number of domains with the same text body in news articles\n",
        "reuse_content_domain_df = domain_url_body_df[['body_hash', 'label', 'domain_hash']].drop_duplicates().groupby(['body_hash', 'domain_hash']).count().reset_index().sort_values(by='domain_hash', ascending=False).head(50)\n",
        "reuse_content_domain_df.to_csv('reuse_content_domain_df.csv', index=False)\n",
        "print('No. of distinct text corpora: {}'.format(reuse_content_df.shape[0]))\n",
        "reuse_content_domain_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voJqIflrIljg"
      },
      "source": [
        "# Find text corpora referenced by more than one domains\n",
        "reuse_content_domain_df[reuse_content_domain_df['label'] > 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrAFsrBtJCff"
      },
      "source": [
        "Noted therefore whilst there were repeated republication of the same news text corpora as the bodies they were confined to within the same web domains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S42iYC0aJIx9"
      },
      "source": [
        "# BigQuery count by domain\n",
        "query_job = client.query(\n",
        "    \"\"\"\n",
        "    SELECT\n",
        "    *\n",
        "    FROM `detect-fake-news-313201.fake_news_sql.src_fake_news`\n",
        "    WHERE BODY_HASH = 'body_7a03935701b11bf99ae50445a0e67793'\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "res_df = query_job.result().to_dataframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmJQ9cqdJ82B"
      },
      "source": [
        "res_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLCxxnvSJ_OM"
      },
      "source": [
        "res_df['body'].iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQTIRDhVKHh8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}