{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prepare_raw_data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LSBxv9dopMZ"
      },
      "source": [
        "# Phase Two - Create a set of raw data of different sample sizes for benchmarking\n",
        "\n",
        "The objectives are to:\n",
        "1. Extract the columns relevant for the project.\n",
        "2. Parse into Parquet format into the GCP Storage where it is ingested into the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VzbNI9SGNVL"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSxr4qJ-oaTA"
      },
      "source": [
        "# Mount to Google Drive to save results\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/MSc/2020-21/Research\\ Project/Colab/\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdwNSXCTpyeG"
      },
      "source": [
        "# Connect to GCP Bucket\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvgk0LpPrHwC"
      },
      "source": [
        "# Set GCP project ID and region to Europe West 2 - London\n",
        "PROJECT = 'fake-news-bs-detector'\n",
        "!gcloud config set project $PROJECT\n",
        "REGION = 'europe-west2'\n",
        "CLUSTER = '{}-cluster'.format(PROJECT)\n",
        "!gcloud config set compute/region $REGION\n",
        "!gcloud config set dataproc/region $REGION\n",
        "\n",
        "!gcloud config list # show some information"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZPEqzHiW7wS"
      },
      "source": [
        "##Â Read in from Google Drive the original file(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beMwFa8nW6wr"
      },
      "source": [
        "# Read in from the Google Drive at mount point\n",
        "src_file_nm = 'risdal.csv'\n",
        "# parquet_file_nm = 'risdal.parquet'\n",
        "src_df = pd.read_csv(src_file_nm)\n",
        "print('Dimension of {}: {} x {}'.format(src_file_nm, src_df.shape[0], src_df.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8x-b24NW65u"
      },
      "source": [
        "src_df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AqM2FiYanlZ"
      },
      "source": [
        "## Simple data profiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QDsOx8RZODn"
      },
      "source": [
        "# Profile of the data set\n",
        "src_df.describe(include='all').T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np7eIb-NoXvK"
      },
      "source": [
        "## Randomly sample the raw data to create new data sets for 1,250, 2,500, 5,000, 7,500, 10,000 observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw5VF_tFdIeA"
      },
      "source": [
        "## Transform data\n",
        "\n",
        "Simple transformation to make it easier to ingest by GCP DataFlow pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpBSVv8yeThM"
      },
      "source": [
        "# Remove any articles with no text\n",
        "parsed_df = src_df[[True if pd.notnull(txt) else False for txt in src_df['text']]].copy()\n",
        "print('Before: Dimension of {}: {} x {}'.format('src_df', src_df.shape[0], src_df.shape[1]))\n",
        "print('After: Dimension of {}: {} x {}'.format('parsed_df', parsed_df.shape[0], parsed_df.shape[1]))\n",
        "parsed_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e3eWtYHovGZ"
      },
      "source": [
        "# Add file name column required for ingestion\n",
        "parsed_df['file_name'] = [parquet_file_nm] * parsed_df.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5FQsXlnhhYr"
      },
      "source": [
        "## Prepare Config YAML file\n",
        "Create a config file for the data set to give instruction to the data ingestion pipeline on GCP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Odq59aK3hpV8"
      },
      "source": [
        "import yaml\n",
        "\n",
        "# Import the template YAML file\n",
        "with open('./template.yml', 'r') as f_read:\n",
        "  try:\n",
        "    template_config_dct = yaml.safe_load(f_read)\n",
        "  except Exception as e:\n",
        "    print('Error: {}'.format(e))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdgzR_5OkKfG"
      },
      "source": [
        "# Preview the template YAML\n",
        "template_config_dct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNA0NYxT_6Pl"
      },
      "source": [
        "# Create a new dict based on the template to configure for the data set\n",
        "parquet_config_dct = template_config_dct.copy()\n",
        "\n",
        "# Mandatory columns\n",
        "parquet_config_dct.update({'mandatory_columns': \n",
        "                            {\n",
        "                                'body': {'column_name': 'text'},\n",
        "                                'label': {'column_name': 'type'},\n",
        "                                'title': {'column_name': 'title'},\n",
        "                                'url': {'column_name': 'site_url'},\n",
        "                                'file_name': {'column_name': 'file_name'}\n",
        "                             }\n",
        "                           })\n",
        "\n",
        "# Source of the data\n",
        "parquet_config_dct.update({'source': 'https://www.kaggle.com/mrisdal/fake-news'})\n",
        "\n",
        "# Supplementary columns\n",
        "parquet_config_dct.update({'supplementary_columns': \n",
        "                            {\n",
        "                                'author': {'column_name': 'author'},\n",
        "                                'classification_date': {'column_name': 'crawled'},\n",
        "                                'detailed_news_label': {'column_name': ''},\n",
        "                                'language': {'column_name': 'language'},\n",
        "                                'publication_date': {'column_name': 'published'},\n",
        "                                'country_of_origin': {'column_name': 'country'}\n",
        "                             }\n",
        "                           })\n",
        "\n",
        "# Custom columns\n",
        "parquet_config_dct.update({'custom_columns': ['likes', 'comments', 'shares', 'replies_count', 'participants_count', 'spam_score', 'main_img_url']})\n",
        "\n",
        "parquet_config_dct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deZCaSFxuhy6"
      },
      "source": [
        "# Sample according to the required sizes for the benchmarking\n",
        "sample_size_ls = [1250, 2500, 5000, 7500, 10000]\n",
        "\n",
        "for sample_sz in sample_size_ls:\n",
        "  # Set destinations\n",
        "  __df = parsed_df.copy()\n",
        "  __df = __df.sample(sample_sz).copy()\n",
        "  print('Dimension: {} x {}'.format(__df.shape[0], __df.shape[1]))\n",
        "  dest_gcp_bucket_nm = 'gs://src_fake_news_bs/to_add'\n",
        "  parquet_file_nm = 'risdal' + '_' + str(sample_sz) + '.parquet'\n",
        "  dest_yaml_file_nm = parquet_file_nm + '_' + str(sample_sz) + '.yml'  # YAML file name must match the same as the corresponding parquet file name, including the file type suffix\n",
        "  dest_yaml_path = dest_gcp_bucket_nm + '/' + dest_yaml_file_nm\n",
        "  dest_parquet_path = dest_gcp_bucket_nm + '/' + parquet_file_nm\n",
        "\n",
        "  # Copy YAML file\n",
        "  # Save to Google Drive mount point then copy to the GCP bucket\n",
        "  with open(dest_yaml_file_nm, 'w') as outfile:\n",
        "      yaml.dump(parquet_config_dct, outfile, default_flow_style=False)\n",
        "\n",
        "  # Gsutil to copy to GCP Cloud Storage\n",
        "  !gsutil cp $dest_yaml_file_nm $dest_yaml_path\n",
        "\n",
        "  # Copy Parquet file\n",
        "  # Save to Google Drive mount point then copy to the GCP bucket\n",
        "  __df.to_parquet(parquet_file_nm)\n",
        "\n",
        "  # Gsutil to copy to GCP Cloud Storage\n",
        "  !gsutil cp $parquet_file_nm $dest_parquet_path\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVngGY-kpMf9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}