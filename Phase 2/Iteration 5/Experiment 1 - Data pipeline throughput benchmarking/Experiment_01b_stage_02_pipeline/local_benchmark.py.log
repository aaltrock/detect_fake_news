lePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636425896
  nanos: 311669588
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_5"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425896
  nanos: 435844182
}
message: "Finished listing 1 files in 0.12416219711303711 seconds."
instruction_id: "bundle_5"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425896
  nanos: 615001440
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425896
  nanos: 615401268
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425896
  nanos: 615706205
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425896
  nanos: 616116523
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425896
  nanos: 624134063
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425896
  nanos: 624524831
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

e921bc5fa85c6f721141ff57160ff57f0de64a9c5cbd721a15a8fed5b98e2534
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 846.8577899932861 seconds.
INFO:root:Successfully completed job in 846.8577899932861 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fe32b890560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fe32b8905f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fe32b890d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-ae334cc8-c698-4a76-8d71-3f6b6a56f7e5'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7fe32b88cf80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7fe32b8900e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fe32b890560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fe32b8905f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7fe32b8907a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7fe32b890830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7fe32b890950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7fe32b8909e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7fe32b890a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7fe32b890b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fe32b890d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7fe32b890cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7fe32b890dd0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 50187
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 50188
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 50189
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 50190
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function read_to_impulse at 0x7fe32b890a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7fe32b890b00> ====================
INFO:root:==================== <function sort_stages at 0x7fe32b890d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7fe32b890cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7fe32b890dd0> ====================
INFO:root:starting control server on port 50187
INFO:root:starting data server on port 50188
INFO:root:starting state server on port 50189
INFO:root:starting logging server on port 50190
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fe32c73d890> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fe32c73d890> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'7f7d992f80d1129196724fa71667733c69813e4778c2376aa03f0ecedaec8751', worker_id = worker_1
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'7f7d992f80d1129196724fa71667733c69813e4778c2376aa03f0ecedaec8751', worker_id = worker_1
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636425904
  nanos: 120632410
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425904
  nanos: 129763603
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636425904
  nanos: 348683118
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425904
  nanos: 358152389
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_2500\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425904
  nanos: 368096351
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425904
  nanos: 369236707
}
message: "Creating insecure control channel for host.docker.internal:50187."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425904
  nanos: 378316879
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425904
  nanos: 382146358
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425904
  nanos: 393015384
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425904
  nanos: 408154726
}
message: "Creating insecure state channel for host.docker.internal:50189."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425904
  nanos: 408934116
}
message: "State channel established."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425904
  nanos: 416126251
}
message: "Creating client data channel for host.docker.internal:50188"
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425904
  nanos: 616956949
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425904
  nanos: 617475509
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425904
  nanos: 676096200
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425904
  nanos: 760842561
}
message: "Refreshing access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425905
  nanos: 245388031
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425906
  nanos: 446358442
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_2500\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_2500`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_2500` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636425906
  nanos: 858191013
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_c6f4ee5d003048ce89d31a34a6d2b4d0 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425908
  nanos: 422086954
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_f15ee20e-b_1636425907_956\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_f15ee20e-b_1636425907_956"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425908
  nanos: 561437129
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425913
  nanos: 799915552
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425914
  nanos: 37669658
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_f15ee20e-b_1636425913_674\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_f15ee20e-b_1636425913_674"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425914
  nanos: 166758537
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425919
  nanos: 317121028
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425919
  nanos: 328630924
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425919
  nanos: 486283779
}
message: "Finished listing 1 files in 0.15770363807678223 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636425923
  nanos: 844734907
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_9"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636425923
  nanos: 987010478
}
message: "Finished listing 1 files in 0.14227747917175293 seconds."
instruction_id: "bundle_9"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:severity: INFO
timestamp {
  seconds: 1636426745
  nanos: 41274547
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426745
  nanos: 41719913
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426745
  nanos: 42142391
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426745
  nanos: 42471885
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426745
  nanos: 45477390
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426745
  nanos: 45970916
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

7f7d992f80d1129196724fa71667733c69813e4778c2376aa03f0ecedaec8751
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 848.318197965622 seconds.
INFO:root:Successfully completed job in 848.318197965622 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
Completed at: 2021-11-09 02:59:05.294548
Duration: 0:28:15.948384
END
Started at: 2021-11-09 02:59:07.401130
/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1935: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported
  temp_location = pcoll.pipeline.options.view_as(
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fcacbb47560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fcacbb475f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fcacbb47d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-20509565-9d49-424c-974e-68e956b4642b'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7fcacbb43f80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7fcacbb470e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fcacbb47560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fcacbb475f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7fcacbb477a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7fcacbb47830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7fcacbb47950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7fcacbb479e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7fcacbb47a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7fcacbb47b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fcacbb47d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7fcacbb47cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7fcacbb47dd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7fcacbb477a0> ====================
INFO:root:==================== <function expand_gbk at 0x7fcacbb47830> ====================
INFO:root:==================== <function sink_flattens at 0x7fcacbb47950> ====================
INFO:root:==================== <function greedily_fuse at 0x7fcacbb479e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7fcacbb47a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7fcacbb47b00> ====================
INFO:root:==================== <function sort_stages at 0x7fcacbb47d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7fcacbb47cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7fcacbb47dd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 54312
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 54313
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 54314
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 54315
INFO:root:starting control server on port 54312
INFO:root:starting data server on port 54313
INFO:root:starting state server on port 54314
INFO:root:starting logging server on port 54315
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fcacc9f2c90> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fcacc9f2c90> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'2b38e1182d500805e7d41bb7cba7b62fa4fcdff9bb64cc14ee0f9055de0559bd', worker_id = worker_0
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'2b38e1182d500805e7d41bb7cba7b62fa4fcdff9bb64cc14ee0f9055de0559bd', worker_id = worker_0
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636426755
  nanos: 79824209
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426755
  nanos: 89531898
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636426755
  nanos: 306558609
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426755
  nanos: 316004753
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_2500\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426755
  nanos: 326067924
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426755
  nanos: 327198266
}
message: "Creating insecure control channel for host.docker.internal:54312."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426755
  nanos: 336424350
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426755
  nanos: 339873075
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426755
  nanos: 350729942
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426755
  nanos: 366727113
}
message: "Creating insecure state channel for host.docker.internal:54314."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426755
  nanos: 367495059
}
message: "State channel established."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426755
  nanos: 375154018
}
message: "Creating client data channel for host.docker.internal:54313"
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636426755
  nanos: 685056686
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426755
  nanos: 685562849
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426755
  nanos: 744559049
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426755
  nanos: 829441547
}
message: "Refreshing access_token"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426757
  nanos: 59967517
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426757
  nanos: 605802536
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_2500\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_2500`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_2500` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636426757
  nanos: 920701742
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_a4cde7c8d7914379b1c7fa2eb31d9d75 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426759
  nanos: 377482891
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_2f7627ee-c_1636426758_500\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_2f7627ee-c_1636426758_500"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426759
  nanos: 516399860
}
message: "Job status: RUNNING"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426764
  nanos: 725563049
}
message: "Job status: DONE"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426765
  nanos: 40539741
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_2f7627ee-c_1636426764_72\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_2f7627ee-c_1636426764_72"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426765
  nanos: 146692276
}
message: "Job status: RUNNING"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426770
  nanos: 296408176
}
message: "Job status: DONE"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426770
  nanos: 309705018
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426770
  nanos: 476082086
}
message: "Finished listing 1 files in 0.16642498970031738 seconds."
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636426775
  nanos: 554813146
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_4"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636426775
  nanos: 683522224
}
message: "Finished listing 1 files in 0.12871575355529785 seconds."
instruction_id: "bundle_4"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:severity: INFO
timestamp {
  seconds: 1636427603
  nanos: 249580144
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427603
  nanos: 250466823
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427603
  nanos: 251071453
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427603
  nanos: 251570940
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427603
  nanos: 265674591
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427603
  nanos: 266353368
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

2b38e1182d500805e7d41bb7cba7b62fa4fcdff9bb64cc14ee0f9055de0559bd
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 855.4750609397888 seconds.
INFO:root:Successfully completed job in 855.4750609397888 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fcacbb47560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fcacbb475f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fcacbb47d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-ed569563-f5aa-4812-a573-e781904eaebc'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7fcacbb43f80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7fcacbb470e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fcacbb47560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fcacbb475f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7fcacbb477a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7fcacbb47830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7fcacbb47950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7fcacbb479e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7fcacbb47a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7fcacbb47b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fcacbb47d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7fcacbb47cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7fcacbb47dd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7fcacbb477a0> ====================
INFO:root:==================== <function expand_gbk at 0x7fcacbb47830> ====================
INFO:root:==================== <function sink_flattens at 0x7fcacbb47950> ====================
INFO:root:==================== <function greedily_fuse at 0x7fcacbb479e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7fcacbb47a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7fcacbb47b00> ====================
INFO:root:==================== <function sort_stages at 0x7fcacbb47d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7fcacbb47cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7fcacbb47dd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 58515
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 58516
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 58517
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 58518
INFO:root:starting control server on port 58515
INFO:root:starting data server on port 58516
INFO:root:starting state server on port 58517
INFO:root:starting logging server on port 58518
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fcab8497e10> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fcab8497e10> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'a0b89d47b2ff8e4838632dd04413fd85111c89b6ad1ab980192955612be6772d', worker_id = worker_1
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'a0b89d47b2ff8e4838632dd04413fd85111c89b6ad1ab980192955612be6772d', worker_id = worker_1
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636427610
  nanos: 532347202
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427610
  nanos: 541304588
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636427610
  nanos: 760909318
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427610
  nanos: 770407676
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_2500\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427610
  nanos: 780559539
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427610
  nanos: 781694412
}
message: "Creating insecure control channel for host.docker.internal:58515."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427610
  nanos: 790749788
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427610
  nanos: 794192075
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427610
  nanos: 804332971
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427610
  nanos: 819813489
}
message: "Creating insecure state channel for host.docker.internal:58517."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427610
  nanos: 820572853
}
message: "State channel established."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427610
  nanos: 827641963
}
message: "Creating client data channel for host.docker.internal:58516"
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636427611
  nanos: 132178068
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427611
  nanos: 132683038
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427611
  nanos: 190088987
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427611
  nanos: 274739027
}
message: "Refreshing access_token"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427612
  nanos: 494776248
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427613
  nanos: 675488948
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_2500\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_2500`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_2500` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636427613
  nanos: 976117134
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_b65fb3adfed64a508074ae9582c6fcd7 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427615
  nanos: 435307264
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_2f7627ee-c_1636427615_395\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_2f7627ee-c_1636427615_395"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427615
  nanos: 542270421
}
message: "Job status: RUNNING"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427620
  nanos: 771475791
}
message: "Job status: DONE"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427621
  nanos: 164839029
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_2f7627ee-c_1636427620_953\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_2f7627ee-c_1636427620_953"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427621
  nanos: 265098810
}
message: "Job status: RUNNING"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427626
  nanos: 437511920
}
message: "Job status: DONE"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427626
  nanos: 449857950
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427626
  nanos: 613932371
}
message: "Finished listing 1 files in 0.16415810585021973 seconds."
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636427630
  nanos: 995881319
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_9"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636427631
  nanos: 123407840
}
message: "Finished listing 1 files in 0.1275327205657959 seconds."
instruction_id: "bundle_9"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:severity: INFO
timestamp {
  seconds: 1636428478
  nanos: 424310207
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428478
  nanos: 424970388
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428478
  nanos: 425572872
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428478
  nanos: 426045656
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428478
  nanos: 437832593
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428478
  nanos: 438440561
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

a0b89d47b2ff8e4838632dd04413fd85111c89b6ad1ab980192955612be6772d
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 875.0530707836151 seconds.
INFO:root:Successfully completed job in 875.0530707836151 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
Completed at: 2021-11-09 03:27:58.669455
Duration: 0:28:51.268325
END
Started at: 2021-11-09 03:28:00.749245
/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1935: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported
  temp_location = pcoll.pipeline.options.view_as(
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f8603730560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f86037305f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f8603730d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-f78039b8-4ddb-4e26-8aa8-436d06e6c0cf'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f860372cf80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f86037300e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f8603730560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f86037305f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f86037307a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f8603730830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f8603730950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f86037309e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f8603730a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f8603730b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f8603730d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f8603730cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f8603730dd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7f86037307a0> ====================
INFO:root:==================== <function expand_gbk at 0x7f8603730830> ====================
INFO:root:==================== <function sink_flattens at 0x7f8603730950> ====================
INFO:root:==================== <function greedily_fuse at 0x7f86037309e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7f8603730a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7f8603730b00> ====================
INFO:root:==================== <function sort_stages at 0x7f8603730d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7f8603730cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7f8603730dd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 62655
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 62656
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 62657
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 62658
INFO:root:starting control server on port 62655
INFO:root:starting data server on port 62656
INFO:root:starting state server on port 62657
INFO:root:starting logging server on port 62658
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7f8604624a10> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7f8604624a10> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'05e8ff2e043492aa4d05aee81b14d04359d9abd8e67511635cb577ba8c45b95f', worker_id = worker_0
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'05e8ff2e043492aa4d05aee81b14d04359d9abd8e67511635cb577ba8c45b95f', worker_id = worker_0
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636428488
  nanos: 464164495
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428488
  nanos: 474254369
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636428488
  nanos: 693046569
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428488
  nanos: 701957464
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_2500\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428488
  nanos: 711860418
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428488
  nanos: 712990283
}
message: "Creating insecure control channel for host.docker.internal:62655."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428488
  nanos: 722179174
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428488
  nanos: 725646734
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428488
  nanos: 735864400
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428488
  nanos: 752231359
}
message: "Creating insecure state channel for host.docker.internal:62657."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428488
  nanos: 752979278
}
message: "State channel established."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428488
  nanos: 760003328
}
message: "Creating client data channel for host.docker.internal:62656"
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428488
  nanos: 961421012
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428488
  nanos: 961956024
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428489
  nanos: 21853208
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428489
  nanos: 105915307
}
message: "Refreshing access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428489
  nanos: 623583555
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428490
  nanos: 121922016
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_2500\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_2500`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_2500` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636428490
  nanos: 525007009
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_e196bf4dbcc04b0e87f98f72a320a83e does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428492
  nanos: 186153411
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_6004df3b-9_1636428491_839\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_6004df3b-9_1636428491_839"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428492
  nanos: 282357692
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428497
  nanos: 502839088
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428497
  nanos: 813964128
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_6004df3b-9_1636428497_21\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_6004df3b-9_1636428497_21"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428497
  nanos: 912706613
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428503
  nanos: 68501949
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428503
  nanos: 80810308
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428503
  nanos: 252236127
}
message: "Finished listing 1 files in 0.1714792251586914 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636428507
  nanos: 659810543
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_4"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636428507
  nanos: 819335460
}
message: "Finished listing 1 files in 0.1595289707183838 seconds."
instruction_id: "bundle_4"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:severity: INFO
timestamp {
  seconds: 1636429329
  nanos: 388903141
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429329
  nanos: 389750719
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429329
  nanos: 390388727
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429329
  nanos: 391056060
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429329
  nanos: 405017375
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429329
  nanos: 405565023
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

05e8ff2e043492aa4d05aee81b14d04359d9abd8e67511635cb577ba8c45b95f
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 848.2493360042572 seconds.
INFO:root:Successfully completed job in 848.2493360042572 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f8603730560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f86037305f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f8603730d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-855f4597-9c78-4ed5-82fd-1fc95129802e'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f860372cf80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f86037300e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f8603730560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f86037305f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f86037307a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f8603730830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f8603730950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f86037309e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f8603730a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f8603730b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f8603730d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f8603730cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f8603730dd0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 50396
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 50397
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 50398
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 50399
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function read_to_impulse at 0x7f8603730a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7f8603730b00> ====================
INFO:root:==================== <function sort_stages at 0x7f8603730d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7f8603730cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7f8603730dd0> ====================
INFO:root:starting control server on port 50396
INFO:root:starting data server on port 50397
INFO:root:starting state server on port 50398
INFO:root:starting logging server on port 50399
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7f85e0266a10> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7f85e0266a10> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'7db51a4083f2e21f2a72be885fd218dac20cab0f93fc71854139043da6166d14', worker_id = worker_1
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'7db51a4083f2e21f2a72be885fd218dac20cab0f93fc71854139043da6166d14', worker_id = worker_1
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636429336
  nanos: 756722927
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429336
  nanos: 765945434
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636429336
  nanos: 984349966
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429336
  nanos: 993331193
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_2500\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429337
  nanos: 3228902
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429337
  nanos: 4394292
}
message: "Creating insecure control channel for host.docker.internal:50396."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429337
  nanos: 12859344
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429337
  nanos: 17388105
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429337
  nanos: 28222560
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429337
  nanos: 43287754
}
message: "Creating insecure state channel for host.docker.internal:50398."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429337
  nanos: 44035673
}
message: "State channel established."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429337
  nanos: 50996065
}
message: "Creating client data channel for host.docker.internal:50397"
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636429337
  nanos: 353176116
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429337
  nanos: 353700876
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429337
  nanos: 410576105
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429337
  nanos: 496413946
}
message: "Refreshing access_token"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429338
  nanos: 17093181
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429339
  nanos: 192501783
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_2500\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_2500`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_2500` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636429339
  nanos: 421237468
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_20b992f49dab4cb585527587c1688d67 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429341
  nanos: 93647241
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_6004df3b-9_1636429340_558\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_6004df3b-9_1636429340_558"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429341
  nanos: 203572511
}
message: "Job status: RUNNING"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429346
  nanos: 411756277
}
message: "Job status: DONE"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429346
  nanos: 716832160
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_6004df3b-9_1636429346_189\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_6004df3b-9_1636429346_189"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429346
  nanos: 828221797
}
message: "Job status: RUNNING"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429351
  nanos: 988457202
}
message: "Job status: DONE"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429352
  nanos: 1003742
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429352
  nanos: 175410270
}
message: "Finished listing 1 files in 0.17445588111877441 seconds."
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636429355
  nanos: 980412483
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_9"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636429356
  nanos: 122738361
}
message: "Finished listing 1 files in 0.14232826232910156 seconds."
instruction_id: "bundle_9"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:severity: INFO
timestamp {
  seconds: 1636430171
  nanos: 233314037
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430171
  nanos: 233987569
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430171
  nanos: 234602212
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430171
  nanos: 235066175
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430171
  nanos: 239427804
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430171
  nanos: 239934921
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

7db51a4083f2e21f2a72be885fd218dac20cab0f93fc71854139043da6166d14
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 841.725790977478 seconds.
INFO:root:Successfully completed job in 841.725790977478 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
Completed at: 2021-11-09 03:56:11.474284
Duration: 0:28:10.725039
END
Started at: 2021-11-09 03:56:13.638492
/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1935: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported
  temp_location = pcoll.pipeline.options.view_as(
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fd7db870560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fd7db8705f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fd7db870d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-3dd3b5ad-27cd-4c1c-ac72-32871870efaa'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7fd7db86cf80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7fd7db8700e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fd7db870560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fd7db8705f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7fd7db8707a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7fd7db870830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7fd7db870950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7fd7db8709e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7fd7db870a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7fd7db870b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fd7db870d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7fd7db870cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7fd7db870dd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7fd7db8707a0> ====================
INFO:root:==================== <function expand_gbk at 0x7fd7db870830> ====================
INFO:root:==================== <function sink_flattens at 0x7fd7db870950> ====================
INFO:root:==================== <function greedily_fuse at 0x7fd7db8709e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7fd7db870a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7fd7db870b00> ====================
INFO:root:==================== <function sort_stages at 0x7fd7db870d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7fd7db870cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7fd7db870dd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 54500
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 54501
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 54502
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 54503
INFO:root:starting control server on port 54500
INFO:root:starting data server on port 54501
INFO:root:starting state server on port 54502
INFO:root:starting logging server on port 54503
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fd7dc731590> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fd7dc731590> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'26d2029c9ac4c111b7d22e0b62ac8e850d07aece1d9218888d7b1dc90dd69d3d', worker_id = worker_0
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'26d2029c9ac4c111b7d22e0b62ac8e850d07aece1d9218888d7b1dc90dd69d3d', worker_id = worker_0
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636430181
  nanos: 353775024
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430181
  nanos: 363985300
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636430181
  nanos: 586097955
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430181
  nanos: 595859527
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_2500\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430181
  nanos: 606699228
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430181
  nanos: 607874631
}
message: "Creating insecure control channel for host.docker.internal:54500."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430181
  nanos: 616641521
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430181
  nanos: 620414733
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430181
  nanos: 631047010
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430181
  nanos: 647265195
}
message: "Creating insecure state channel for host.docker.internal:54502."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430181
  nanos: 648064613
}
message: "State channel established."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430181
  nanos: 655390501
}
message: "Creating client data channel for host.docker.internal:54501"
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430181
  nanos: 858407497
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430181
  nanos: 858936071
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430181
  nanos: 917012929
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430182
  nanos: 1556634
}
message: "Refreshing access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430183
  nanos: 192157745
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430184
  nanos: 334913969
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_2500\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_2500`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_2500` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636430184
  nanos: 771184682
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_3b731ef803a24e8898c340724a159764 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430187
  nanos: 261864423
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_2d0d611f-f_1636430186_55\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_2d0d611f-f_1636430186_55"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430187
  nanos: 394728422
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430192
  nanos: 640340805
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430192
  nanos: 897658586
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_2d0d611f-f_1636430192_999\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_2d0d611f-f_1636430192_999"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430192
  nanos: 975831508
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430198
  nanos: 115211725
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430198
  nanos: 120627403
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636430198
  nanos: 281701803
}
message: "Finished listing 1 files in 0.1610856056213379 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636431033
  nanos: 154207468
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_5"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431033
  nanos: 281526565
}
message: "Finished listing 1 files in 0.1273047924041748 seconds."
instruction_id: "bundle_5"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431033
  nanos: 504188299
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431033
  nanos: 504785299
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431033
  nanos: 505191087
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431033
  nanos: 505593299
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431033
  nanos: 515369653
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431033
  nanos: 515909433
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

26d2029c9ac4c111b7d22e0b62ac8e850d07aece1d9218888d7b1dc90dd69d3d
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 859.5058097839355 seconds.
INFO:root:Successfully completed job in 859.5058097839355 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fd7db870560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fd7db8705f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fd7db870d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-1cc56fd0-667a-4d74-a302-b008d1a54b62'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7fd7db86cf80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7fd7db8700e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fd7db870560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fd7db8705f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7fd7db8707a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7fd7db870830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7fd7db870950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7fd7db8709e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7fd7db870a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7fd7db870b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fd7db870d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7fd7db870cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7fd7db870dd0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 58619
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 58620
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 58621
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 58622
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function read_to_impulse at 0x7fd7db870a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7fd7db870b00> ====================
INFO:root:==================== <function sort_stages at 0x7fd7db870d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7fd7db870cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7fd7db870dd0> ====================
INFO:root:starting control server on port 58619
INFO:root:starting data server on port 58620
INFO:root:starting state server on port 58621
INFO:root:starting logging server on port 58622
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fd7dc815e10> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fd7dc815e10> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'948502a7a4c60e0cd327cd102269eb5bf61d09017b51edb9553ee214de9fbcd8', worker_id = worker_1
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'948502a7a4c60e0cd327cd102269eb5bf61d09017b51edb9553ee214de9fbcd8', worker_id = worker_1
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636431040
  nanos: 844546556
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431040
  nanos: 854290246
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636431041
  nanos: 74889898
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431041
  nanos: 84137678
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_2500\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431041
  nanos: 94237327
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431041
  nanos: 95380306
}
message: "Creating insecure control channel for host.docker.internal:58619."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431041
  nanos: 104274988
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431041
  nanos: 107766389
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431041
  nanos: 118137836
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431041
  nanos: 133704900
}
message: "Creating insecure state channel for host.docker.internal:58621."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431041
  nanos: 134475708
}
message: "State channel established."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431041
  nanos: 141299486
}
message: "Creating client data channel for host.docker.internal:58620"
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431041
  nanos: 343933105
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431041
  nanos: 344472169
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431041
  nanos: 403663396
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431041
  nanos: 494796037
}
message: "Refreshing access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431042
  nanos: 755478382
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431043
  nanos: 284955501
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_2500\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_2500`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_2500` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636431043
  nanos: 615102291
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_28815492fb1b406dba3c569f98358c3a does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431045
  nanos: 71520328
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_2d0d611f-f_1636431044_889\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_2d0d611f-f_1636431044_889"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431045
  nanos: 196069478
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431050
  nanos: 397158622
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431050
  nanos: 700971841
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_2d0d611f-f_1636431050_207\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_2d0d611f-f_1636431050_207"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431050
  nanos: 796041250
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431055
  nanos: 945247173
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431055
  nanos: 957132101
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431056
  nanos: 127717971
}
message: "Finished listing 1 files in 0.17063522338867188 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636431894
  nanos: 234861850
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_10"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431894
  nanos: 367272853
}
message: "Finished listing 1 files in 0.13241219520568848 seconds."
instruction_id: "bundle_10"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431894
  nanos: 524111986
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431894
  nanos: 524490356
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431894
  nanos: 524772405
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431894
  nanos: 525066614
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431894
  nanos: 527498960
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431894
  nanos: 527854442
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

948502a7a4c60e0cd327cd102269eb5bf61d09017b51edb9553ee214de9fbcd8
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 860.8951480388641 seconds.
INFO:root:Successfully completed job in 860.8951480388641 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
Completed at: 2021-11-09 04:24:54.771335
Duration: 0:28:41.132843
END
Started at: 2021-11-09 04:24:56.950697
/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1935: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported
  temp_location = pcoll.pipeline.options.view_as(
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fab32d7f560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fab32d7f5f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fab32d7fd40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-e0052c77-fd5f-41c5-acf6-37027dd41871'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7fab32d7bf80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7fab32d7f0e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fab32d7f560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fab32d7f5f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7fab32d7f7a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7fab32d7f830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7fab32d7f950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7fab32d7f9e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7fab32d7fa70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7fab32d7fb00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fab32d7fd40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7fab32d7fcb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7fab32d7fdd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7fab32d7f7a0> ====================
INFO:root:==================== <function expand_gbk at 0x7fab32d7f830> ====================
INFO:root:==================== <function sink_flattens at 0x7fab32d7f950> ====================
INFO:root:==================== <function greedily_fuse at 0x7fab32d7f9e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7fab32d7fa70> ====================
INFO:root:==================== <function impulse_to_input at 0x7fab32d7fb00> ====================
INFO:root:==================== <function sort_stages at 0x7fab32d7fd40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7fab32d7fcb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7fab32d7fdd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 62757
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 62758
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 62759
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 62760
INFO:root:starting control server on port 62757
INFO:root:starting data server on port 62758
INFO:root:starting state server on port 62759
INFO:root:starting logging server on port 62760
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fab33a33390> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fab33a33390> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'f964e1f7af81a39cb08fa37873decad0924e4bdef37b0a2ff5bc7d982f24cf14', worker_id = worker_0
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'f964e1f7af81a39cb08fa37873decad0924e4bdef37b0a2ff5bc7d982f24cf14', worker_id = worker_0
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636431904
  nanos: 656978845
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431904
  nanos: 669304370
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636431904
  nanos: 888100385
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431904
  nanos: 897614955
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_5000\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431904
  nanos: 908740758
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431904
  nanos: 909946918
}
message: "Creating insecure control channel for host.docker.internal:62757."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431904
  nanos: 918779611
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431904
  nanos: 922303915
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431904
  nanos: 932627201
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431904
  nanos: 947920322
}
message: "Creating insecure state channel for host.docker.internal:62759."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431904
  nanos: 948680877
}
message: "State channel established."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431904
  nanos: 956251621
}
message: "Creating client data channel for host.docker.internal:62758"
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431905
  nanos: 158039093
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431905
  nanos: 158569574
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431905
  nanos: 218809843
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431905
  nanos: 303422689
}
message: "Refreshing access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431906
  nanos: 492848157
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431906
  nanos: 999491691
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_5000\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_5000`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_5000` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636431907
  nanos: 358667612
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_7af243bc21cd4abb81f0e7b095e858bc does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431908
  nanos: 505430698
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_7732a74e-9_1636431908_7\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_7732a74e-9_1636431908_7"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431908
  nanos: 614771604
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431913
  nanos: 819135189
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431914
  nanos: 55832147
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_7732a74e-9_1636431913_774\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_7732a74e-9_1636431913_774"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431914
  nanos: 178180932
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431919
  nanos: 316982507
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431919
  nanos: 328279733
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636431919
  nanos: 494686126
}
message: "Finished listing 1 files in 0.16645383834838867 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636433574
  nanos: 720309019
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_5"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433574
  nanos: 850161075
}
message: "Finished listing 1 files in 0.1298513412475586 seconds."
instruction_id: "bundle_5"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433575
  nanos: 108561038
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433575
  nanos: 109074592
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433575
  nanos: 109514951
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433575
  nanos: 109882116
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433575
  nanos: 113971233
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433575
  nanos: 114445686
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

f964e1f7af81a39cb08fa37873decad0924e4bdef37b0a2ff5bc7d982f24cf14
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 1677.8576321601868 seconds.
INFO:root:Successfully completed job in 1677.8576321601868 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fab32d7f560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fab32d7f5f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fab32d7fd40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-8185dea3-75ff-4299-9e89-51f2b7f42b11'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7fab32d7bf80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7fab32d7f0e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fab32d7f560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fab32d7f5f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7fab32d7f7a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7fab32d7f830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7fab32d7f950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7fab32d7f9e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7fab32d7fa70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7fab32d7fb00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fab32d7fd40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7fab32d7fcb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7fab32d7fdd0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 54506
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 54507
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 54508
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 54509
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function greedily_fuse at 0x7fab32d7f9e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7fab32d7fa70> ====================
INFO:root:==================== <function impulse_to_input at 0x7fab32d7fb00> ====================
INFO:root:==================== <function sort_stages at 0x7fab32d7fd40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7fab32d7fcb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7fab32d7fdd0> ====================
INFO:root:starting control server on port 54506
INFO:root:starting data server on port 54507
INFO:root:starting state server on port 54508
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:root:starting logging server on port 54509
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fab338de590> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fab338de590> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'614a63372c9a31673a66ad986e595c83109adf8b363df3a5b8887996cee20680', worker_id = worker_1
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'614a63372c9a31673a66ad986e595c83109adf8b363df3a5b8887996cee20680', worker_id = worker_1
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636433582
  nanos: 678707122
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433582
  nanos: 688978672
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636433582
  nanos: 908030509
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433582
  nanos: 916800737
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_5000\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433582
  nanos: 927400350
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433582
  nanos: 928543806
}
message: "Creating insecure control channel for host.docker.internal:54506."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433582
  nanos: 937783479
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433582
  nanos: 941296339
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433582
  nanos: 952204704
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433582
  nanos: 967736721
}
message: "Creating insecure state channel for host.docker.internal:54508."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433582
  nanos: 968508481
}
message: "State channel established."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433582
  nanos: 975737333
}
message: "Creating client data channel for host.docker.internal:54507"
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433583
  nanos: 177407979
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433583
  nanos: 177939414
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433583
  nanos: 236370801
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433583
  nanos: 321329355
}
message: "Refreshing access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433584
  nanos: 23519515
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433585
  nanos: 195981264
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_5000\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_5000`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_5000` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636433585
  nanos: 502390384
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_9bac4c5ce43f4c188b5bd49b1d4889b1 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433586
  nanos: 864212036
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_7732a74e-9_1636433586_526\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_7732a74e-9_1636433586_526"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433587
  nanos: 5181789
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433592
  nanos: 233664989
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433592
  nanos: 526337862
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_7732a74e-9_1636433592_824\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_7732a74e-9_1636433592_824"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433592
  nanos: 675929784
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433597
  nanos: 850725173
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433597
  nanos: 861860036
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433598
  nanos: 19082784
}
message: "Finished listing 1 files in 0.15727949142456055 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636433605
  nanos: 563165187
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_9"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636433605
  nanos: 718202829
}
message: "Finished listing 1 files in 0.15504813194274902 seconds."
instruction_id: "bundle_9"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:severity: INFO
timestamp {
  seconds: 1636435270
  nanos: 916791439
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435270
  nanos: 917639970
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435270
  nanos: 918180704
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435270
  nanos: 918701887
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435270
  nanos: 937895059
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435270
  nanos: 938403129
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

614a63372c9a31673a66ad986e595c83109adf8b363df3a5b8887996cee20680
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 1695.693950176239 seconds.
INFO:root:Successfully completed job in 1695.693950176239 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
Completed at: 2021-11-09 05:21:11.197171
Duration: 0:56:14.246474
END
Started at: 2021-11-09 05:21:13.406718
/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1935: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported
  temp_location = pcoll.pipeline.options.view_as(
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fc5b2e40560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fc5b2e405f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fc5b2e40d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-921ded6f-244e-4e55-8750-0529f3ecbe4a'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7fc5b2e3cf80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7fc5b2e400e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fc5b2e40560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fc5b2e405f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7fc5b2e407a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7fc5b2e40830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7fc5b2e40950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7fc5b2e409e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7fc5b2e40a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7fc5b2e40b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fc5b2e40d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7fc5b2e40cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7fc5b2e40dd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7fc5b2e407a0> ====================
INFO:root:==================== <function expand_gbk at 0x7fc5b2e40830> ====================
INFO:root:==================== <function sink_flattens at 0x7fc5b2e40950> ====================
INFO:root:==================== <function greedily_fuse at 0x7fc5b2e409e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7fc5b2e40a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7fc5b2e40b00> ====================
INFO:root:==================== <function sort_stages at 0x7fc5b2e40d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7fc5b2e40cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7fc5b2e40dd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 62657
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 62658
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 62659
INFO:root:starting control server on port 62657
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 62660
INFO:root:starting data server on port 62658
INFO:root:starting state server on port 62659
INFO:root:starting logging server on port 62660
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fc5a14e9250> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fc5a14e9250> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'a30b594051c49754be5fb130808efef6f1a63cda06ab3f1d2b797d825da9998a', worker_id = worker_0
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'a30b594051c49754be5fb130808efef6f1a63cda06ab3f1d2b797d825da9998a', worker_id = worker_0
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636435280
  nanos: 935050487
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435280
  nanos: 945917844
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636435281
  nanos: 163684368
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435281
  nanos: 172918796
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_5000\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435281
  nanos: 183132171
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435281
  nanos: 184296369
}
message: "Creating insecure control channel for host.docker.internal:62657."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435281
  nanos: 193983793
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435281
  nanos: 197640419
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435281
  nanos: 208272218
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435281
  nanos: 223706722
}
message: "Creating insecure state channel for host.docker.internal:62659."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435281
  nanos: 224488258
}
message: "State channel established."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435281
  nanos: 231581211
}
message: "Creating client data channel for host.docker.internal:62658"
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435281
  nanos: 433088064
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435281
  nanos: 433634042
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435281
  nanos: 492058992
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435281
  nanos: 576659440
}
message: "Refreshing access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435282
  nanos: 843307018
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435283
  nanos: 953764438
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_5000\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_5000`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_5000` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636435284
  nanos: 322879552
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_f35d6d7efc884aee9d2773f84b3ffe3e does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435285
  nanos: 984425783
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_3ad89d56-4_1636435285_338\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_3ad89d56-4_1636435285_338"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435286
  nanos: 121096372
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435291
  nanos: 339796781
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435291
  nanos: 711084127
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_3ad89d56-4_1636435291_581\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_3ad89d56-4_1636435291_581"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435291
  nanos: 827841520
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435297
  nanos: 3540277
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435297
  nanos: 15927791
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636435297
  nanos: 177087783
}
message: "Finished listing 1 files in 0.16121435165405273 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636436915
  nanos: 186953067
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_5"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436915
  nanos: 307354211
}
message: "Finished listing 1 files in 0.12038397789001465 seconds."
instruction_id: "bundle_5"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436915
  nanos: 509719371
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436915
  nanos: 510274887
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436915
  nanos: 510630130
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436915
  nanos: 511029720
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436915
  nanos: 514493942
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436915
  nanos: 515010118
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

a30b594051c49754be5fb130808efef6f1a63cda06ab3f1d2b797d825da9998a
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 1641.7938752174377 seconds.
INFO:root:Successfully completed job in 1641.7938752174377 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fc5b2e40560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fc5b2e405f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fc5b2e40d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-c621b243-a959-4050-a356-142bdc699a0f'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7fc5b2e3cf80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7fc5b2e400e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fc5b2e40560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fc5b2e405f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7fc5b2e407a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7fc5b2e40830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7fc5b2e40950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7fc5b2e409e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7fc5b2e40a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7fc5b2e40b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fc5b2e40d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7fc5b2e40cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7fc5b2e40dd0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 54325
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 54326
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 54327
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 54328
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function read_to_impulse at 0x7fc5b2e40a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7fc5b2e40b00> ====================
INFO:root:==================== <function sort_stages at 0x7fc5b2e40d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7fc5b2e40cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7fc5b2e40dd0> ====================
INFO:root:starting control server on port 54325
INFO:root:starting data server on port 54326
INFO:root:starting state server on port 54327
INFO:root:starting logging server on port 54328
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fc5a152e1d0> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fc5a152e1d0> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'077043e7ac41bebcf607da634c94427090237846aed06778543457243a70a183', worker_id = worker_1
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'077043e7ac41bebcf607da634c94427090237846aed06778543457243a70a183', worker_id = worker_1
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636436922
  nanos: 989385366
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436922
  nanos: 999320983
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636436923
  nanos: 218668937
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436923
  nanos: 228252887
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_5000\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436923
  nanos: 239347457
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436923
  nanos: 240491628
}
message: "Creating insecure control channel for host.docker.internal:54325."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436923
  nanos: 249128818
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436923
  nanos: 252729177
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436923
  nanos: 264036417
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436923
  nanos: 279059410
}
message: "Creating insecure state channel for host.docker.internal:54327."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436923
  nanos: 279819250
}
message: "State channel established."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436923
  nanos: 286748409
}
message: "Creating client data channel for host.docker.internal:54326"
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436923
  nanos: 489565849
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436923
  nanos: 490100622
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436923
  nanos: 548733711
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436923
  nanos: 633329629
}
message: "Refreshing access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436924
  nanos: 232495546
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436924
  nanos: 778250932
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_5000\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_5000`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_5000` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636436925
  nanos: 128333568
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_855e66a8aec84090a3926933a1e084bb does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436926
  nanos: 999597311
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_3ad89d56-4_1636436926_623\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_3ad89d56-4_1636436926_623"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436927
  nanos: 121671438
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436932
  nanos: 312540769
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436932
  nanos: 724652290
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_3ad89d56-4_1636436932_147\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_3ad89d56-4_1636436932_147"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436932
  nanos: 862869977
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436938
  nanos: 3975629
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436938
  nanos: 15384912
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436938
  nanos: 173996448
}
message: "Finished listing 1 files in 0.15866446495056152 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636436945
  nanos: 832923650
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_9"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636436945
  nanos: 969614744
}
message: "Finished listing 1 files in 0.13669204711914062 seconds."
instruction_id: "bundle_9"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:severity: INFO
timestamp {
  seconds: 1636438584
  nanos: 930351734
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438584
  nanos: 931498765
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438584
  nanos: 932149171
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438584
  nanos: 932664871
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438584
  nanos: 955566406
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438584
  nanos: 956219911
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

077043e7ac41bebcf607da634c94427090237846aed06778543457243a70a183
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 1669.305144071579 seconds.
INFO:root:Successfully completed job in 1669.305144071579 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
Completed at: 2021-11-09 06:16:25.195669
Duration: 0:55:11.788951
END
Started at: 2021-11-09 06:16:27.384565
/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1935: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported
  temp_location = pcoll.pipeline.options.view_as(
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f7da2e67560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f7da2e675f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f7da2e67d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-3af55f9d-1ce1-4729-8149-c1423b29d26e'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f7da2e63f80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f7da2e670e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f7da2e67560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f7da2e675f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f7da2e677a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f7da2e67830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f7da2e67950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f7da2e679e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f7da2e67a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f7da2e67b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f7da2e67d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f7da2e67cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f7da2e67dd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7f7da2e677a0> ====================
INFO:root:==================== <function expand_gbk at 0x7f7da2e67830> ====================
INFO:root:==================== <function sink_flattens at 0x7f7da2e67950> ====================
INFO:root:==================== <function greedily_fuse at 0x7f7da2e679e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7f7da2e67a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7f7da2e67b00> ====================
INFO:root:==================== <function sort_stages at 0x7f7da2e67d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7f7da2e67cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7f7da2e67dd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 62445
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 62446
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 62447
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 62448
INFO:root:starting control server on port 62445
INFO:root:starting data server on port 62446
INFO:root:starting state server on port 62447
INFO:root:starting logging server on port 62448
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7f7d914a36d0> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7f7d914a36d0> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'1ad2637026b43a29b033c09981bb3606b4704bfb447fa2b9e3c1efbad1b5b8ab', worker_id = worker_0
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'1ad2637026b43a29b033c09981bb3606b4704bfb447fa2b9e3c1efbad1b5b8ab', worker_id = worker_0
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636438595
  nanos: 125769376
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438595
  nanos: 135308742
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636438595
  nanos: 352177381
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438595
  nanos: 361496925
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_5000\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438595
  nanos: 371475219
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438595
  nanos: 372749328
}
message: "Creating insecure control channel for host.docker.internal:62445."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438595
  nanos: 381368398
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438595
  nanos: 384625434
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438595
  nanos: 395378112
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438595
  nanos: 411419391
}
message: "Creating insecure state channel for host.docker.internal:62447."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438595
  nanos: 412175416
}
message: "State channel established."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438595
  nanos: 419202327
}
message: "Creating client data channel for host.docker.internal:62446"
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438595
  nanos: 621625185
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438595
  nanos: 622171163
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438595
  nanos: 680035114
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438595
  nanos: 764527559
}
message: "Refreshing access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438596
  nanos: 424330234
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438597
  nanos: 623113393
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_5000\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_5000`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_5000` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636438598
  nanos: 25870084
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_805d6b81641540aaa7ef503045894bcd does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438599
  nanos: 689599275
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_9396822e-5_1636438599_656\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_9396822e-5_1636438599_656"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438599
  nanos: 796138525
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438605
  nanos: 7786750
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438605
  nanos: 421130657
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_9396822e-5_1636438605_705\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_9396822e-5_1636438605_705"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438605
  nanos: 574408292
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438610
  nanos: 755764007
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438610
  nanos: 767068862
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438610
  nanos: 925161123
}
message: "Finished listing 1 files in 0.15815234184265137 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636438616
  nanos: 780714511
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_4"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636438616
  nanos: 936028718
}
message: "Finished listing 1 files in 0.15532255172729492 seconds."
instruction_id: "bundle_4"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:severity: INFO
timestamp {
  seconds: 1636440220
  nanos: 837351083
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440220
  nanos: 837977170
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440220
  nanos: 838346242
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440220
  nanos: 838825225
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440220
  nanos: 845946311
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440220
  nanos: 846471786
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

1ad2637026b43a29b033c09981bb3606b4704bfb447fa2b9e3c1efbad1b5b8ab
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 1633.1422581672668 seconds.
INFO:root:Successfully completed job in 1633.1422581672668 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f7da2e67560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f7da2e675f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f7da2e67d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-873eab38-842f-4f7a-a4bd-108637c3bea5'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f7da2e63f80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f7da2e670e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f7da2e67560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f7da2e675f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f7da2e677a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f7da2e67830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f7da2e67950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f7da2e679e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f7da2e67a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f7da2e67b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f7da2e67d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f7da2e67cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f7da2e67dd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7f7da2e677a0> ====================
INFO:root:==================== <function expand_gbk at 0x7f7da2e67830> ====================
INFO:root:==================== <function sink_flattens at 0x7f7da2e67950> ====================
INFO:root:==================== <function greedily_fuse at 0x7f7da2e679e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7f7da2e67a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7f7da2e67b00> ====================
INFO:root:==================== <function sort_stages at 0x7f7da2e67d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7f7da2e67cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7f7da2e67dd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 54080
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 54081
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 54082
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 54083
INFO:root:starting control server on port 54080
INFO:root:starting data server on port 54081
INFO:root:starting state server on port 54082
INFO:root:starting logging server on port 54083
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7f7d9147add0> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7f7d9147add0> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'8d0d89f1a5fd449ec8cabec03cb82d28bbcb69421b97382b66928cd8a84d397c', worker_id = worker_1
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'8d0d89f1a5fd449ec8cabec03cb82d28bbcb69421b97382b66928cd8a84d397c', worker_id = worker_1
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636440228
  nanos: 219935655
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440228
  nanos: 230453252
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636440228
  nanos: 448687791
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440228
  nanos: 457529306
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_5000\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440228
  nanos: 467499256
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440228
  nanos: 468614578
}
message: "Creating insecure control channel for host.docker.internal:54080."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440228
  nanos: 477817773
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440228
  nanos: 481535196
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440228
  nanos: 493354558
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440228
  nanos: 509251117
}
message: "Creating insecure state channel for host.docker.internal:54082."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440228
  nanos: 509991168
}
message: "State channel established."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440228
  nanos: 517327070
}
message: "Creating client data channel for host.docker.internal:54081"
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636440228
  nanos: 823273897
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440228
  nanos: 823784589
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440228
  nanos: 881688833
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440228
  nanos: 966974735
}
message: "Refreshing access_token"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440229
  nanos: 835264205
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440230
  nanos: 344474554
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_5000\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_5000`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_5000` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636440230
  nanos: 702625036
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_0d05440c1f324400ab846be7a4e1990a does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440231
  nanos: 949510812
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_9396822e-5_1636440231_128\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_9396822e-5_1636440231_128"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440232
  nanos: 76398849
}
message: "Job status: RUNNING"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440237
  nanos: 287730455
}
message: "Job status: DONE"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440237
  nanos: 683317184
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_9396822e-5_1636440237_802\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_9396822e-5_1636440237_802"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440237
  nanos: 791405916
}
message: "Job status: RUNNING"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440242
  nanos: 931806325
}
message: "Job status: DONE"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440242
  nanos: 943158864
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636440243
  nanos: 116533994
}
message: "Finished listing 1 files in 0.17342734336853027 seconds."
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636441885
  nanos: 325520038
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_10"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441885
  nanos: 445601940
}
message: "Finished listing 1 files in 0.12008786201477051 seconds."
instruction_id: "bundle_10"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441885
  nanos: 633481979
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441885
  nanos: 633977890
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441885
  nanos: 634297132
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441885
  nanos: 634586095
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441885
  nanos: 637428045
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441885
  nanos: 637793302
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

8d0d89f1a5fd449ec8cabec03cb82d28bbcb69421b97382b66928cd8a84d397c
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 1664.687848329544 seconds.
INFO:root:Successfully completed job in 1664.687848329544 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
Completed at: 2021-11-09 07:11:25.898968
Duration: 0:54:58.514403
END
Started at: 2021-11-09 07:11:28.122478
/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1935: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported
  temp_location = pcoll.pipeline.options.view_as(
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f8d92f38560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f8d92f385f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f8d92f38d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-1e7780e0-31d4-4aff-a1bf-0b10fc6ce1bc'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f8d92f34f80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f8d92f380e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f8d92f38560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f8d92f385f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f8d92f387a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f8d92f38830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f8d92f38950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f8d92f389e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f8d92f38a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f8d92f38b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f8d92f38d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f8d92f38cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f8d92f38dd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7f8d92f387a0> ====================
INFO:root:==================== <function expand_gbk at 0x7f8d92f38830> ====================
INFO:root:==================== <function sink_flattens at 0x7f8d92f38950> ====================
INFO:root:==================== <function greedily_fuse at 0x7f8d92f389e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7f8d92f38a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7f8d92f38b00> ====================
INFO:root:==================== <function sort_stages at 0x7f8d92f38d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7f8d92f38cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7f8d92f38dd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 62184
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 62185
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 62186
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 62187
INFO:root:starting control server on port 62184
INFO:root:starting data server on port 62185
INFO:root:starting state server on port 62186
INFO:root:starting logging server on port 62187
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7f8d81228ad0> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7f8d81228ad0> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'6d7cd0c04a641cf7c0cb9d2e5507c374979bb95519c9dc5690a9e93b1ff01fdb', worker_id = worker_0
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'6d7cd0c04a641cf7c0cb9d2e5507c374979bb95519c9dc5690a9e93b1ff01fdb', worker_id = worker_0
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636441895
  nanos: 787973642
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441895
  nanos: 797114133
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636441896
  nanos: 15316486
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441896
  nanos: 24929761
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_5000\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441896
  nanos: 35070896
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441896
  nanos: 36193370
}
message: "Creating insecure control channel for host.docker.internal:62184."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441896
  nanos: 46013593
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441896
  nanos: 49545288
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441896
  nanos: 59692621
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441896
  nanos: 75730323
}
message: "Creating insecure state channel for host.docker.internal:62186."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441896
  nanos: 76476335
}
message: "State channel established."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441896
  nanos: 83575487
}
message: "Creating client data channel for host.docker.internal:62185"
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441896
  nanos: 282820940
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441896
  nanos: 283343791
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441896
  nanos: 340423822
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441896
  nanos: 425182580
}
message: "Refreshing access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441897
  nanos: 20080327
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441897
  nanos: 554564714
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_5000\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_5000`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_5000` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636441897
  nanos: 923959970
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_e4415cae588f45298b84e6b022184973 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441899
  nanos: 279573678
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_e1def835-e_1636441898_190\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_e1def835-e_1636441898_190"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441899
  nanos: 413775682
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441904
  nanos: 607054233
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441904
  nanos: 831919908
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_e1def835-e_1636441904_871\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_e1def835-e_1636441904_871"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441904
  nanos: 929875135
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441910
  nanos: 103940010
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441910
  nanos: 114740133
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636441910
  nanos: 280810832
}
message: "Finished listing 1 files in 0.16614341735839844 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636443592
  nanos: 536828041
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_5"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443592
  nanos: 663533210
}
message: "Finished listing 1 files in 0.1267085075378418 seconds."
instruction_id: "bundle_5"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443592
  nanos: 840523958
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443592
  nanos: 840895175
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443592
  nanos: 841176986
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443592
  nanos: 841468572
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443592
  nanos: 844053745
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443592
  nanos: 844438314
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

6d7cd0c04a641cf7c0cb9d2e5507c374979bb95519c9dc5690a9e93b1ff01fdb
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 1704.4120860099792 seconds.
INFO:root:Successfully completed job in 1704.4120860099792 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f8d92f38560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f8d92f385f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f8d92f38d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-5ccf80df-229f-4b9a-8da3-13dd2d0b7243'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f8d92f34f80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f8d92f380e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f8d92f38560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f8d92f385f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f8d92f387a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f8d92f38830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f8d92f38950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f8d92f389e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f8d92f38a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f8d92f38b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f8d92f38d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f8d92f38cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f8d92f38dd0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 53971
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 53972
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 53973
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 53974
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function read_to_impulse at 0x7f8d92f38a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7f8d92f38b00> ====================
INFO:root:==================== <function sort_stages at 0x7f8d92f38d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7f8d92f38cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7f8d92f38dd0> ====================
INFO:root:starting control server on port 53971
INFO:root:starting data server on port 53972
INFO:root:starting state server on port 53973
INFO:root:starting logging server on port 53974
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7f8d8127b3d0> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7f8d8127b3d0> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'951b51ae63c832c41e59cadbf9f1b2c19a7e18f36bd7457c888afb3b09a1dc3b', worker_id = worker_1
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'951b51ae63c832c41e59cadbf9f1b2c19a7e18f36bd7457c888afb3b09a1dc3b', worker_id = worker_1
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636443600
  nanos: 378745079
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443600
  nanos: 388226985
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636443600
  nanos: 610632181
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443600
  nanos: 620163679
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_5000\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443600
  nanos: 630357265
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443600
  nanos: 631572723
}
message: "Creating insecure control channel for host.docker.internal:53971."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443600
  nanos: 640243053
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443600
  nanos: 644020318
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443600
  nanos: 654566764
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443600
  nanos: 669568300
}
message: "Creating insecure state channel for host.docker.internal:53973."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443600
  nanos: 670334577
}
message: "State channel established."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443600
  nanos: 677330732
}
message: "Creating client data channel for host.docker.internal:53972"
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636443600
  nanos: 982341289
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443600
  nanos: 982843637
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443601
  nanos: 40733098
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443601
  nanos: 125705242
}
message: "Refreshing access_token"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443601
  nanos: 656450986
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443602
  nanos: 810345411
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_5000\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_5000`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_5000` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636443603
  nanos: 106622219
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_2b5d97268ee84b3ab82ba3b2625cd738 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443605
  nanos: 82589864
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_e1def835-e_1636443604_249\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_e1def835-e_1636443604_249"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443605
  nanos: 215451240
}
message: "Job status: RUNNING"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443610
  nanos: 501476287
}
message: "Job status: DONE"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443610
  nanos: 914251089
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_e1def835-e_1636443610_42\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_e1def835-e_1636443610_42"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443611
  nanos: 55930137
}
message: "Job status: RUNNING"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443616
  nanos: 221561193
}
message: "Job status: DONE"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443616
  nanos: 232861280
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443616
  nanos: 404958963
}
message: "Finished listing 1 files in 0.17217540740966797 seconds."
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636443625
  nanos: 93225240
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_9"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636443625
  nanos: 217145204
}
message: "Finished listing 1 files in 0.12393021583557129 seconds."
instruction_id: "bundle_9"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:severity: INFO
timestamp {
  seconds: 1636445269
  nanos: 908761024
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445269
  nanos: 909641027
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445269
  nanos: 910156250
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445269
  nanos: 910548210
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445269
  nanos: 925635814
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445269
  nanos: 926180124
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

951b51ae63c832c41e59cadbf9f1b2c19a7e18f36bd7457c888afb3b09a1dc3b
Completed at: 2021-11-09 08:07:50.173638
Duration: 0:56:22.051160
END
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 1676.9517152309418 seconds.
INFO:root:Successfully completed job in 1676.9517152309418 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
Started at: 2021-11-09 08:07:52.340809
/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1935: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported
  temp_location = pcoll.pipeline.options.view_as(
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7faaeacc8560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7faaeacc85f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7faaeacc8d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-cb63d041-5332-4d59-a53b-89985adbc8b7'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7faaeacc4f80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7faaeacc80e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7faaeacc8560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7faaeacc85f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7faaeacc87a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7faaeacc8830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7faaeacc8950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7faaeacc89e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7faaeacc8a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7faaeacc8b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7faaeacc8d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7faaeacc8cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7faaeacc8dd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7faaeacc87a0> ====================
INFO:root:==================== <function expand_gbk at 0x7faaeacc8830> ====================
INFO:root:==================== <function sink_flattens at 0x7faaeacc8950> ====================
INFO:root:==================== <function greedily_fuse at 0x7faaeacc89e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7faaeacc8a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7faaeacc8b00> ====================
INFO:root:==================== <function sort_stages at 0x7faaeacc8d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7faaeacc8cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7faaeacc8dd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 62167
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 62168
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 62169
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 62170
INFO:root:starting control server on port 62167
INFO:root:starting data server on port 62168
INFO:root:starting state server on port 62169
INFO:root:starting logging server on port 62170
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7faaeb97aad0> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7faaeb97aad0> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'c846d54effbfe653aa516c38b8f42df8194f914ce8a4fd595185f0dbe1cd06af', worker_id = worker_0
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'c846d54effbfe653aa516c38b8f42df8194f914ce8a4fd595185f0dbe1cd06af', worker_id = worker_0
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636445280
  nanos: 256294250
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445280
  nanos: 267374038
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636445280
  nanos: 487750053
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445280
  nanos: 498234033
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_5000\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445280
  nanos: 510029315
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445280
  nanos: 511214494
}
message: "Creating insecure control channel for host.docker.internal:62167."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445280
  nanos: 519634962
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445280
  nanos: 523161411
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445280
  nanos: 534803628
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445280
  nanos: 551243782
}
message: "Creating insecure state channel for host.docker.internal:62169."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445280
  nanos: 552025079
}
message: "State channel established."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445280
  nanos: 559474945
}
message: "Creating client data channel for host.docker.internal:62168"
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445280
  nanos: 762422323
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445280
  nanos: 762954950
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445280
  nanos: 820663213
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445280
  nanos: 905564785
}
message: "Refreshing access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445282
  nanos: 49663543
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445282
  nanos: 931079149
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_5000\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_5000`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_5000` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636445283
  nanos: 207373142
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_39dd35a56d1a4f8780a8b1f20fdfd465 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445284
  nanos: 682448148
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_4b9a70e6-b_1636445284_93\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_4b9a70e6-b_1636445284_93"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445284
  nanos: 828015565
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445290
  nanos: 21161317
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445290
  nanos: 264944314
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_4b9a70e6-b_1636445290_878\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_4b9a70e6-b_1636445290_878"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445290
  nanos: 359349250
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445295
  nanos: 559329748
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445295
  nanos: 571111202
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636445295
  nanos: 734277009
}
message: "Finished listing 1 files in 0.16321635246276855 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636446968
  nanos: 20344972
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_5"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446968
  nanos: 152981281
}
message: "Finished listing 1 files in 0.13263797760009766 seconds."
instruction_id: "bundle_5"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446968
  nanos: 352420091
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446968
  nanos: 352851390
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446968
  nanos: 353191375
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446968
  nanos: 353579759
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446968
  nanos: 354937076
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446968
  nanos: 355289936
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

c846d54effbfe653aa516c38b8f42df8194f914ce8a4fd595185f0dbe1cd06af
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 1695.717736005783 seconds.
INFO:root:Successfully completed job in 1695.717736005783 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7faaeacc8560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7faaeacc85f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7faaeacc8d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-f2f1fd41-1f4b-4493-8ad3-7640d9c465c9'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7faaeacc4f80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7faaeacc80e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7faaeacc8560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7faaeacc85f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7faaeacc87a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7faaeacc8830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7faaeacc8950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7faaeacc89e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7faaeacc8a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7faaeacc8b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7faaeacc8d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7faaeacc8cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7faaeacc8dd0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 53996
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 53997
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 53998
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 53999
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function read_to_impulse at 0x7faaeacc8a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7faaeacc8b00> ====================
INFO:root:==================== <function sort_stages at 0x7faaeacc8d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7faaeacc8cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7faaeacc8dd0> ====================
INFO:root:starting control server on port 53996
INFO:root:starting data server on port 53997
INFO:root:starting state server on port 53998
INFO:root:starting logging server on port 53999
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7faad9546910> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7faad9546910> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'7d240d50159eb5a5636a873524e21f8ee3a49946d9e03a3542f5634feb478a39', worker_id = worker_1
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'7d240d50159eb5a5636a873524e21f8ee3a49946d9e03a3542f5634feb478a39', worker_id = worker_1
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636446975
  nanos: 943582534
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446975
  nanos: 953590869
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636446976
  nanos: 172918081
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446976
  nanos: 182340621
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_5000\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446976
  nanos: 192676305
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446976
  nanos: 193823814
}
message: "Creating insecure control channel for host.docker.internal:53996."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446976
  nanos: 202657222
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446976
  nanos: 206476211
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446976
  nanos: 217879056
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446976
  nanos: 234207391
}
message: "Creating insecure state channel for host.docker.internal:53998."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446976
  nanos: 234976291
}
message: "State channel established."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446976
  nanos: 242403030
}
message: "Creating client data channel for host.docker.internal:53997"
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446976
  nanos: 445322275
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446976
  nanos: 445867300
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446976
  nanos: 504212617
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446976
  nanos: 588140487
}
message: "Refreshing access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446977
  nanos: 312357902
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446978
  nanos: 589581489
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_5000\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_5000`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_5000` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636446978
  nanos: 994773387
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_c40c91daa5c243aca8bcbf71723e5a99 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446980
  nanos: 382777690
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_4b9a70e6-b_1636446979_322\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_4b9a70e6-b_1636446979_322"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446980
  nanos: 516628980
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446985
  nanos: 740122079
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446985
  nanos: 969012975
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_4b9a70e6-b_1636446985_646\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_4b9a70e6-b_1636446985_646"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446986
  nanos: 95232725
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446991
  nanos: 267724990
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446991
  nanos: 279359340
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446991
  nanos: 461011171
}
message: "Finished listing 1 files in 0.1817021369934082 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636446999
  nanos: 612404108
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_9"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636446999
  nanos: 766214847
}
message: "Finished listing 1 files in 0.15381884574890137 seconds."
instruction_id: "bundle_9"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:severity: INFO
timestamp {
  seconds: 1636448676
  nanos: 146934986
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448676
  nanos: 147707462
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448676
  nanos: 148493289
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448676
  nanos: 149141311
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448676
  nanos: 167784214
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448676
  nanos: 168661832
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

7d240d50159eb5a5636a873524e21f8ee3a49946d9e03a3542f5634feb478a39
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 1707.6599788665771 seconds.
INFO:root:Successfully completed job in 1707.6599788665771 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
Completed at: 2021-11-09 09:04:36.420929
Duration: 0:56:44.080120
END
Started at: 2021-11-09 09:04:38.605299
/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1935: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported
  temp_location = pcoll.pipeline.options.view_as(
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f9ddad27560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f9ddad275f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f9ddad27d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-56902ba4-52ac-460f-8051-2026d253d49b'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f9ddad23f80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f9ddad270e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f9ddad27560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f9ddad275f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f9ddad277a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f9ddad27830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f9ddad27950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f9ddad279e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f9ddad27a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f9ddad27b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f9ddad27d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f9ddad27cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f9ddad27dd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7f9ddad277a0> ====================
INFO:root:==================== <function expand_gbk at 0x7f9ddad27830> ====================
INFO:root:==================== <function sink_flattens at 0x7f9ddad27950> ====================
INFO:root:==================== <function greedily_fuse at 0x7f9ddad279e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7f9ddad27a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7f9ddad27b00> ====================
INFO:root:==================== <function sort_stages at 0x7f9ddad27d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7f9ddad27cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7f9ddad27dd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 62177
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 62178
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 62179
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 62180
INFO:root:starting control server on port 62177
INFO:root:starting data server on port 62178
INFO:root:starting state server on port 62179
INFO:root:starting logging server on port 62180
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7f9ddb9ec2d0> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7f9ddb9ec2d0> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'112a9352ebcd7812190fc55b02f4f94631c53825732cd7ea475880d47f118c9d', worker_id = worker_0
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'112a9352ebcd7812190fc55b02f4f94631c53825732cd7ea475880d47f118c9d', worker_id = worker_0
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636448686
  nanos: 411830902
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448686
  nanos: 422000408
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636448686
  nanos: 642198085
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448686
  nanos: 651798486
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_7500\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448686
  nanos: 661934137
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448686
  nanos: 663090467
}
message: "Creating insecure control channel for host.docker.internal:62177."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448686
  nanos: 671877622
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448686
  nanos: 675560951
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448686
  nanos: 686453580
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448686
  nanos: 701932191
}
message: "Creating insecure state channel for host.docker.internal:62179."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448686
  nanos: 702698707
}
message: "State channel established."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448686
  nanos: 709704399
}
message: "Creating client data channel for host.docker.internal:62178"
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448686
  nanos: 912781238
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448686
  nanos: 913329362
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448686
  nanos: 971386909
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448687
  nanos: 56188344
}
message: "Refreshing access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448688
  nanos: 222865581
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448689
  nanos: 321744203
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_7500\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_7500`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_7500` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636448689
  nanos: 604283094
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_792c243147094bb1b836f0a3cfeada96 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448691
  nanos: 63819169
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_0e2dabfa-f_1636448690_634\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_0e2dabfa-f_1636448690_634"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448691
  nanos: 208583116
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448696
  nanos: 437022447
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448696
  nanos: 791519403
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_0e2dabfa-f_1636448696_497\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_0e2dabfa-f_1636448696_497"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448696
  nanos: 891026496
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448702
  nanos: 50036191
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448702
  nanos: 61343431
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636448702
  nanos: 230180025
}
message: "Finished listing 1 files in 0.1688838005065918 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636451169
  nanos: 886747360
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_5"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451170
  nanos: 22231578
}
message: "Finished listing 1 files in 0.13547992706298828 seconds."
instruction_id: "bundle_5"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451170
  nanos: 244919776
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451170
  nanos: 245598316
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451170
  nanos: 246012210
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451170
  nanos: 246367216
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451170
  nanos: 259041070
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451170
  nanos: 259622812
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

112a9352ebcd7812190fc55b02f4f94631c53825732cd7ea475880d47f118c9d
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 2491.4630410671234 seconds.
INFO:root:Successfully completed job in 2491.4630410671234 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f9ddad27560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f9ddad275f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f9ddad27d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-be667c07-041f-4651-b41d-9ccdc95a830a'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f9ddad23f80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f9ddad270e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f9ddad27560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f9ddad275f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f9ddad277a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f9ddad27830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f9ddad27950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f9ddad279e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f9ddad27a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f9ddad27b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f9ddad27d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f9ddad27cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f9ddad27dd0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 57928
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 57929
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 57930
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 57931
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function read_to_impulse at 0x7f9ddad27a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7f9ddad27b00> ====================
INFO:root:==================== <function sort_stages at 0x7f9ddad27d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7f9ddad27cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7f9ddad27dd0> ====================
INFO:root:starting control server on port 57928
INFO:root:starting data server on port 57929
INFO:root:starting state server on port 57930
INFO:root:starting logging server on port 57931
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7f9dc9717f10> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7f9dc9717f10> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'8a22fe129d001041ead772d11156ce7bfb5b8f6a2e8677529739e65824c6c616', worker_id = worker_1
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'8a22fe129d001041ead772d11156ce7bfb5b8f6a2e8677529739e65824c6c616', worker_id = worker_1
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636451178
  nanos: 170205354
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451178
  nanos: 180129528
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636451178
  nanos: 401243448
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451178
  nanos: 410777091
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_7500\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451178
  nanos: 420912027
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451178
  nanos: 422053813
}
message: "Creating insecure control channel for host.docker.internal:57928."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451178
  nanos: 431581497
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451178
  nanos: 435161113
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451178
  nanos: 445723056
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451178
  nanos: 461208105
}
message: "Creating insecure state channel for host.docker.internal:57930."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451178
  nanos: 461974620
}
message: "State channel established."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451178
  nanos: 468842744
}
message: "Creating client data channel for host.docker.internal:57929"
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451178
  nanos: 672317981
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451178
  nanos: 672879457
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451178
  nanos: 733281135
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451178
  nanos: 817659378
}
message: "Refreshing access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451180
  nanos: 107142925
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451180
  nanos: 753909111
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_7500\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_7500`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_7500` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636451180
  nanos: 990127086
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_3cd1fe2990974390bd4803f314178af8 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451182
  nanos: 524338483
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_0e2dabfa-f_1636451182_226\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_0e2dabfa-f_1636451182_226"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451182
  nanos: 622876167
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451187
  nanos: 838974475
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451188
  nanos: 153467178
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_0e2dabfa-f_1636451187_340\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_0e2dabfa-f_1636451187_340"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451188
  nanos: 252521991
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451193
  nanos: 427264690
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451193
  nanos: 438489198
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636451193
  nanos: 650745868
}
message: "Finished listing 1 files in 0.21230435371398926 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636453626
  nanos: 832979202
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_10"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453626
  nanos: 969486236
}
message: "Finished listing 1 files in 0.13649463653564453 seconds."
instruction_id: "bundle_10"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453627
  nanos: 193618774
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453627
  nanos: 194185495
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453627
  nanos: 194735288
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453627
  nanos: 195176601
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453627
  nanos: 206495523
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453627
  nanos: 206966876
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

8a22fe129d001041ead772d11156ce7bfb5b8f6a2e8677529739e65824c6c616
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 2456.750210046768 seconds.
INFO:root:Successfully completed job in 2456.750210046768 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
Completed at: 2021-11-09 10:27:07.524216
Duration: 1:22:28.918917
END
Started at: 2021-11-09 10:27:09.770189
/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1935: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported
  temp_location = pcoll.pipeline.options.view_as(
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fb07accf560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fb07accf5f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fb07accfd40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-7278d53f-c208-4c70-9600-fc54e442bdfa'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7fb07accbf80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7fb07accf0e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fb07accf560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fb07accf5f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7fb07accf7a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7fb07accf830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7fb07accf950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7fb07accf9e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7fb07accfa70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7fb07accfb00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fb07accfd40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7fb07accfcb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7fb07accfdd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7fb07accf7a0> ====================
INFO:root:==================== <function expand_gbk at 0x7fb07accf830> ====================
INFO:root:==================== <function sink_flattens at 0x7fb07accf950> ====================
INFO:root:==================== <function greedily_fuse at 0x7fb07accf9e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7fb07accfa70> ====================
INFO:root:==================== <function impulse_to_input at 0x7fb07accfb00> ====================
INFO:root:==================== <function sort_stages at 0x7fb07accfd40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7fb07accfcb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7fb07accfdd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 53569
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 53570
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 53571
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 53572
INFO:root:starting control server on port 53569
INFO:root:starting data server on port 53570
INFO:root:starting state server on port 53571
INFO:root:starting logging server on port 53572
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fb069442990> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fb069442990> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'b11969a818ca0f15ca89262d69b8a7a6a01e09bf2de145528397bd18a3e7014b', worker_id = worker_0
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'b11969a818ca0f15ca89262d69b8a7a6a01e09bf2de145528397bd18a3e7014b', worker_id = worker_0
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636453637
  nanos: 608657360
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453637
  nanos: 619265079
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636453637
  nanos: 838336467
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453637
  nanos: 847909688
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_7500\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453637
  nanos: 858019590
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453637
  nanos: 859148979
}
message: "Creating insecure control channel for host.docker.internal:53569."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453637
  nanos: 867328882
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453637
  nanos: 871033668
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453637
  nanos: 881850719
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453637
  nanos: 898903608
}
message: "Creating insecure state channel for host.docker.internal:53571."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453637
  nanos: 899671792
}
message: "State channel established."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453637
  nanos: 906866550
}
message: "Creating client data channel for host.docker.internal:53570"
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453638
  nanos: 110961198
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453638
  nanos: 111521005
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453638
  nanos: 172023296
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453638
  nanos: 256899356
}
message: "Refreshing access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453638
  nanos: 994749546
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453639
  nanos: 674221038
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_7500\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_7500`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_7500` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636453639
  nanos: 961591482
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_96f73ff670774c37b1e7ee425282b6b5 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453641
  nanos: 318217039
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_8312522e-b_1636453640_323\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_8312522e-b_1636453640_323"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453641
  nanos: 420587301
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453646
  nanos: 614639520
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453647
  nanos: 46810865
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_8312522e-b_1636453646_545\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_8312522e-b_1636453646_545"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453647
  nanos: 185413837
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453652
  nanos: 365276098
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453652
  nanos: 376945257
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636453652
  nanos: 538371086
}
message: "Finished listing 1 files in 0.16147971153259277 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636456175
  nanos: 174201965
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_5"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456175
  nanos: 317784786
}
message: "Finished listing 1 files in 0.14359545707702637 seconds."
instruction_id: "bundle_5"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456175
  nanos: 530311822
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456175
  nanos: 530936241
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456175
  nanos: 531306266
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456175
  nanos: 531773090
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456175
  nanos: 544363975
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456175
  nanos: 544918060
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

b11969a818ca0f15ca89262d69b8a7a6a01e09bf2de145528397bd18a3e7014b
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 2545.8227841854095 seconds.
INFO:root:Successfully completed job in 2545.8227841854095 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fb07accf560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fb07accf5f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fb07accfd40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-a0a3595f-bd5c-4d96-9635-b7052799c88a'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7fb07accbf80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7fb07accf0e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fb07accf560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fb07accf5f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7fb07accf7a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7fb07accf830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7fb07accf950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7fb07accf9e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7fb07accfa70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7fb07accfb00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fb07accfd40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7fb07accfcb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7fb07accfdd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7fb07accf7a0> ====================
INFO:root:==================== <function expand_gbk at 0x7fb07accf830> ====================
INFO:root:==================== <function sink_flattens at 0x7fb07accf950> ====================
INFO:root:==================== <function greedily_fuse at 0x7fb07accf9e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7fb07accfa70> ====================
INFO:root:==================== <function impulse_to_input at 0x7fb07accfb00> ====================
INFO:root:==================== <function sort_stages at 0x7fb07accfd40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7fb07accfcb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7fb07accfdd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 49412
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 49413
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 49414
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 49415
INFO:root:starting control server on port 49412
INFO:root:starting data server on port 49413
INFO:root:starting state server on port 49414
INFO:root:starting logging server on port 49415
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fb07c5aabd0> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fb07c5aabd0> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'539a5e963752f9437b77f5a2febaef7ca3e50230d44d61790ff2f55852157d19', worker_id = worker_1
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'539a5e963752f9437b77f5a2febaef7ca3e50230d44d61790ff2f55852157d19', worker_id = worker_1
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636456183
  nanos: 739758729
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456183
  nanos: 749703407
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636456183
  nanos: 972239255
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456183
  nanos: 981955528
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_7500\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456183
  nanos: 992637395
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456183
  nanos: 993844985
}
message: "Creating insecure control channel for host.docker.internal:49412."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456184
  nanos: 3532648
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456184
  nanos: 7853507
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456184
  nanos: 19180536
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456184
  nanos: 37406921
}
message: "Creating insecure state channel for host.docker.internal:49414."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456184
  nanos: 38289070
}
message: "State channel established."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456184
  nanos: 45840740
}
message: "Creating client data channel for host.docker.internal:49413"
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456184
  nanos: 253023147
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456184
  nanos: 253584384
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456184
  nanos: 315679550
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456184
  nanos: 400780439
}
message: "Refreshing access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456185
  nanos: 549084424
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456186
  nanos: 66172838
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_7500\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_7500`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_7500` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636456186
  nanos: 357946872
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_e995c8e7afc44cbe8c54ed3654043fda does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456187
  nanos: 492253780
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_8312522e-b_1636456187_940\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_8312522e-b_1636456187_940"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456187
  nanos: 626472234
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456192
  nanos: 852602958
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456193
  nanos: 248251914
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_8312522e-b_1636456192_772\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_8312522e-b_1636456192_772"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456193
  nanos: 393787384
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456198
  nanos: 562072753
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456198
  nanos: 573369026
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636456198
  nanos: 743651628
}
message: "Finished listing 1 files in 0.17033052444458008 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636458808
  nanos: 131392955
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_10"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458808
  nanos: 343202114
}
message: "Finished listing 1 files in 0.21177983283996582 seconds."
instruction_id: "bundle_10"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458808
  nanos: 557079076
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458808
  nanos: 557519435
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458808
  nanos: 557829380
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458808
  nanos: 558098316
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458808
  nanos: 560983180
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458808
  nanos: 561481952
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

539a5e963752f9437b77f5a2febaef7ca3e50230d44d61790ff2f55852157d19
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 2632.814002275467 seconds.
INFO:root:Successfully completed job in 2632.814002275467 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
Completed at: 2021-11-09 11:53:29.112967
Duration: 1:26:19.342778
END
Started at: 2021-11-09 11:53:31.402305
/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1935: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported
  temp_location = pcoll.pipeline.options.view_as(
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fbf02e97560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fbf02e975f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fbf02e97d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-d5ce9c45-f71e-4c31-a2f0-56588c2107e1'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7fbf02e93f80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7fbf02e970e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fbf02e97560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fbf02e975f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7fbf02e977a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7fbf02e97830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7fbf02e97950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7fbf02e979e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7fbf02e97a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7fbf02e97b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fbf02e97d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7fbf02e97cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7fbf02e97dd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7fbf02e977a0> ====================
INFO:root:==================== <function expand_gbk at 0x7fbf02e97830> ====================
INFO:root:==================== <function sink_flattens at 0x7fbf02e97950> ====================
INFO:root:==================== <function greedily_fuse at 0x7fbf02e979e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7fbf02e97a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7fbf02e97b00> ====================
INFO:root:==================== <function sort_stages at 0x7fbf02e97d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7fbf02e97cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7fbf02e97dd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 61815
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 61816
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 61817
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 61818
INFO:root:starting control server on port 61815
INFO:root:starting data server on port 61816
INFO:root:starting state server on port 61817
INFO:root:starting logging server on port 61818
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fbef1453dd0> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fbef1453dd0> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'bacd31b17500a0fa72155ce47a9f25e2c20f2f37f3eb94c3bb90dd193f5e5803', worker_id = worker_0
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'bacd31b17500a0fa72155ce47a9f25e2c20f2f37f3eb94c3bb90dd193f5e5803', worker_id = worker_0
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636458819
  nanos: 288050651
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458819
  nanos: 298279285
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636458819
  nanos: 519633769
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458819
  nanos: 529180526
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_7500\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458819
  nanos: 539848566
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458819
  nanos: 541083097
}
message: "Creating insecure control channel for host.docker.internal:61815."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458819
  nanos: 550698041
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458819
  nanos: 554742813
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458819
  nanos: 565705776
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458819
  nanos: 582882642
}
message: "Creating insecure state channel for host.docker.internal:61817."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458819
  nanos: 583735227
}
message: "State channel established."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458819
  nanos: 591750621
}
message: "Creating client data channel for host.docker.internal:61816"
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458819
  nanos: 798624515
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458819
  nanos: 799192428
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458819
  nanos: 860889196
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458819
  nanos: 946237087
}
message: "Refreshing access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458821
  nanos: 119114160
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458822
  nanos: 373303890
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_7500\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_7500`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_7500` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636458822
  nanos: 697160005
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_daabc685f0734847a443d510717e3b18 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458824
  nanos: 257105588
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_dc2d6924-6_1636458823_582\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_dc2d6924-6_1636458823_582"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458824
  nanos: 388377428
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458829
  nanos: 628762960
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458829
  nanos: 991766452
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_dc2d6924-6_1636458829_807\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_dc2d6924-6_1636458829_807"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458830
  nanos: 120124101
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458835
  nanos: 308491706
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458835
  nanos: 320237874
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458835
  nanos: 495556116
}
message: "Finished listing 1 files in 0.1753690242767334 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636458846
  nanos: 882791280
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_4"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636458847
  nanos: 57142019
}
message: "Finished listing 1 files in 0.17435383796691895 seconds."
instruction_id: "bundle_4"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
ERROR:root:severity: ERROR
timestamp {
  seconds: 1636460143
  nanos: 122853279
}
message: "Error in _start_upload while inserting file gs://src_fake_news_bs/added_ttl_json_7500/5f9b3ed1fba29530de77ca004e0098793c9e20ebd6b64382669f5e7c14dda6f4.ttl: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py\", line 706, in _start_upload\n    self._client.objects.Insert(self._insert_request, upload=self._upload)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py\", line 1154, in Insert\n    upload=upload, upload_config=upload_config)\n  File \"/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py\", line 731, in _RunMethod\n    return self.ProcessHttpResponse(method_config, http_response, request)\n  File \"/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py\", line 737, in ProcessHttpResponse\n    self.__ProcessHttpResponse(method_config, http_response, request))\n  File \"/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py\", line 604, in __ProcessHttpResponse\n    http_response, method_config=method_config, request=request)\napitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2F5f9b3ed1fba29530de77ca004e0098793c9e20ebd6b64382669f5e7c14dda6f4.ttl&uploadType=resumable&upload_id=ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA>: response: <{\'content-type\': \'text/plain; charset=utf-8\', \'x-guploader-uploadid\': \'ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA\', \'content-length\': \'0\', \'date\': \'Tue, 09 Nov 2021 12:15:42 GMT\', \'server\': \'UploadServer\', \'alt-svc\': \'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"\', \'status\': \'503\'}>, content <>\n"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:713"
thread: "Thread-3758"

ERROR:root:severity: ERROR
timestamp {
  seconds: 1636460143
  nanos: 378938436
}
message: "Error processing instruction bundle_5. Original traceback is\nTraceback (most recent call last):\n  File \"apache_beam/runners/common.py\", line 1232, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process\n  File \"apache_beam/runners/common.py\", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs\n  File \"02_bigquery_to_bucket_dataflow.py\", line 33, in process\n    yield None\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py\", line 215, in close\n    self._uploader.finish()\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py\", line 733, in finish\n    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py\", line 706, in _start_upload\n    self._client.objects.Insert(self._insert_request, upload=self._upload)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py\", line 1154, in Insert\n    upload=upload, upload_config=upload_config)\n  File \"/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py\", line 731, in _RunMethod\n    return self.ProcessHttpResponse(method_config, http_response, request)\n  File \"/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py\", line 737, in ProcessHttpResponse\n    self.__ProcessHttpResponse(method_config, http_response, request))\n  File \"/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py\", line 604, in __ProcessHttpResponse\n    http_response, method_config=method_config, request=request)\napitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2F5f9b3ed1fba29530de77ca004e0098793c9e20ebd6b64382669f5e7c14dda6f4.ttl&uploadType=resumable&upload_id=ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA>: response: <{\'content-type\': \'text/plain; charset=utf-8\', \'x-guploader-uploadid\': \'ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA\', \'content-length\': \'0\', \'date\': \'Tue, 09 Nov 2021 12:15:42 GMT\', \'server\': \'UploadServer\', \'alt-svc\': \'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"\', \'status\': \'503\'}>, content <>\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 284, in _execute\n    response = task()\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 357, in <lambda>\n    lambda: self.create_worker().do_instruction(request), request)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 603, in do_instruction\n    getattr(request, request_type), request.instruction_id)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 640, in process_bundle\n    bundle_processor.process_bundle(instruction_id))\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 997, in process_bundle\n    element.data)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 222, in process_encoded\n    self.output(decoded_value)\n  File \"apache_beam/runners/worker/operations.py\", line 351, in apache_beam.runners.worker.operations.Operation.output\n  File \"apache_beam/runners/worker/operations.py\", line 353, in apache_beam.runners.worker.operations.Operation.output\n  File \"apache_beam/runners/worker/operations.py\", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive\n  File \"apache_beam/runners/worker/operations.py\", line 712, in apache_beam.runners.worker.operations.DoOperation.process\n  File \"apache_beam/runners/worker/operations.py\", line 713, in apache_beam.runners.worker.operations.DoOperation.process\n  File \"apache_beam/runners/common.py\", line 1234, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n  File \"apache_beam/runners/common.py\", line 1232, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process\n  File \"apache_beam/runners/common.py\", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs\n  File \"apache_beam/runners/worker/operations.py\", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive\n  File \"apache_beam/runners/worker/operations.py\", line 712, in apache_beam.runners.worker.operations.DoOperation.process\n  File \"apache_beam/runners/worker/operations.py\", line 713, in apache_beam.runners.worker.operations.DoOperation.process\n  File \"apache_beam/runners/common.py\", line 1234, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n  File \"apache_beam/runners/common.py\", line 1232, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process\n  File \"apache_beam/runners/common.py\", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs\n  File \"apache_beam/runners/worker/operations.py\", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive\n  File \"apache_beam/runners/worker/operations.py\", line 712, in apache_beam.runners.worker.operations.DoOperation.process\n  File \"apache_beam/runners/worker/operations.py\", line 713, in apache_beam.runners.worker.operations.DoOperation.process\n  File \"apache_beam/runners/common.py\", line 1234, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n  File \"apache_beam/runners/common.py\", line 1232, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 752, in apache_beam.runners.common.PerWindowInvoker.invoke_process\n  File \"apache_beam/runners/common.py\", line 875, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\n  File \"apache_beam/runners/common.py\", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs\n  File \"apache_beam/runners/worker/operations.py\", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive\n  File \"apache_beam/runners/worker/operations.py\", line 712, in apache_beam.runners.worker.operations.DoOperation.process\n  File \"apache_beam/runners/worker/operations.py\", line 713, in apache_beam.runners.worker.operations.DoOperation.process\n  File \"apache_beam/runners/common.py\", line 1234, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n  File \"apache_beam/runners/common.py\", line 1232, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process\n  File \"apache_beam/runners/common.py\", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs\n  File \"apache_beam/runners/worker/operations.py\", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive\n  File \"apache_beam/runners/worker/operations.py\", line 712, in apache_beam.runners.worker.operations.DoOperation.process\n  File \"apache_beam/runners/worker/operations.py\", line 713, in apache_beam.runners.worker.operations.DoOperation.process\n  File \"apache_beam/runners/common.py\", line 1234, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 1315, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n  File \"apache_beam/runners/common.py\", line 1232, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process\n  File \"apache_beam/runners/common.py\", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs\n  File \"02_bigquery_to_bucket_dataflow.py\", line 33, in process\n    yield None\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py\", line 215, in close\n    self._uploader.finish()\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py\", line 733, in finish\n    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py\", line 706, in _start_upload\n    self._client.objects.Insert(self._insert_request, upload=self._upload)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py\", line 1154, in Insert\n    upload=upload, upload_config=upload_config)\n  File \"/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py\", line 731, in _RunMethod\n    return self.ProcessHttpResponse(method_config, http_response, request)\n  File \"/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py\", line 737, in ProcessHttpResponse\n    self.__ProcessHttpResponse(method_config, http_response, request))\n  File \"/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py\", line 604, in __ProcessHttpResponse\n    http_response, method_config=method_config, request=request)\nRuntimeError: apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2F5f9b3ed1fba29530de77ca004e0098793c9e20ebd6b64382669f5e7c14dda6f4.ttl&uploadType=resumable&upload_id=ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA>: response: <{\'content-type\': \'text/plain; charset=utf-8\', \'x-guploader-uploadid\': \'ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA\', \'content-length\': \'0\', \'date\': \'Tue, 09 Nov 2021 12:15:42 GMT\', \'server\': \'UploadServer\', \'alt-svc\': \'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"\', \'status\': \'503\'}>, content <> [while running \'Push to GraphDB\']\n\n"
instruction_id: "bundle_5"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:291"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460143
  nanos: 387327432
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460143
  nanos: 388205051
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460143
  nanos: 388921260
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460143
  nanos: 389445781
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460143
  nanos: 393028020
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460143
  nanos: 393602132
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

bacd31b17500a0fa72155ce47a9f25e2c20f2f37f3eb94c3bb90dd193f5e5803
ERROR:apache_beam.runners.portability.local_job_service:Error running pipeline.
Traceback (most recent call last):
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/local_job_service.py", line 274, in _run_job
    self._pipeline_proto)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 206, in run_via_runner_api
    return self.run_stages(stage_context, stages)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 385, in run_stages
    runner_execution_context, bundle_context_manager)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 653, in _run_stage
    bundle_manager))
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 770, in _run_bundle
    data_input, data_output, input_timers, expected_timer_output)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 1120, in process_bundle
    raise RuntimeError(result.error)
RuntimeError: Traceback (most recent call last):
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "02_bigquery_to_bucket_dataflow.py", line 33, in process
    yield None
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py", line 215, in close
    self._uploader.finish()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 733, in finish
    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 706, in _start_upload
    self._client.objects.Insert(self._insert_request, upload=self._upload)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py", line 1154, in Insert
    upload=upload, upload_config=upload_config)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 731, in _RunMethod
    return self.ProcessHttpResponse(method_config, http_response, request)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 737, in ProcessHttpResponse
    self.__ProcessHttpResponse(method_config, http_response, request))
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 604, in __ProcessHttpResponse
    http_response, method_config=method_config, request=request)
apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2F5f9b3ed1fba29530de77ca004e0098793c9e20ebd6b64382669f5e7c14dda6f4.ttl&uploadType=resumable&upload_id=ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA>: response: <{'content-type': 'text/plain; charset=utf-8', 'x-guploader-uploadid': 'ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA', 'content-length': '0', 'date': 'Tue, 09 Nov 2021 12:15:42 GMT', 'server': 'UploadServer', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"', 'status': '503'}>, content <>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 284, in _execute
    response = task()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 357, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 603, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 640, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py", line 997, in process_bundle
    element.data)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py", line 222, in process_encoded
    self.output(decoded_value)
  File "apache_beam/runners/worker/operations.py", line 351, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 353, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 752, in apache_beam.runners.common.PerWindowInvoker.invoke_process
  File "apache_beam/runners/common.py", line 875, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1315, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "02_bigquery_to_bucket_dataflow.py", line 33, in process
    yield None
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py", line 215, in close
    self._uploader.finish()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 733, in finish
    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 706, in _start_upload
    self._client.objects.Insert(self._insert_request, upload=self._upload)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py", line 1154, in Insert
    upload=upload, upload_config=upload_config)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 731, in _RunMethod
    return self.ProcessHttpResponse(method_config, http_response, request)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 737, in ProcessHttpResponse
    self.__ProcessHttpResponse(method_config, http_response, request))
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 604, in __ProcessHttpResponse
    http_response, method_config=method_config, request=request)
RuntimeError: apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2F5f9b3ed1fba29530de77ca004e0098793c9e20ebd6b64382669f5e7c14dda6f4.ttl&uploadType=resumable&upload_id=ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA>: response: <{'content-type': 'text/plain; charset=utf-8', 'x-guploader-uploadid': 'ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA', 'content-length': '0', 'date': 'Tue, 09 Nov 2021 12:15:42 GMT', 'server': 'UploadServer', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"', 'status': '503'}>, content <> [while running 'Push to GraphDB']

ERROR:root:Traceback (most recent call last):
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/local_job_service.py", line 274, in _run_job
    self._pipeline_proto)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 206, in run_via_runner_api
    return self.run_stages(stage_context, stages)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 385, in run_stages
    runner_execution_context, bundle_context_manager)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 653, in _run_stage
    bundle_manager))
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 770, in _run_bundle
    data_input, data_output, input_timers, expected_timer_output)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 1120, in process_bundle
    raise RuntimeError(result.error)
RuntimeError: Traceback (most recent call last):
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "02_bigquery_to_bucket_dataflow.py", line 33, in process
    yield None
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py", line 215, in close
    self._uploader.finish()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 733, in finish
    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 706, in _start_upload
    self._client.objects.Insert(self._insert_request, upload=self._upload)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py", line 1154, in Insert
    upload=upload, upload_config=upload_config)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 731, in _RunMethod
    return self.ProcessHttpResponse(method_config, http_response, request)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 737, in ProcessHttpResponse
    self.__ProcessHttpResponse(method_config, http_response, request))
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 604, in __ProcessHttpResponse
    http_response, method_config=method_config, request=request)
apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2F5f9b3ed1fba29530de77ca004e0098793c9e20ebd6b64382669f5e7c14dda6f4.ttl&uploadType=resumable&upload_id=ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA>: response: <{'content-type': 'text/plain; charset=utf-8', 'x-guploader-uploadid': 'ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA', 'content-length': '0', 'date': 'Tue, 09 Nov 2021 12:15:42 GMT', 'server': 'UploadServer', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"', 'status': '503'}>, content <>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 284, in _execute
    response = task()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 357, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 603, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 640, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py", line 997, in process_bundle
    element.data)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py", line 222, in process_encoded
    self.output(decoded_value)
  File "apache_beam/runners/worker/operations.py", line 351, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 353, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 752, in apache_beam.runners.common.PerWindowInvoker.invoke_process
  File "apache_beam/runners/common.py", line 875, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1315, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "02_bigquery_to_bucket_dataflow.py", line 33, in process
    yield None
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py", line 215, in close
    self._uploader.finish()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 733, in finish
    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 706, in _start_upload
    self._client.objects.Insert(self._insert_request, upload=self._upload)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py", line 1154, in Insert
    upload=upload, upload_config=upload_config)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 731, in _RunMethod
    return self.ProcessHttpResponse(method_config, http_response, request)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 737, in ProcessHttpResponse
    self.__ProcessHttpResponse(method_config, http_response, request))
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 604, in __ProcessHttpResponse
    http_response, method_config=method_config, request=request)
RuntimeError: apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2F5f9b3ed1fba29530de77ca004e0098793c9e20ebd6b64382669f5e7c14dda6f4.ttl&uploadType=resumable&upload_id=ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA>: response: <{'content-type': 'text/plain; charset=utf-8', 'x-guploader-uploadid': 'ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA', 'content-length': '0', 'date': 'Tue, 09 Nov 2021 12:15:42 GMT', 'server': 'UploadServer', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"', 'status': '503'}>, content <> [while running 'Push to GraphDB']


ERROR:root:Error running pipeline.
Traceback (most recent call last):
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/local_job_service.py", line 274, in _run_job
    self._pipeline_proto)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 206, in run_via_runner_api
    return self.run_stages(stage_context, stages)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 385, in run_stages
    runner_execution_context, bundle_context_manager)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 653, in _run_stage
    bundle_manager))
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 770, in _run_bundle
    data_input, data_output, input_timers, expected_timer_output)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 1120, in process_bundle
    raise RuntimeError(result.error)
RuntimeError: Traceback (most recent call last):
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "02_bigquery_to_bucket_dataflow.py", line 33, in process
    yield None
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py", line 215, in close
    self._uploader.finish()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 733, in finish
    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 706, in _start_upload
    self._client.objects.Insert(self._insert_request, upload=self._upload)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py", line 1154, in Insert
    upload=upload, upload_config=upload_config)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 731, in _RunMethod
    return self.ProcessHttpResponse(method_config, http_response, request)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 737, in ProcessHttpResponse
    self.__ProcessHttpResponse(method_config, http_response, request))
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 604, in __ProcessHttpResponse
    http_response, method_config=method_config, request=request)
apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2F5f9b3ed1fba29530de77ca004e0098793c9e20ebd6b64382669f5e7c14dda6f4.ttl&uploadType=resumable&upload_id=ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA>: response: <{'content-type': 'text/plain; charset=utf-8', 'x-guploader-uploadid': 'ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA', 'content-length': '0', 'date': 'Tue, 09 Nov 2021 12:15:42 GMT', 'server': 'UploadServer', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"', 'status': '503'}>, content <>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 284, in _execute
    response = task()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 357, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 603, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 640, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py", line 997, in process_bundle
    element.data)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py", line 222, in process_encoded
    self.output(decoded_value)
  File "apache_beam/runners/worker/operations.py", line 351, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 353, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 752, in apache_beam.runners.common.PerWindowInvoker.invoke_process
  File "apache_beam/runners/common.py", line 875, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1315, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "02_bigquery_to_bucket_dataflow.py", line 33, in process
    yield None
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py", line 215, in close
    self._uploader.finish()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 733, in finish
    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 706, in _start_upload
    self._client.objects.Insert(self._insert_request, upload=self._upload)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py", line 1154, in Insert
    upload=upload, upload_config=upload_config)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 731, in _RunMethod
    return self.ProcessHttpResponse(method_config, http_response, request)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 737, in ProcessHttpResponse
    self.__ProcessHttpResponse(method_config, http_response, request))
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 604, in __ProcessHttpResponse
    http_response, method_config=method_config, request=request)
RuntimeError: apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2F5f9b3ed1fba29530de77ca004e0098793c9e20ebd6b64382669f5e7c14dda6f4.ttl&uploadType=resumable&upload_id=ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA>: response: <{'content-type': 'text/plain; charset=utf-8', 'x-guploader-uploadid': 'ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA', 'content-length': '0', 'date': 'Tue, 09 Nov 2021 12:15:42 GMT', 'server': 'UploadServer', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"', 'status': '503'}>, content <> [while running 'Push to GraphDB']

INFO:apache_beam.runners.portability.portable_runner:Job state changed to FAILED
Traceback (most recent call last):
  File "02_bigquery_to_bucket_dataflow.py", line 419, in <module>
    run()
  File "02_bigquery_to_bucket_dataflow.py", line 403, in run
    | 'Push to GraphDB' >> beam.ParDo(WriteTurtleJSON())
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/pipeline.py", line 587, in __exit__
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/local_job_service.py", line 274, in _run_job
    self._pipeline_proto)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 206, in run_via_runner_api
    return self.run_stages(stage_context, stages)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 385, in run_stages
    runner_execution_context, bundle_context_manager)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 653, in _run_stage
    bundle_manager))
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 770, in _run_bundle
    data_input, data_output, input_timers, expected_timer_output)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 1120, in process_bundle
    raise RuntimeError(result.error)
RuntimeError: Traceback (most recent call last):
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "02_bigquery_to_bucket_dataflow.py", line 33, in process
    yield None
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py", line 215, in close
    self._uploader.finish()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 733, in finish
    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 706, in _start_upload
    self._client.objects.Insert(self._insert_request, upload=self._upload)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py", line 1154, in Insert
    upload=upload, upload_config=upload_config)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 731, in _RunMethod
    return self.ProcessHttpResponse(method_config, http_response, request)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 737, in ProcessHttpResponse
    self.__ProcessHttpResponse(method_config, http_response, request))
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 604, in __ProcessHttpResponse
    http_response, method_config=method_config, request=request)
apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2F5f9b3ed1fba29530de77ca004e0098793c9e20ebd6b64382669f5e7c14dda6f4.ttl&uploadType=resumable&upload_id=ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA>: response: <{'content-type': 'text/plain; charset=utf-8', 'x-guploader-uploadid': 'ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA', 'content-length': '0', 'date': 'Tue, 09 Nov 2021 12:15:42 GMT', 'server': 'UploadServer', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"', 'status': '503'}>, content <>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 284, in _execute
    response = task()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 357, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 603, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 640, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py", line 997, in process_bundle
    element.data)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py", line 222, in process_encoded
    self.output(decoded_value)
  File "apache_beam/runners/worker/operations.py", line 351, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 353, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 752, in apache_beam.runners.common.PerWindowInvoker.invoke_process
  File "apache_beam/runners/common.py", line 875, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1315, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "02_bigquery_to_bucket_dataflow.py", line 33, in process
    yield None
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py", line 215, in close
    self._uploader.finish()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 733, in finish
    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 706, in _start_upload
    self._client.objects.Insert(self._insert_request, upload=self._upload)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py", line 1154, in Insert
    upload=upload, upload_config=upload_config)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 731, in _RunMethod
    return self.ProcessHttpResponse(method_config, http_response, request)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 737, in ProcessHttpResponse
    self.__ProcessHttpResponse(method_config, http_response, request))
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 604, in __ProcessHttpResponse
    http_response, method_config=method_config, request=request)
RuntimeError: apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2F5f9b3ed1fba29530de77ca004e0098793c9e20ebd6b64382669f5e7c14dda6f4.ttl&uploadType=resumable&upload_id=ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA>: response: <{'content-type': 'text/plain; charset=utf-8', 'x-guploader-uploadid': 'ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA', 'content-length': '0', 'date': 'Tue, 09 Nov 2021 12:15:42 GMT', 'server': 'UploadServer', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"', 'status': '503'}>, content <> [while running 'Push to GraphDB']


    self.result.wait_until_finish()
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/portable_runner.py", line 600, in wait_until_finish
    raise self._runtime_exception
RuntimeError: Pipeline job-d5ce9c45-f71e-4c31-a2f0-56588c2107e1 failed in state FAILED: Error running pipeline.
Traceback (most recent call last):
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/local_job_service.py", line 274, in _run_job
    self._pipeline_proto)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 206, in run_via_runner_api
    return self.run_stages(stage_context, stages)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 385, in run_stages
    runner_execution_context, bundle_context_manager)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 653, in _run_stage
    bundle_manager))
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 770, in _run_bundle
    data_input, data_output, input_timers, expected_timer_output)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 1120, in process_bundle
    raise RuntimeError(result.error)
RuntimeError: Traceback (most recent call last):
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "02_bigquery_to_bucket_dataflow.py", line 33, in process
    yield None
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py", line 215, in close
    self._uploader.finish()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 733, in finish
    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 706, in _start_upload
    self._client.objects.Insert(self._insert_request, upload=self._upload)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py", line 1154, in Insert
    upload=upload, upload_config=upload_config)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 731, in _RunMethod
    return self.ProcessHttpResponse(method_config, http_response, request)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 737, in ProcessHttpResponse
    self.__ProcessHttpResponse(method_config, http_response, request))
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 604, in __ProcessHttpResponse
    http_response, method_config=method_config, request=request)
apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2F5f9b3ed1fba29530de77ca004e0098793c9e20ebd6b64382669f5e7c14dda6f4.ttl&uploadType=resumable&upload_id=ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA>: response: <{'content-type': 'text/plain; charset=utf-8', 'x-guploader-uploadid': 'ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA', 'content-length': '0', 'date': 'Tue, 09 Nov 2021 12:15:42 GMT', 'server': 'UploadServer', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"', 'status': '503'}>, content <>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 284, in _execute
    response = task()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 357, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 603, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 640, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py", line 997, in process_bundle
    element.data)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py", line 222, in process_encoded
    self.output(decoded_value)
  File "apache_beam/runners/worker/operations.py", line 351, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 353, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 752, in apache_beam.runners.common.PerWindowInvoker.invoke_process
  File "apache_beam/runners/common.py", line 875, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1315, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "02_bigquery_to_bucket_dataflow.py", line 33, in process
    yield None
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py", line 215, in close
    self._uploader.finish()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 733, in finish
    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 706, in _start_upload
    self._client.objects.Insert(self._insert_request, upload=self._upload)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py", line 1154, in Insert
    upload=upload, upload_config=upload_config)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 731, in _RunMethod
    return self.ProcessHttpResponse(method_config, http_response, request)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 737, in ProcessHttpResponse
    self.__ProcessHttpResponse(method_config, http_response, request))
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 604, in __ProcessHttpResponse
    http_response, method_config=method_config, request=request)
RuntimeError: apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2F5f9b3ed1fba29530de77ca004e0098793c9e20ebd6b64382669f5e7c14dda6f4.ttl&uploadType=resumable&upload_id=ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA>: response: <{'content-type': 'text/plain; charset=utf-8', 'x-guploader-uploadid': 'ADPycdt84ruoelaM86f2Hwyw0kzyOqUKqP0uaG2tSuh5kD7imQaIjD2aHTbvT9SXaHUFTDBxuYWysmMoN17QHIZjuehIOSWokA', 'content-length': '0', 'date': 'Tue, 09 Nov 2021 12:15:42 GMT', 'server': 'UploadServer', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"', 'status': '503'}>, content <> [while running 'Push to GraphDB']

Started at: 2021-11-09 12:15:45.925729
/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1935: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported
  temp_location = pcoll.pipeline.options.view_as(
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fb8caef1560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fb8caef15f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fb8caef1d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-46659e5a-11a9-4336-8158-f487701e8645'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7fb8caeedf80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7fb8caef10e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fb8caef1560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fb8caef15f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7fb8caef17a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7fb8caef1830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7fb8caef1950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7fb8caef19e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7fb8caef1a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7fb8caef1b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fb8caef1d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7fb8caef1cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7fb8caef1dd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7fb8caef17a0> ====================
INFO:root:==================== <function expand_gbk at 0x7fb8caef1830> ====================
INFO:root:==================== <function sink_flattens at 0x7fb8caef1950> ====================
INFO:root:==================== <function greedily_fuse at 0x7fb8caef19e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7fb8caef1a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7fb8caef1b00> ====================
INFO:root:==================== <function sort_stages at 0x7fb8caef1d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7fb8caef1cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7fb8caef1dd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 51694
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 51695
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 51696
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 51697
INFO:root:starting control server on port 51694
INFO:root:starting data server on port 51695
INFO:root:starting state server on port 51696
INFO:root:starting logging server on port 51697
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fb8b9359850> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fb8b9359850> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'befaca90bfdd4889fdae146c218b4ad4e1140f8e9ba48b7187a3288c05f60f97', worker_id = worker_0
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'befaca90bfdd4889fdae146c218b4ad4e1140f8e9ba48b7187a3288c05f60f97', worker_id = worker_0
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636460153
  nanos: 616508483
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460153
  nanos: 626105785
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636460153
  nanos: 845107316
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460153
  nanos: 854741096
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_7500\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460153
  nanos: 865378141
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460153
  nanos: 866644620
}
message: "Creating insecure control channel for host.docker.internal:51694."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460153
  nanos: 876264095
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460153
  nanos: 880265474
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460153
  nanos: 890862941
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460153
  nanos: 907647132
}
message: "Creating insecure state channel for host.docker.internal:51696."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460153
  nanos: 908461093
}
message: "State channel established."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460153
  nanos: 916313648
}
message: "Creating client data channel for host.docker.internal:51695"
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460154
  nanos: 118206024
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460154
  nanos: 118807792
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460154
  nanos: 179559946
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460154
  nanos: 263848543
}
message: "Refreshing access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460154
  nanos: 842562675
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460155
  nanos: 355045795
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_7500\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_7500`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_7500` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636460155
  nanos: 646944284
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_573855a5172c4088b792d59223920119 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460157
  nanos: 100685834
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_50102860-1_1636460156_531\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_50102860-1_1636460156_531"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460157
  nanos: 205368280
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460162
  nanos: 408970355
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460162
  nanos: 730067014
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_50102860-1_1636460162_402\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_50102860-1_1636460162_402"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460162
  nanos: 826697587
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460167
  nanos: 963382720
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460167
  nanos: 975400209
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636460168
  nanos: 148978710
}
message: "Finished listing 1 files in 0.1736292839050293 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
ERROR:root:severity: ERROR
timestamp {
  seconds: 1636461038
  nanos: 608872890
}
message: "Error in _start_upload while inserting file gs://src_fake_news_bs/added_ttl_json_7500/cdf241bb6217b891559f0e87003506458a1d1903a958648cbfa29d1a625ea423.ttl: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py\", line 706, in _start_upload\n    self._client.objects.Insert(self._insert_request, upload=self._upload)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py\", line 1154, in Insert\n    upload=upload, upload_config=upload_config)\n  File \"/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py\", line 731, in _RunMethod\n    return self.ProcessHttpResponse(method_config, http_response, request)\n  File \"/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py\", line 737, in ProcessHttpResponse\n    self.__ProcessHttpResponse(method_config, http_response, request))\n  File \"/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py\", line 604, in __ProcessHttpResponse\n    http_response, method_config=method_config, request=request)\napitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2Fcdf241bb6217b891559f0e87003506458a1d1903a958648cbfa29d1a625ea423.ttl&uploadType=resumable&upload_id=ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo>: response: <{\'content-type\': \'text/plain; charset=utf-8\', \'x-guploader-uploadid\': \'ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo\', \'content-length\': \'0\', \'date\': \'Tue, 09 Nov 2021 12:30:38 GMT\', \'server\': \'UploadServer\', \'alt-svc\': \'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"\', \'status\': \'503\'}>, content <>\n"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:713"
thread: "Thread-2478"

ERROR:root:severity: ERROR
timestamp {
  seconds: 1636461038
  nanos: 808286666
}
message: "Error processing instruction bundle_3. Original traceback is\nTraceback (most recent call last):\n  File \"apache_beam/runners/common.py\", line 1232, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process\n  File \"apache_beam/runners/common.py\", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs\n  File \"02_bigquery_to_bucket_dataflow.py\", line 33, in process\n    yield None\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py\", line 215, in close\n    self._uploader.finish()\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py\", line 733, in finish\n    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py\", line 706, in _start_upload\n    self._client.objects.Insert(self._insert_request, upload=self._upload)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py\", line 1154, in Insert\n    upload=upload, upload_config=upload_config)\n  File \"/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py\", line 731, in _RunMethod\n    return self.ProcessHttpResponse(method_config, http_response, request)\n  File \"/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py\", line 737, in ProcessHttpResponse\n    self.__ProcessHttpResponse(method_config, http_response, request))\n  File \"/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py\", line 604, in __ProcessHttpResponse\n    http_response, method_config=method_config, request=request)\napitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2Fcdf241bb6217b891559f0e87003506458a1d1903a958648cbfa29d1a625ea423.ttl&uploadType=resumable&upload_id=ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo>: response: <{\'content-type\': \'text/plain; charset=utf-8\', \'x-guploader-uploadid\': \'ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo\', \'content-length\': \'0\', \'date\': \'Tue, 09 Nov 2021 12:30:38 GMT\', \'server\': \'UploadServer\', \'alt-svc\': \'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"\', \'status\': \'503\'}>, content <>\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 284, in _execute\n    response = task()\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 357, in <lambda>\n    lambda: self.create_worker().do_instruction(request), request)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 603, in do_instruction\n    getattr(request, request_type), request.instruction_id)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 640, in process_bundle\n    bundle_processor.process_bundle(instruction_id))\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 997, in process_bundle\n    element.data)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 222, in process_encoded\n    self.output(decoded_value)\n  File \"apache_beam/runners/worker/operations.py\", line 351, in apache_beam.runners.worker.operations.Operation.output\n  File \"apache_beam/runners/worker/operations.py\", line 353, in apache_beam.runners.worker.operations.Operation.output\n  File \"apache_beam/runners/worker/operations.py\", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive\n  File \"apache_beam/runners/worker/operations.py\", line 712, in apache_beam.runners.worker.operations.DoOperation.process\n  File \"apache_beam/runners/worker/operations.py\", line 713, in apache_beam.runners.worker.operations.DoOperation.process\n  File \"apache_beam/runners/common.py\", line 1234, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n  File \"apache_beam/runners/common.py\", line 1232, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process\n  File \"apache_beam/runners/common.py\", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs\n  File \"apache_beam/runners/worker/operations.py\", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive\n  File \"apache_beam/runners/worker/operations.py\", line 712, in apache_beam.runners.worker.operations.DoOperation.process\n  File \"apache_beam/runners/worker/operations.py\", line 713, in apache_beam.runners.worker.operations.DoOperation.process\n  File \"apache_beam/runners/common.py\", line 1234, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n  File \"apache_beam/runners/common.py\", line 1232, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process\n  File \"apache_beam/runners/common.py\", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs\n  File \"apache_beam/runners/worker/operations.py\", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive\n  File \"apache_beam/runners/worker/operations.py\", line 712, in apache_beam.runners.worker.operations.DoOperation.process\n  File \"apache_beam/runners/worker/operations.py\", line 713, in apache_beam.runners.worker.operations.DoOperation.process\n  File \"apache_beam/runners/common.py\", line 1234, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n  File \"apache_beam/runners/common.py\", line 1232, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 752, in apache_beam.runners.common.PerWindowInvoker.invoke_process\n  File \"apache_beam/runners/common.py\", line 875, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\n  File \"apache_beam/runners/common.py\", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs\n  File \"apache_beam/runners/worker/operations.py\", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive\n  File \"apache_beam/runners/worker/operations.py\", line 712, in apache_beam.runners.worker.operations.DoOperation.process\n  File \"apache_beam/runners/worker/operations.py\", line 713, in apache_beam.runners.worker.operations.DoOperation.process\n  File \"apache_beam/runners/common.py\", line 1234, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n  File \"apache_beam/runners/common.py\", line 1232, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process\n  File \"apache_beam/runners/common.py\", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs\n  File \"apache_beam/runners/worker/operations.py\", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive\n  File \"apache_beam/runners/worker/operations.py\", line 712, in apache_beam.runners.worker.operations.DoOperation.process\n  File \"apache_beam/runners/worker/operations.py\", line 713, in apache_beam.runners.worker.operations.DoOperation.process\n  File \"apache_beam/runners/common.py\", line 1234, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 1315, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n  File \"apache_beam/runners/common.py\", line 1232, in apache_beam.runners.common.DoFnRunner.process\n  File \"apache_beam/runners/common.py\", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process\n  File \"apache_beam/runners/common.py\", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs\n  File \"02_bigquery_to_bucket_dataflow.py\", line 33, in process\n    yield None\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py\", line 215, in close\n    self._uploader.finish()\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py\", line 733, in finish\n    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py\", line 706, in _start_upload\n    self._client.objects.Insert(self._insert_request, upload=self._upload)\n  File \"/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py\", line 1154, in Insert\n    upload=upload, upload_config=upload_config)\n  File \"/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py\", line 731, in _RunMethod\n    return self.ProcessHttpResponse(method_config, http_response, request)\n  File \"/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py\", line 737, in ProcessHttpResponse\n    self.__ProcessHttpResponse(method_config, http_response, request))\n  File \"/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py\", line 604, in __ProcessHttpResponse\n    http_response, method_config=method_config, request=request)\nRuntimeError: apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2Fcdf241bb6217b891559f0e87003506458a1d1903a958648cbfa29d1a625ea423.ttl&uploadType=resumable&upload_id=ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo>: response: <{\'content-type\': \'text/plain; charset=utf-8\', \'x-guploader-uploadid\': \'ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo\', \'content-length\': \'0\', \'date\': \'Tue, 09 Nov 2021 12:30:38 GMT\', \'server\': \'UploadServer\', \'alt-svc\': \'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"\', \'status\': \'503\'}>, content <> [while running \'Push to GraphDB\']\n\n"
instruction_id: "bundle_3"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:291"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461038
  nanos: 815653800
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461038
  nanos: 816319465
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461038
  nanos: 817044496
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461038
  nanos: 817656040
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461038
  nanos: 828670263
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461038
  nanos: 829463958
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

befaca90bfdd4889fdae146c218b4ad4e1140f8e9ba48b7187a3288c05f60f97
ERROR:apache_beam.runners.portability.local_job_service:Error running pipeline.
Traceback (most recent call last):
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/local_job_service.py", line 274, in _run_job
    self._pipeline_proto)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 206, in run_via_runner_api
    return self.run_stages(stage_context, stages)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 385, in run_stages
    runner_execution_context, bundle_context_manager)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 653, in _run_stage
    bundle_manager))
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 770, in _run_bundle
    data_input, data_output, input_timers, expected_timer_output)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 1120, in process_bundle
    raise RuntimeError(result.error)
RuntimeError: Traceback (most recent call last):
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "02_bigquery_to_bucket_dataflow.py", line 33, in process
    yield None
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py", line 215, in close
    self._uploader.finish()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 733, in finish
    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 706, in _start_upload
    self._client.objects.Insert(self._insert_request, upload=self._upload)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py", line 1154, in Insert
    upload=upload, upload_config=upload_config)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 731, in _RunMethod
    return self.ProcessHttpResponse(method_config, http_response, request)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 737, in ProcessHttpResponse
    self.__ProcessHttpResponse(method_config, http_response, request))
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 604, in __ProcessHttpResponse
    http_response, method_config=method_config, request=request)
apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2Fcdf241bb6217b891559f0e87003506458a1d1903a958648cbfa29d1a625ea423.ttl&uploadType=resumable&upload_id=ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo>: response: <{'content-type': 'text/plain; charset=utf-8', 'x-guploader-uploadid': 'ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo', 'content-length': '0', 'date': 'Tue, 09 Nov 2021 12:30:38 GMT', 'server': 'UploadServer', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"', 'status': '503'}>, content <>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 284, in _execute
    response = task()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 357, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 603, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 640, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py", line 997, in process_bundle
    element.data)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py", line 222, in process_encoded
    self.output(decoded_value)
  File "apache_beam/runners/worker/operations.py", line 351, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 353, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 752, in apache_beam.runners.common.PerWindowInvoker.invoke_process
  File "apache_beam/runners/common.py", line 875, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1315, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "02_bigquery_to_bucket_dataflow.py", line 33, in process
    yield None
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py", line 215, in close
    self._uploader.finish()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 733, in finish
    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 706, in _start_upload
    self._client.objects.Insert(self._insert_request, upload=self._upload)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py", line 1154, in Insert
    upload=upload, upload_config=upload_config)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 731, in _RunMethod
    return self.ProcessHttpResponse(method_config, http_response, request)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 737, in ProcessHttpResponse
    self.__ProcessHttpResponse(method_config, http_response, request))
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 604, in __ProcessHttpResponse
    http_response, method_config=method_config, request=request)
RuntimeError: apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2Fcdf241bb6217b891559f0e87003506458a1d1903a958648cbfa29d1a625ea423.ttl&uploadType=resumable&upload_id=ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo>: response: <{'content-type': 'text/plain; charset=utf-8', 'x-guploader-uploadid': 'ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo', 'content-length': '0', 'date': 'Tue, 09 Nov 2021 12:30:38 GMT', 'server': 'UploadServer', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"', 'status': '503'}>, content <> [while running 'Push to GraphDB']

ERROR:root:Traceback (most recent call last):
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/local_job_service.py", line 274, in _run_job
    self._pipeline_proto)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 206, in run_via_runner_api
    return self.run_stages(stage_context, stages)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 385, in run_stages
    runner_execution_context, bundle_context_manager)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 653, in _run_stage
    bundle_manager))
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 770, in _run_bundle
    data_input, data_output, input_timers, expected_timer_output)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 1120, in process_bundle
    raise RuntimeError(result.error)
RuntimeError: Traceback (most recent call last):
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "02_bigquery_to_bucket_dataflow.py", line 33, in process
    yield None
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py", line 215, in close
    self._uploader.finish()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 733, in finish
    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 706, in _start_upload
    self._client.objects.Insert(self._insert_request, upload=self._upload)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py", line 1154, in Insert
    upload=upload, upload_config=upload_config)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 731, in _RunMethod
    return self.ProcessHttpResponse(method_config, http_response, request)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 737, in ProcessHttpResponse
    self.__ProcessHttpResponse(method_config, http_response, request))
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 604, in __ProcessHttpResponse
    http_response, method_config=method_config, request=request)
apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2Fcdf241bb6217b891559f0e87003506458a1d1903a958648cbfa29d1a625ea423.ttl&uploadType=resumable&upload_id=ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo>: response: <{'content-type': 'text/plain; charset=utf-8', 'x-guploader-uploadid': 'ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo', 'content-length': '0', 'date': 'Tue, 09 Nov 2021 12:30:38 GMT', 'server': 'UploadServer', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"', 'status': '503'}>, content <>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 284, in _execute
    response = task()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 357, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 603, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 640, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py", line 997, in process_bundle
    element.data)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py", line 222, in process_encoded
    self.output(decoded_value)
  File "apache_beam/runners/worker/operations.py", line 351, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 353, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 752, in apache_beam.runners.common.PerWindowInvoker.invoke_process
  File "apache_beam/runners/common.py", line 875, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1315, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "02_bigquery_to_bucket_dataflow.py", line 33, in process
    yield None
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py", line 215, in close
    self._uploader.finish()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 733, in finish
    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 706, in _start_upload
    self._client.objects.Insert(self._insert_request, upload=self._upload)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py", line 1154, in Insert
    upload=upload, upload_config=upload_config)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 731, in _RunMethod
    return self.ProcessHttpResponse(method_config, http_response, request)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 737, in ProcessHttpResponse
    self.__ProcessHttpResponse(method_config, http_response, request))
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 604, in __ProcessHttpResponse
    http_response, method_config=method_config, request=request)
RuntimeError: apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2Fcdf241bb6217b891559f0e87003506458a1d1903a958648cbfa29d1a625ea423.ttl&uploadType=resumable&upload_id=ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo>: response: <{'content-type': 'text/plain; charset=utf-8', 'x-guploader-uploadid': 'ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo', 'content-length': '0', 'date': 'Tue, 09 Nov 2021 12:30:38 GMT', 'server': 'UploadServer', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"', 'status': '503'}>, content <> [while running 'Push to GraphDB']


ERROR:root:Error running pipeline.
Traceback (most recent call last):
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/local_job_service.py", line 274, in _run_job
    self._pipeline_proto)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 206, in run_via_runner_api
    return self.run_stages(stage_context, stages)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 385, in run_stages
    runner_execution_context, bundle_context_manager)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 653, in _run_stage
    bundle_manager))
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 770, in _run_bundle
    data_input, data_output, input_timers, expected_timer_output)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 1120, in process_bundle
    raise RuntimeError(result.error)
RuntimeError: Traceback (most recent call last):
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "02_bigquery_to_bucket_dataflow.py", line 33, in process
    yield None
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py", line 215, in close
    self._uploader.finish()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 733, in finish
    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 706, in _start_upload
    self._client.objects.Insert(self._insert_request, upload=self._upload)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py", line 1154, in Insert
    upload=upload, upload_config=upload_config)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 731, in _RunMethod
    return self.ProcessHttpResponse(method_config, http_response, request)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 737, in ProcessHttpResponse
    self.__ProcessHttpResponse(method_config, http_response, request))
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 604, in __ProcessHttpResponse
    http_response, method_config=method_config, request=request)
apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2Fcdf241bb6217b891559f0e87003506458a1d1903a958648cbfa29d1a625ea423.ttl&uploadType=resumable&upload_id=ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo>: response: <{'content-type': 'text/plain; charset=utf-8', 'x-guploader-uploadid': 'ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo', 'content-length': '0', 'date': 'Tue, 09 Nov 2021 12:30:38 GMT', 'server': 'UploadServer', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"', 'status': '503'}>, content <>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 284, in _execute
    response = task()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 357, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 603, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 640, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py", line 997, in process_bundle
    element.data)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py", line 222, in process_encoded
    self.output(decoded_value)
  File "apache_beam/runners/worker/operations.py", line 351, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 353, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 752, in apache_beam.runners.common.PerWindowInvoker.invoke_process
  File "apache_beam/runners/common.py", line 875, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1315, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "02_bigquery_to_bucket_dataflow.py", line 33, in process
    yield None
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py", line 215, in close
    self._uploader.finish()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 733, in finish
    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 706, in _start_upload
    self._client.objects.Insert(self._insert_request, upload=self._upload)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py", line 1154, in Insert
    upload=upload, upload_config=upload_config)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 731, in _RunMethod
    return self.ProcessHttpResponse(method_config, http_response, request)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 737, in ProcessHttpResponse
    self.__ProcessHttpResponse(method_config, http_response, request))
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 604, in __ProcessHttpResponse
    http_response, method_config=method_config, request=request)
RuntimeError: apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2Fcdf241bb6217b891559f0e87003506458a1d1903a958648cbfa29d1a625ea423.ttl&uploadType=resumable&upload_id=ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo>: response: <{'content-type': 'text/plain; charset=utf-8', 'x-guploader-uploadid': 'ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo', 'content-length': '0', 'date': 'Tue, 09 Nov 2021 12:30:38 GMT', 'server': 'UploadServer', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"', 'status': '503'}>, content <> [while running 'Push to GraphDB']

INFO:apache_beam.runners.portability.portable_runner:Job state changed to FAILED
Traceback (most recent call last):
  File "02_bigquery_to_bucket_dataflow.py", line 419, in <module>
    run()
  File "02_bigquery_to_bucket_dataflow.py", line 403, in run
    | 'Push to GraphDB' >> beam.ParDo(WriteTurtleJSON())
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/pipeline.py", line 587, in __exit__
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/local_job_service.py", line 274, in _run_job
    self._pipeline_proto)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 206, in run_via_runner_api
    return self.run_stages(stage_context, stages)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 385, in run_stages
    runner_execution_context, bundle_context_manager)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 653, in _run_stage
    bundle_manager))
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 770, in _run_bundle
    data_input, data_output, input_timers, expected_timer_output)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 1120, in process_bundle
    raise RuntimeError(result.error)
RuntimeError: Traceback (most recent call last):
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "02_bigquery_to_bucket_dataflow.py", line 33, in process
    yield None
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py", line 215, in close
    self._uploader.finish()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 733, in finish
    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 706, in _start_upload
    self._client.objects.Insert(self._insert_request, upload=self._upload)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py", line 1154, in Insert
    upload=upload, upload_config=upload_config)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 731, in _RunMethod
    return self.ProcessHttpResponse(method_config, http_response, request)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 737, in ProcessHttpResponse
    self.__ProcessHttpResponse(method_config, http_response, request))
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 604, in __ProcessHttpResponse
    http_response, method_config=method_config, request=request)
apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2Fcdf241bb6217b891559f0e87003506458a1d1903a958648cbfa29d1a625ea423.ttl&uploadType=resumable&upload_id=ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo>: response: <{'content-type': 'text/plain; charset=utf-8', 'x-guploader-uploadid': 'ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo', 'content-length': '0', 'date': 'Tue, 09 Nov 2021 12:30:38 GMT', 'server': 'UploadServer', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"', 'status': '503'}>, content <>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 284, in _execute
    response = task()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 357, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 603, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 640, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py", line 997, in process_bundle
    element.data)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py", line 222, in process_encoded
    self.output(decoded_value)
  File "apache_beam/runners/worker/operations.py", line 351, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 353, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 752, in apache_beam.runners.common.PerWindowInvoker.invoke_process
  File "apache_beam/runners/common.py", line 875, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1315, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "02_bigquery_to_bucket_dataflow.py", line 33, in process
    yield None
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py", line 215, in close
    self._uploader.finish()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 733, in finish
    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 706, in _start_upload
    self._client.objects.Insert(self._insert_request, upload=self._upload)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py", line 1154, in Insert
    upload=upload, upload_config=upload_config)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 731, in _RunMethod
    return self.ProcessHttpResponse(method_config, http_response, request)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 737, in ProcessHttpResponse
    self.__ProcessHttpResponse(method_config, http_response, request))
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 604, in __ProcessHttpResponse
    http_response, method_config=method_config, request=request)
RuntimeError: apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2Fcdf241bb6217b891559f0e87003506458a1d1903a958648cbfa29d1a625ea423.ttl&uploadType=resumable&upload_id=ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo>: response: <{'content-type': 'text/plain; charset=utf-8', 'x-guploader-uploadid': 'ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo', 'content-length': '0', 'date': 'Tue, 09 Nov 2021 12:30:38 GMT', 'server': 'UploadServer', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"', 'status': '503'}>, content <> [while running 'Push to GraphDB']


    self.result.wait_until_finish()
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/portable_runner.py", line 600, in wait_until_finish
    raise self._runtime_exception
RuntimeError: Pipeline job-46659e5a-11a9-4336-8158-f487701e8645 failed in state FAILED: Error running pipeline.
Traceback (most recent call last):
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/local_job_service.py", line 274, in _run_job
    self._pipeline_proto)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 206, in run_via_runner_api
    return self.run_stages(stage_context, stages)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 385, in run_stages
    runner_execution_context, bundle_context_manager)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 653, in _run_stage
    bundle_manager))
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 770, in _run_bundle
    data_input, data_output, input_timers, expected_timer_output)
  File "/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py", line 1120, in process_bundle
    raise RuntimeError(result.error)
RuntimeError: Traceback (most recent call last):
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "02_bigquery_to_bucket_dataflow.py", line 33, in process
    yield None
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py", line 215, in close
    self._uploader.finish()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 733, in finish
    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 706, in _start_upload
    self._client.objects.Insert(self._insert_request, upload=self._upload)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py", line 1154, in Insert
    upload=upload, upload_config=upload_config)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 731, in _RunMethod
    return self.ProcessHttpResponse(method_config, http_response, request)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 737, in ProcessHttpResponse
    self.__ProcessHttpResponse(method_config, http_response, request))
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 604, in __ProcessHttpResponse
    http_response, method_config=method_config, request=request)
apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2Fcdf241bb6217b891559f0e87003506458a1d1903a958648cbfa29d1a625ea423.ttl&uploadType=resumable&upload_id=ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo>: response: <{'content-type': 'text/plain; charset=utf-8', 'x-guploader-uploadid': 'ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo', 'content-length': '0', 'date': 'Tue, 09 Nov 2021 12:30:38 GMT', 'server': 'UploadServer', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"', 'status': '503'}>, content <>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 284, in _execute
    response = task()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 357, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 603, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py", line 640, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py", line 997, in process_bundle
    element.data)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py", line 222, in process_encoded
    self.output(decoded_value)
  File "apache_beam/runners/worker/operations.py", line 351, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 353, in apache_beam.runners.worker.operations.Operation.output
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 752, in apache_beam.runners.common.PerWindowInvoker.invoke_process
  File "apache_beam/runners/common.py", line 875, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1299, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "apache_beam/runners/worker/operations.py", line 215, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File "apache_beam/runners/worker/operations.py", line 712, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/worker/operations.py", line 713, in apache_beam.runners.worker.operations.DoOperation.process
  File "apache_beam/runners/common.py", line 1234, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 1315, in apache_beam.runners.common.DoFnRunner._reraise_augmented
  File "apache_beam/runners/common.py", line 1232, in apache_beam.runners.common.DoFnRunner.process
  File "apache_beam/runners/common.py", line 571, in apache_beam.runners.common.SimpleInvoker.invoke_process
  File "apache_beam/runners/common.py", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs
  File "02_bigquery_to_bucket_dataflow.py", line 33, in process
    yield None
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/filesystemio.py", line 215, in close
    self._uploader.finish()
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 733, in finish
    raise self._upload_thread.last_error  # pylint: disable=raising-bad-type
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py", line 706, in _start_upload
    self._client.objects.Insert(self._insert_request, upload=self._upload)
  File "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py", line 1154, in Insert
    upload=upload, upload_config=upload_config)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 731, in _RunMethod
    return self.ProcessHttpResponse(method_config, http_response, request)
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 737, in ProcessHttpResponse
    self.__ProcessHttpResponse(method_config, http_response, request))
  File "/usr/local/lib/python3.7/site-packages/apitools/base/py/base_api.py", line 604, in __ProcessHttpResponse
    http_response, method_config=method_config, request=request)
RuntimeError: apitools.base.py.exceptions.HttpError: HttpError accessing <https://www.googleapis.com/resumable/upload/storage/v1/b/src_fake_news_bs/o?alt=json&name=added_ttl_json_7500%2Fcdf241bb6217b891559f0e87003506458a1d1903a958648cbfa29d1a625ea423.ttl&uploadType=resumable&upload_id=ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo>: response: <{'content-type': 'text/plain; charset=utf-8', 'x-guploader-uploadid': 'ADPycdtR_wXbVINblakYJ-M0HJd2oFoBgbfcewjnlOGjbKpEvjMlLD6ZSeyGBWPWed8Qge4nAwpNI9P_GcdmitOB9yo', 'content-length': '0', 'date': 'Tue, 09 Nov 2021 12:30:38 GMT', 'server': 'UploadServer', 'alt-svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"', 'status': '503'}>, content <> [while running 'Push to GraphDB']

Started at: 2021-11-09 12:30:41.245166
/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1935: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported
  temp_location = pcoll.pipeline.options.view_as(
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fca131f8560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fca131f85f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fca131f8d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-2ceef14b-263f-41f5-8a69-2476c25ca414'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7fca131f4f80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7fca131f80e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fca131f8560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fca131f85f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7fca131f87a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7fca131f8830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7fca131f8950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7fca131f89e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7fca131f8a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7fca131f8b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fca131f8d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7fca131f8cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7fca131f8dd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7fca131f87a0> ====================
INFO:root:==================== <function expand_gbk at 0x7fca131f8830> ====================
INFO:root:==================== <function sink_flattens at 0x7fca131f8950> ====================
INFO:root:==================== <function greedily_fuse at 0x7fca131f89e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7fca131f8a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7fca131f8b00> ====================
INFO:root:==================== <function sort_stages at 0x7fca131f8d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7fca131f8cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7fca131f8dd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 55848
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 55849
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 55850
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 55851
INFO:root:starting control server on port 55848
INFO:root:starting data server on port 55849
INFO:root:starting state server on port 55850
INFO:root:starting logging server on port 55851
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fca13fcb890> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fca13fcb890> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'018718438ba24ea2345dc461e64f8ffe63eaef2b262f3fd45e028606df794d64', worker_id = worker_0
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'018718438ba24ea2345dc461e64f8ffe63eaef2b262f3fd45e028606df794d64', worker_id = worker_0
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636461048
  nanos: 965116500
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461048
  nanos: 974703788
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636461049
  nanos: 196142911
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461049
  nanos: 205987691
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_7500\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461049
  nanos: 216500282
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461049
  nanos: 217696905
}
message: "Creating insecure control channel for host.docker.internal:55848."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461049
  nanos: 227176189
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461049
  nanos: 231487989
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461049
  nanos: 242978096
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461049
  nanos: 259788990
}
message: "Creating insecure state channel for host.docker.internal:55850."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461049
  nanos: 260591030
}
message: "State channel established."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461049
  nanos: 268036127
}
message: "Creating client data channel for host.docker.internal:55849"
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636461049
  nanos: 577026844
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461049
  nanos: 577556133
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461049
  nanos: 638403892
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461049
  nanos: 723222255
}
message: "Refreshing access_token"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461051
  nanos: 1589536
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461051
  nanos: 544856548
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_7500\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_7500`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_7500` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636461051
  nanos: 859699249
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_e2f4fbe3aeb44534ada6ba85b7c93c59 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461053
  nanos: 210749149
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_178b29ed-2_1636461052_636\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_178b29ed-2_1636461052_636"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461053
  nanos: 385535955
}
message: "Job status: RUNNING"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461058
  nanos: 625454425
}
message: "Job status: DONE"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461059
  nanos: 46836853
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_178b29ed-2_1636461058_859\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_178b29ed-2_1636461058_859"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461059
  nanos: 193172931
}
message: "Job status: RUNNING"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461064
  nanos: 344427347
}
message: "Job status: DONE"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461064
  nanos: 357168436
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636461064
  nanos: 530540227
}
message: "Finished listing 1 files in 0.17344379425048828 seconds."
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636463567
  nanos: 18143653
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_5"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463567
  nanos: 155473947
}
message: "Finished listing 1 files in 0.13734054565429688 seconds."
instruction_id: "bundle_5"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463567
  nanos: 406841278
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463567
  nanos: 407516241
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463567
  nanos: 407948255
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463567
  nanos: 408417940
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463567
  nanos: 410507678
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463567
  nanos: 411095142
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

018718438ba24ea2345dc461e64f8ffe63eaef2b262f3fd45e028606df794d64
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 2526.0921869277954 seconds.
INFO:root:Successfully completed job in 2526.0921869277954 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fca131f8560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fca131f85f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fca131f8d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-4b6002a0-3d8b-49df-a231-f198f503fe66'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7fca131f4f80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7fca131f80e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fca131f8560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fca131f85f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7fca131f87a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7fca131f8830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7fca131f8950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7fca131f89e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7fca131f8a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7fca131f8b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fca131f8d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7fca131f8cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7fca131f8dd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7fca131f87a0> ====================
INFO:root:==================== <function expand_gbk at 0x7fca131f8830> ====================
INFO:root:==================== <function sink_flattens at 0x7fca131f8950> ====================
INFO:root:==================== <function greedily_fuse at 0x7fca131f89e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7fca131f8a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7fca131f8b00> ====================
INFO:root:==================== <function sort_stages at 0x7fca131f8d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7fca131f8cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7fca131f8dd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 51726
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 51727
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 51728
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 51729
INFO:root:starting control server on port 51726
INFO:root:starting data server on port 51727
INFO:root:starting state server on port 51728
INFO:root:starting logging server on port 51729
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fca0185af50> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fca0185af50> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'7c1aa6511c7f0f93dc96457ac79542001ae178bc41eb39ca567e3468784b90c6', worker_id = worker_1
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'7c1aa6511c7f0f93dc96457ac79542001ae178bc41eb39ca567e3468784b90c6', worker_id = worker_1
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636463575
  nanos: 337006092
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463575
  nanos: 346859931
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636463575
  nanos: 570553064
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463575
  nanos: 579947471
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_7500\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463575
  nanos: 590539455
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463575
  nanos: 591734409
}
message: "Creating insecure control channel for host.docker.internal:51726."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463575
  nanos: 601508378
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463575
  nanos: 605549097
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463575
  nanos: 616774082
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463575
  nanos: 634404182
}
message: "Creating insecure state channel for host.docker.internal:51728."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463575
  nanos: 635233163
}
message: "State channel established."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463575
  nanos: 643323421
}
message: "Creating client data channel for host.docker.internal:51727"
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636463575
  nanos: 952882528
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463575
  nanos: 953416109
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463576
  nanos: 12460708
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463576
  nanos: 97902059
}
message: "Refreshing access_token"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463576
  nanos: 755583047
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463577
  nanos: 420968532
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_7500\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_7500`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_7500` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636463577
  nanos: 720827102
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_41e7a153c9914c3db5566eadd26d0431 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463579
  nanos: 76158761
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_178b29ed-2_1636463578_187\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_178b29ed-2_1636463578_187"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463579
  nanos: 204811573
}
message: "Job status: RUNNING"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463584
  nanos: 458906650
}
message: "Job status: DONE"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463584
  nanos: 809097051
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_178b29ed-2_1636463584_354\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_178b29ed-2_1636463584_354"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463584
  nanos: 950039148
}
message: "Job status: RUNNING"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463590
  nanos: 101416110
}
message: "Job status: DONE"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463590
  nanos: 113172292
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463590
  nanos: 285378456
}
message: "Finished listing 1 files in 0.1722574234008789 seconds."
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636463600
  nanos: 706715106
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_9"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636463600
  nanos: 840039014
}
message: "Finished listing 1 files in 0.13333773612976074 seconds."
instruction_id: "bundle_9"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:severity: INFO
timestamp {
  seconds: 1636466097
  nanos: 86648941
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466097
  nanos: 87468862
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466097
  nanos: 87928771
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466097
  nanos: 88454484
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466097
  nanos: 115445137
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466097
  nanos: 115960597
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

7c1aa6511c7f0f93dc96457ac79542001ae178bc41eb39ca567e3468784b90c6
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 2529.4710998535156 seconds.
INFO:root:Successfully completed job in 2529.4710998535156 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
Completed at: 2021-11-09 13:54:57.512003
Duration: 1:24:16.266837
END
Started at: 2021-11-09 13:54:59.780051
/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1935: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported
  temp_location = pcoll.pipeline.options.view_as(
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7faf4ad87560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7faf4ad875f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7faf4ad87d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-b2449b64-b69f-4c60-a797-a3f812961cd7'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7faf4ad83f80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7faf4ad870e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7faf4ad87560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7faf4ad875f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7faf4ad877a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7faf4ad87830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7faf4ad87950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7faf4ad879e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7faf4ad87a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7faf4ad87b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7faf4ad87d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7faf4ad87cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7faf4ad87dd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7faf4ad877a0> ====================
INFO:root:==================== <function expand_gbk at 0x7faf4ad87830> ====================
INFO:root:==================== <function sink_flattens at 0x7faf4ad87950> ====================
INFO:root:==================== <function greedily_fuse at 0x7faf4ad879e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7faf4ad87a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7faf4ad87b00> ====================
INFO:root:==================== <function sort_stages at 0x7faf4ad87d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7faf4ad87cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7faf4ad87dd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 63907
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 63908
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 63909
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 63910
INFO:root:starting control server on port 63907
INFO:root:starting data server on port 63908
INFO:root:starting state server on port 63909
INFO:root:starting logging server on port 63910
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7faf3952b1d0> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7faf3952b1d0> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'8d079664b159ffd92dc856b8de2546e469788b429ee6e4f166d524f224d1c542', worker_id = worker_0
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'8d079664b159ffd92dc856b8de2546e469788b429ee6e4f166d524f224d1c542', worker_id = worker_0
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636466107
  nanos: 877758979
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466107
  nanos: 886213541
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636466108
  nanos: 110163450
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466108
  nanos: 119683504
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_10000\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466108
  nanos: 130250692
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466108
  nanos: 131608486
}
message: "Creating insecure control channel for host.docker.internal:63907."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466108
  nanos: 141507863
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466108
  nanos: 146035909
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466108
  nanos: 157943487
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466108
  nanos: 175324440
}
message: "Creating insecure state channel for host.docker.internal:63909."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466108
  nanos: 176131248
}
message: "State channel established."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466108
  nanos: 183833360
}
message: "Creating client data channel for host.docker.internal:63908"
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636466108
  nanos: 493080139
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466108
  nanos: 493638038
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466108
  nanos: 552289009
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466108
  nanos: 637360334
}
message: "Refreshing access_token"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466109
  nanos: 295635938
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466109
  nanos: 984959602
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_10000\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_10000`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_10000` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636466110
  nanos: 362641572
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_1e987d36618b4ee29b9048e39a877121 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466111
  nanos: 925029993
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_c4f89a46-9_1636466111_948\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_c4f89a46-9_1636466111_948"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466112
  nanos: 57112216
}
message: "Job status: RUNNING"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466117
  nanos: 281394004
}
message: "Job status: DONE"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466117
  nanos: 651884317
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_c4f89a46-9_1636466117_363\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_c4f89a46-9_1636466117_363"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466117
  nanos: 741744995
}
message: "Job status: RUNNING"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466122
  nanos: 898693323
}
message: "Job status: DONE"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466122
  nanos: 909821748
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466123
  nanos: 76210021
}
message: "Finished listing 1 files in 0.16643905639648438 seconds."
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636466137
  nanos: 886885404
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_4"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636466138
  nanos: 17305850
}
message: "Finished listing 1 files in 0.13042402267456055 seconds."
instruction_id: "bundle_4"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:severity: INFO
timestamp {
  seconds: 1636469738
  nanos: 706580400
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469738
  nanos: 707233905
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469738
  nanos: 707589387
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469738
  nanos: 707927465
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469738
  nanos: 738905906
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469738
  nanos: 739371776
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

8d079664b159ffd92dc856b8de2546e469788b429ee6e4f166d524f224d1c542
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 3638.885613679886 seconds.
INFO:root:Successfully completed job in 3638.885613679886 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7faf4ad87560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7faf4ad875f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7faf4ad87d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-5ee8cbb1-619a-47d4-a2ee-2069972e11d3'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7faf4ad83f80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7faf4ad870e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7faf4ad87560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7faf4ad875f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7faf4ad877a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7faf4ad87830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7faf4ad87950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7faf4ad879e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7faf4ad87a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7faf4ad87b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7faf4ad87d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7faf4ad87cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7faf4ad87dd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7faf4ad877a0> ====================
INFO:root:==================== <function expand_gbk at 0x7faf4ad87830> ====================
INFO:root:==================== <function sink_flattens at 0x7faf4ad87950> ====================
INFO:root:==================== <function greedily_fuse at 0x7faf4ad879e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7faf4ad87a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7faf4ad87b00> ====================
INFO:root:==================== <function sort_stages at 0x7faf4ad87d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7faf4ad87cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7faf4ad87dd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 64073
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 64074
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 64075
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 64076
INFO:root:starting control server on port 64073
INFO:root:starting data server on port 64074
INFO:root:starting state server on port 64075
INFO:root:starting logging server on port 64076
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7faf395464d0> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7faf395464d0> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'1899df9b66dfae950e2b08324e09636e46bea044378bbd1958ce6bd1ff63335e', worker_id = worker_1
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'1899df9b66dfae950e2b08324e09636e46bea044378bbd1958ce6bd1ff63335e', worker_id = worker_1
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636469746
  nanos: 856414318
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469746
  nanos: 866266250
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636469747
  nanos: 86949110
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469747
  nanos: 96468210
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_10000\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469747
  nanos: 106975793
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469747
  nanos: 108213663
}
message: "Creating insecure control channel for host.docker.internal:64073."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469747
  nanos: 117458105
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469747
  nanos: 121193408
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469747
  nanos: 132787942
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469747
  nanos: 148947715
}
message: "Creating insecure state channel for host.docker.internal:64075."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469747
  nanos: 149756193
}
message: "State channel established."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469747
  nanos: 157065153
}
message: "Creating client data channel for host.docker.internal:64074"
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469747
  nanos: 363075494
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469747
  nanos: 363624334
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469747
  nanos: 428244590
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469747
  nanos: 512981176
}
message: "Refreshing access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469748
  nanos: 204889059
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469748
  nanos: 785223484
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_10000\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_10000`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_10000` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636469749
  nanos: 62037944
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_e06a1c71248642fe9d7ce398589546e0 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469750
  nanos: 815169095
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_c4f89a46-9_1636469750_676\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_c4f89a46-9_1636469750_676"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469750
  nanos: 917625427
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469756
  nanos: 148573637
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469756
  nanos: 468970060
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_c4f89a46-9_1636469756_633\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_c4f89a46-9_1636469756_633"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469756
  nanos: 626765966
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469761
  nanos: 773988246
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469761
  nanos: 786612510
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636469761
  nanos: 959748506
}
message: "Finished listing 1 files in 0.1731867790222168 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636473201
  nanos: 349244117
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_10"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473201
  nanos: 473844051
}
message: "Finished listing 1 files in 0.12460589408874512 seconds."
instruction_id: "bundle_10"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473201
  nanos: 716275453
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473201
  nanos: 716907978
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473201
  nanos: 717332124
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473201
  nanos: 717768907
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473201
  nanos: 721367597
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473201
  nanos: 721950531
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

1899df9b66dfae950e2b08324e09636e46bea044378bbd1958ce6bd1ff63335e
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 3462.746570825577 seconds.
INFO:root:Successfully completed job in 3462.746570825577 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
Completed at: 2021-11-09 15:53:22.126134
Duration: 1:58:22.346083
END
Started at: 2021-11-09 15:53:27.916345
/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1935: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported
  temp_location = pcoll.pipeline.options.view_as(
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7ff03b158560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7ff03b1585f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7ff03b158d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-b363aca6-b972-4dfd-8c05-cf1b9e141f4d'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7ff03b154f80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7ff03b1580e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7ff03b158560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7ff03b1585f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7ff03b1587a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7ff03b158830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7ff03b158950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7ff03b1589e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7ff03b158a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7ff03b158b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7ff03b158d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7ff03b158cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7ff03b158dd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7ff03b1587a0> ====================
INFO:root:==================== <function expand_gbk at 0x7ff03b158830> ====================
INFO:root:==================== <function sink_flattens at 0x7ff03b158950> ====================
INFO:root:==================== <function greedily_fuse at 0x7ff03b1589e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7ff03b158a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7ff03b158b00> ====================
INFO:root:==================== <function sort_stages at 0x7ff03b158d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7ff03b158cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7ff03b158dd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 64085
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 64086
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 64087
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 64088
INFO:root:starting control server on port 64085
INFO:root:starting data server on port 64086
INFO:root:starting state server on port 64087
INFO:root:starting logging server on port 64088
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7ff03ba9b5d0> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7ff03ba9b5d0> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'c6f8c0a001680f5a43f5a85b3a5f2fd184dd6e18fed2ff53e59e384224f866ef', worker_id = worker_0
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'c6f8c0a001680f5a43f5a85b3a5f2fd184dd6e18fed2ff53e59e384224f866ef', worker_id = worker_0
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636473216
  nanos: 828572273
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473216
  nanos: 837700843
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636473217
  nanos: 55766105
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473217
  nanos: 64533472
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_10000\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473217
  nanos: 74170351
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473217
  nanos: 75419425
}
message: "Creating insecure control channel for host.docker.internal:64085."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473217
  nanos: 84625959
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473217
  nanos: 88719129
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473217
  nanos: 99266529
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473217
  nanos: 115645647
}
message: "Creating insecure state channel for host.docker.internal:64087."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473217
  nanos: 116423368
}
message: "State channel established."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473217
  nanos: 123969554
}
message: "Creating client data channel for host.docker.internal:64086"
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473217
  nanos: 323346853
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473217
  nanos: 323888301
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473217
  nanos: 383443355
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473217
  nanos: 468240499
}
message: "Refreshing access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473218
  nanos: 626619100
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473219
  nanos: 295069932
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_10000\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_10000`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_10000` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636473219
  nanos: 578711986
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_4aeed2455f424e8cbec2a8f70728b0db does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473221
  nanos: 134486436
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_2bfc78ee-c_1636473220_476\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_2bfc78ee-c_1636473220_476"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473221
  nanos: 260619878
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473226
  nanos: 480968713
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473226
  nanos: 877891540
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_2bfc78ee-c_1636473226_925\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_2bfc78ee-c_1636473226_925"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473227
  nanos: 4647970
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473232
  nanos: 179741859
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473232
  nanos: 191680908
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473232
  nanos: 369662523
}
message: "Finished listing 1 files in 0.1780388355255127 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636473245
  nanos: 711620807
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_4"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636473245
  nanos: 866111993
}
message: "Finished listing 1 files in 0.1544966697692871 seconds."
instruction_id: "bundle_4"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:severity: INFO
timestamp {
  seconds: 1636476731
  nanos: 48727035
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476731
  nanos: 49508333
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476731
  nanos: 50053596
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476731
  nanos: 50493240
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476731
  nanos: 65525531
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476731
  nanos: 66219806
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

c6f8c0a001680f5a43f5a85b3a5f2fd184dd6e18fed2ff53e59e384224f866ef
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 3521.7534472942352 seconds.
INFO:root:Successfully completed job in 3521.7534472942352 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7ff03b158560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7ff03b1585f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7ff03b158d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-b862bbc5-77b9-4263-8030-51d6b8c17880'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7ff03b154f80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7ff03b1580e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7ff03b158560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7ff03b1585f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7ff03b1587a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7ff03b158830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7ff03b158950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7ff03b1589e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7ff03b158a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7ff03b158b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7ff03b158d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7ff03b158cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7ff03b158dd0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 64185
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 64186
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 64187
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 64188
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function read_to_impulse at 0x7ff03b158a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7ff03b158b00> ====================
INFO:root:==================== <function sort_stages at 0x7ff03b158d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7ff03b158cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7ff03b158dd0> ====================
INFO:root:starting control server on port 64185
INFO:root:starting data server on port 64186
INFO:root:starting state server on port 64187
INFO:root:starting logging server on port 64188
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7ff03bab3f50> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7ff03bab3f50> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'3d837afb71a51b668a5b462ea4053064da406c4f5e69220e7a2b3293f669bd83', worker_id = worker_1
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'3d837afb71a51b668a5b462ea4053064da406c4f5e69220e7a2b3293f669bd83', worker_id = worker_1
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636476738
  nanos: 770213365
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476738
  nanos: 779895305
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636476739
  nanos: 1271247
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476739
  nanos: 10790824
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_10000\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476739
  nanos: 21405696
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476739
  nanos: 22615194
}
message: "Creating insecure control channel for host.docker.internal:64185."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476739
  nanos: 32412052
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476739
  nanos: 35987854
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476739
  nanos: 46944141
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476739
  nanos: 63580513
}
message: "Creating insecure state channel for host.docker.internal:64187."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476739
  nanos: 64358949
}
message: "State channel established."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476739
  nanos: 70893764
}
message: "Creating client data channel for host.docker.internal:64186"
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476739
  nanos: 278998374
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476739
  nanos: 279751777
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476739
  nanos: 340910673
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476739
  nanos: 425265312
}
message: "Refreshing access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476740
  nanos: 712950944
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476741
  nanos: 889151811
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_10000\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_10000`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_10000` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636476742
  nanos: 292824268
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_956b86a1edc04932b01088276be1c585 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476743
  nanos: 643151760
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_2bfc78ee-c_1636476743_276\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_2bfc78ee-c_1636476743_276"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476743
  nanos: 782180070
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476749
  nanos: 12595653
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476749
  nanos: 372510194
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_2bfc78ee-c_1636476749_619\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_2bfc78ee-c_1636476749_619"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476749
  nanos: 504708051
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476754
  nanos: 671154737
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476754
  nanos: 682663202
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476754
  nanos: 856514453
}
message: "Finished listing 1 files in 0.17390179634094238 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636476768
  nanos: 564353227
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_9"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636476768
  nanos: 717698812
}
message: "Finished listing 1 files in 0.1533524990081787 seconds."
instruction_id: "bundle_9"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:severity: INFO
timestamp {
  seconds: 1636480297
  nanos: 991297483
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480297
  nanos: 992043733
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480297
  nanos: 992525100
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480297
  nanos: 992873907
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480298
  nanos: 20884513
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480298
  nanos: 21442890
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

3d837afb71a51b668a5b462ea4053064da406c4f5e69220e7a2b3293f669bd83
Completed at: 2021-11-09 17:51:38.607565
Duration: 1:58:10.691220
END
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 3567.132558107376 seconds.
INFO:root:Successfully completed job in 3567.132558107376 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
Started at: 2021-11-09 17:51:40.962069
/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1935: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported
  temp_location = pcoll.pipeline.options.view_as(
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fa192bae560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fa192bae5f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fa192baed40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-72c19b9f-6fa9-458f-82c7-671b46146648'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7fa192babf80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7fa192bae0e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fa192bae560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fa192bae5f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7fa192bae7a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7fa192bae830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7fa192bae950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7fa192bae9e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7fa192baea70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7fa192baeb00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fa192baed40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7fa192baecb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7fa192baedd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7fa192bae7a0> ====================
INFO:root:==================== <function expand_gbk at 0x7fa192bae830> ====================
INFO:root:==================== <function sink_flattens at 0x7fa192bae950> ====================
INFO:root:==================== <function greedily_fuse at 0x7fa192bae9e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7fa192baea70> ====================
INFO:root:==================== <function impulse_to_input at 0x7fa192baeb00> ====================
INFO:root:==================== <function sort_stages at 0x7fa192baed40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7fa192baecb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7fa192baedd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 64371
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 64372
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 64373
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 64374
INFO:root:starting control server on port 64371
INFO:root:starting data server on port 64372
INFO:root:starting state server on port 64373
INFO:root:starting logging server on port 64374
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fa1815fbed0> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fa1815fbed0> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'636e8fc33cc3a2feb2196b3af688ef0a2d34cad464cc7a735015872073277b7c', worker_id = worker_0
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'636e8fc33cc3a2feb2196b3af688ef0a2d34cad464cc7a735015872073277b7c', worker_id = worker_0
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636480308
  nanos: 959152698
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480308
  nanos: 968916416
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636480309
  nanos: 187245130
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480309
  nanos: 196878433
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_10000\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480309
  nanos: 207119941
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480309
  nanos: 208263635
}
message: "Creating insecure control channel for host.docker.internal:64371."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480309
  nanos: 217159748
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480309
  nanos: 220658063
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480309
  nanos: 232076644
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480309
  nanos: 248016119
}
message: "Creating insecure state channel for host.docker.internal:64373."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480309
  nanos: 248799324
}
message: "State channel established."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480309
  nanos: 256421327
}
message: "Creating client data channel for host.docker.internal:64372"
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636480309
  nanos: 558784246
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480309
  nanos: 559287786
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480309
  nanos: 616621971
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480309
  nanos: 701101779
}
message: "Refreshing access_token"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480310
  nanos: 825426101
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480311
  nanos: 519540548
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_10000\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_10000`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_10000` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636480311
  nanos: 907524585
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_16573236f6134a5f8075cae2df091ab8 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480313
  nanos: 667171478
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_bbee8acc-0_1636480313_365\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_bbee8acc-0_1636480313_365"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480313
  nanos: 797940969
}
message: "Job status: RUNNING"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480319
  nanos: 20064115
}
message: "Job status: DONE"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480319
  nanos: 396763563
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_bbee8acc-0_1636480319_823\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_bbee8acc-0_1636480319_823"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480319
  nanos: 501404762
}
message: "Job status: RUNNING"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480324
  nanos: 686995983
}
message: "Job status: DONE"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480324
  nanos: 691633462
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480324
  nanos: 848016977
}
message: "Finished listing 1 files in 0.15638494491577148 seconds."
instruction_id: "bundle_2"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636480336
  nanos: 972442626
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_4"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636480337
  nanos: 102189540
}
message: "Finished listing 1 files in 0.1297600269317627 seconds."
instruction_id: "bundle_4"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:severity: INFO
timestamp {
  seconds: 1636483919
  nanos: 317504167
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483919
  nanos: 318420171
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483919
  nanos: 318797588
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483919
  nanos: 319093704
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483919
  nanos: 362221479
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483919
  nanos: 362796783
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

636e8fc33cc3a2feb2196b3af688ef0a2d34cad464cc7a735015872073277b7c
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 3618.5064041614532 seconds.
INFO:root:Successfully completed job in 3618.5064041614532 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fa192bae560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fa192bae5f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fa192baed40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-5dc06060-4972-4a44-87e3-aa33e13bb78c'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7fa192babf80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7fa192bae0e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fa192bae560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fa192bae5f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7fa192bae7a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7fa192bae830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7fa192bae950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7fa192bae9e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7fa192baea70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7fa192baeb00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fa192baed40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7fa192baecb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7fa192baedd0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 64970
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 64971
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 64972
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 64973
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function read_to_impulse at 0x7fa192baea70> ====================
INFO:root:==================== <function impulse_to_input at 0x7fa192baeb00> ====================
INFO:root:==================== <function sort_stages at 0x7fa192baed40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7fa192baecb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7fa192baedd0> ====================
INFO:root:starting control server on port 64970
INFO:root:starting data server on port 64971
INFO:root:starting state server on port 64972
INFO:root:starting logging server on port 64973
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fa1819f6ed0> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7fa1819f6ed0> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'2da40bb46e704ce340e4beb1f71db8845c474580a52e75da65d78526d23917fe', worker_id = worker_1
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'2da40bb46e704ce340e4beb1f71db8845c474580a52e75da65d78526d23917fe', worker_id = worker_1
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636483927
  nanos: 630635499
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483927
  nanos: 641239404
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636483927
  nanos: 865172624
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483927
  nanos: 874696016
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_10000\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483927
  nanos: 885260820
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483927
  nanos: 886475324
}
message: "Creating insecure control channel for host.docker.internal:64970."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483927
  nanos: 895519495
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483927
  nanos: 899356365
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483927
  nanos: 910920858
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483927
  nanos: 929291486
}
message: "Creating insecure state channel for host.docker.internal:64972."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483927
  nanos: 930102348
}
message: "State channel established."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483927
  nanos: 937601566
}
message: "Creating client data channel for host.docker.internal:64971"
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483928
  nanos: 144789218
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483928
  nanos: 145408868
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483928
  nanos: 208229541
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483928
  nanos: 293510913
}
message: "Refreshing access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483929
  nanos: 578260660
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483930
  nanos: 659333944
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_10000\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_10000`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_10000` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636483930
  nanos: 995591402
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_2bcf756388b442cb82e50c2deeac791f does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483932
  nanos: 413208484
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_bbee8acc-0_1636483931_142\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_bbee8acc-0_1636483931_142"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483932
  nanos: 516584157
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483937
  nanos: 727955341
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483938
  nanos: 145538330
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_bbee8acc-0_1636483937_261\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_bbee8acc-0_1636483937_261"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483938
  nanos: 289922714
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483943
  nanos: 425673961
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483943
  nanos: 437154054
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636483943
  nanos: 609084606
}
message: "Finished listing 1 files in 0.1719820499420166 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:severity: INFO
timestamp {
  seconds: 1636487528
  nanos: 877084016
}
message: "Refreshing due to a 401 (attempt 1/2)"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:185"
thread: "Thread-9902"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487528
  nanos: 957000970
}
message: "Refreshing access_token"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-9902"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636487536
  nanos: 296994924
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_10"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487536
  nanos: 435072183
}
message: "Finished listing 1 files in 0.13808560371398926 seconds."
instruction_id: "bundle_10"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487536
  nanos: 726820945
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487536
  nanos: 727524757
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487536
  nanos: 728003263
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487536
  nanos: 728545188
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487536
  nanos: 740955114
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487536
  nanos: 741600036
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

2da40bb46e704ce340e4beb1f71db8845c474580a52e75da65d78526d23917fe
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 3617.245945930481 seconds.
INFO:root:Successfully completed job in 3617.245945930481 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
Completed at: 2021-11-09 19:52:17.450287
Duration: 2:00:36.488218
END
Started at: 2021-11-09 19:52:19.714825
/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1935: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported
  temp_location = pcoll.pipeline.options.view_as(
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7ff59a7e7560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7ff59a7e75f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7ff59a7e7d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-a2c1182d-d3f9-4ce4-8d27-2b755a83fb14'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7ff59a7e3f80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7ff59a7e70e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7ff59a7e7560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7ff59a7e75f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7ff59a7e77a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7ff59a7e7830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7ff59a7e7950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7ff59a7e79e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7ff59a7e7a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7ff59a7e7b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7ff59a7e7d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7ff59a7e7cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7ff59a7e7dd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7ff59a7e77a0> ====================
INFO:root:==================== <function expand_gbk at 0x7ff59a7e7830> ====================
INFO:root:==================== <function sink_flattens at 0x7ff59a7e7950> ====================
INFO:root:==================== <function greedily_fuse at 0x7ff59a7e79e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7ff59a7e7a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7ff59a7e7b00> ====================
INFO:root:==================== <function sort_stages at 0x7ff59a7e7d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7ff59a7e7cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7ff59a7e7dd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 49180
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 49181
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 49182
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 49183
INFO:root:starting control server on port 49180
INFO:root:starting data server on port 49181
INFO:root:starting state server on port 49182
INFO:root:starting logging server on port 49183
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7ff589bcae10> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7ff589bcae10> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'43add0e1f5bd0494549ff6c411e506ba02e229e6c50e93a022ef4ab1df075688', worker_id = worker_0
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'43add0e1f5bd0494549ff6c411e506ba02e229e6c50e93a022ef4ab1df075688', worker_id = worker_0
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636487547
  nanos: 458418607
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487547
  nanos: 468255043
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636487547
  nanos: 690412521
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487547
  nanos: 699414968
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_10000\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487547
  nanos: 709137201
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487547
  nanos: 710264921
}
message: "Creating insecure control channel for host.docker.internal:49180."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487547
  nanos: 719395875
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487547
  nanos: 723244190
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487547
  nanos: 735607147
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487547
  nanos: 753539085
}
message: "Creating insecure state channel for host.docker.internal:49182."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487547
  nanos: 754332065
}
message: "State channel established."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487547
  nanos: 761653184
}
message: "Creating client data channel for host.docker.internal:49181"
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487547
  nanos: 969606399
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487547
  nanos: 970197677
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487548
  nanos: 31831026
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487548
  nanos: 118770837
}
message: "Refreshing access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487549
  nanos: 285263299
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487549
  nanos: 812773704
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_10000\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_10000`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_10000` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636487550
  nanos: 158858299
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_d5a6d02625924d6cb8a9fffe90552f94 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487551
  nanos: 824693918
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_dcdeb4a3-f_1636487551_251\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_dcdeb4a3-f_1636487551_251"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487551
  nanos: 962194681
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487557
  nanos: 150981426
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487557
  nanos: 385392904
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_dcdeb4a3-f_1636487557_567\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_dcdeb4a3-f_1636487557_567"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487557
  nanos: 533004522
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487562
  nanos: 680337905
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487562
  nanos: 690915822
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487562
  nanos: 861400365
}
message: "Finished listing 1 files in 0.17052912712097168 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636487577
  nanos: 152059555
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_4"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636487577
  nanos: 357000112
}
message: "Finished listing 1 files in 0.20494413375854492 seconds."
instruction_id: "bundle_4"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:severity: INFO
timestamp {
  seconds: 1636491146
  nanos: 896994352
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491146
  nanos: 898735046
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491146
  nanos: 899413585
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491146
  nanos: 900001287
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491146
  nanos: 903371334
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491146
  nanos: 904204368
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

43add0e1f5bd0494549ff6c411e506ba02e229e6c50e93a022ef4ab1df075688
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 3607.3877260684967 seconds.
INFO:root:Successfully completed job in 3607.3877260684967 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7ff59a7e7560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7ff59a7e75f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7ff59a7e7d40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-f5d6b6c4-80fa-470f-853c-57e72d0a5796'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7ff59a7e3f80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7ff59a7e70e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7ff59a7e7560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7ff59a7e75f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7ff59a7e77a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7ff59a7e7830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7ff59a7e7950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7ff59a7e79e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7ff59a7e7a70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7ff59a7e7b00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7ff59a7e7d40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7ff59a7e7cb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7ff59a7e7dd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7ff59a7e77a0> ====================
INFO:root:==================== <function expand_gbk at 0x7ff59a7e7830> ====================
INFO:root:==================== <function sink_flattens at 0x7ff59a7e7950> ====================
INFO:root:==================== <function greedily_fuse at 0x7ff59a7e79e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7ff59a7e7a70> ====================
INFO:root:==================== <function impulse_to_input at 0x7ff59a7e7b00> ====================
INFO:root:==================== <function sort_stages at 0x7ff59a7e7d40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7ff59a7e7cb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7ff59a7e7dd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 50183
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 50184
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 50185
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 50186
INFO:root:starting control server on port 50183
INFO:root:starting data server on port 50184
INFO:root:starting state server on port 50185
INFO:root:starting logging server on port 50186
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7ff589c92bd0> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7ff589c92bd0> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'26331cb6ea8c139e08322727845ff45f1aae3fabad0bdc30bba4c7c1f3d3991f', worker_id = worker_1
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'26331cb6ea8c139e08322727845ff45f1aae3fabad0bdc30bba4c7c1f3d3991f', worker_id = worker_1
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636491155
  nanos: 317512512
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491155
  nanos: 328634262
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636491155
  nanos: 567330121
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491155
  nanos: 577021837
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_10000\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491155
  nanos: 587777614
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491155
  nanos: 588991403
}
message: "Creating insecure control channel for host.docker.internal:50183."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491155
  nanos: 599538087
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491155
  nanos: 604117393
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491155
  nanos: 615370988
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491155
  nanos: 632942914
}
message: "Creating insecure state channel for host.docker.internal:50185."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491155
  nanos: 633811950
}
message: "State channel established."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491155
  nanos: 642138004
}
message: "Creating client data channel for host.docker.internal:50184"
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636491155
  nanos: 961768627
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491155
  nanos: 962340831
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491156
  nanos: 25842905
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491156
  nanos: 113894462
}
message: "Refreshing access_token"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491157
  nanos: 380980253
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491158
  nanos: 572648048
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_10000\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_10000`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_10000` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636491158
  nanos: 885169267
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_d20268b88c0f42e6bc44608b482e21f6 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491160
  nanos: 357295989
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_dcdeb4a3-f_1636491159_113\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_dcdeb4a3-f_1636491159_113"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491160
  nanos: 502346277
}
message: "Job status: RUNNING"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491165
  nanos: 726171255
}
message: "Job status: DONE"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491165
  nanos: 949036836
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_dcdeb4a3-f_1636491165_956\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_dcdeb4a3-f_1636491165_956"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491166
  nanos: 40748119
}
message: "Job status: RUNNING"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491171
  nanos: 215363502
}
message: "Job status: DONE"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491171
  nanos: 228723287
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491171
  nanos: 415316820
}
message: "Finished listing 1 files in 0.18665027618408203 seconds."
instruction_id: "bundle_7"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636491187
  nanos: 291800737
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_9"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636491187
  nanos: 437000274
}
message: "Finished listing 1 files in 0.14520835876464844 seconds."
instruction_id: "bundle_9"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:severity: INFO
timestamp {
  seconds: 1636494679
  nanos: 409443855
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494679
  nanos: 410653591
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494679
  nanos: 411308765
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494679
  nanos: 411895990
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494679
  nanos: 487173318
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494679
  nanos: 487655878
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

26331cb6ea8c139e08322727845ff45f1aae3fabad0bdc30bba4c7c1f3d3991f
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 3532.2949018478394 seconds.
INFO:root:Successfully completed job in 3532.2949018478394 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
Completed at: 2021-11-09 21:51:20.154902
Duration: 1:59:00.440077
END
Started at: 2021-11-09 21:51:22.766951
/Users/aaronaltrock/PycharmProjects/detect_fake_news_data_flow_bs/venv/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1935: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported
  temp_location = pcoll.pipeline.options.view_as(
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f8a6119e560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f8a6119e5f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f8a6119ed40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-1b91e668-d5a4-4cd9-b5ae-d50efa85fc94'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f8a6119bf80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f8a6119e0e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f8a6119e560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f8a6119e5f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f8a6119e7a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f8a6119e830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f8a6119e950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f8a6119e9e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f8a6119ea70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f8a6119eb00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f8a6119ed40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f8a6119ecb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f8a6119edd0> ====================
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function expand_sdf at 0x7f8a6119e7a0> ====================
INFO:root:==================== <function expand_gbk at 0x7f8a6119e830> ====================
INFO:root:==================== <function sink_flattens at 0x7f8a6119e950> ====================
INFO:root:==================== <function greedily_fuse at 0x7f8a6119e9e0> ====================
INFO:root:==================== <function read_to_impulse at 0x7f8a6119ea70> ====================
INFO:root:==================== <function impulse_to_input at 0x7f8a6119eb00> ====================
INFO:root:==================== <function sort_stages at 0x7f8a6119ed40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7f8a6119ecb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7f8a6119edd0> ====================
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 50753
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 50754
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 50755
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 50756
INFO:root:starting control server on port 50753
INFO:root:starting data server on port 50754
INFO:root:starting state server on port 50755
INFO:root:starting logging server on port 50756
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7f8a61564e90> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7f8a61564e90> for environment ref_Environment_default_environment_1 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'6dcf6d2f1aa2adb86ad217ad429ec9604bbe5fbc7d9d6994e2d62b3e27d9a860', worker_id = worker_0
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'6dcf6d2f1aa2adb86ad217ad429ec9604bbe5fbc7d9d6994e2d62b3e27d9a860', worker_id = worker_0
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636494690
  nanos: 881481170
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494690
  nanos: 891200065
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636494691
  nanos: 112920045
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494691
  nanos: 122534036
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_10000\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494691
  nanos: 133068799
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494691
  nanos: 134208202
}
message: "Creating insecure control channel for host.docker.internal:50753."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494691
  nanos: 143332958
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494691
  nanos: 146834373
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494691
  nanos: 157292127
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494691
  nanos: 172578334
}
message: "Creating insecure state channel for host.docker.internal:50755."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494691
  nanos: 173343896
}
message: "State channel established."
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494691
  nanos: 181127309
}
message: "Creating client data channel for host.docker.internal:50754"
instruction_id: "bundle_1"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494691
  nanos: 385722160
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494691
  nanos: 386264801
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494691
  nanos: 468831300
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494691
  nanos: 553604841
}
message: "Refreshing access_token"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494692
  nanos: 775761127
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494693
  nanos: 268654584
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_10000\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_10000`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_10000` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636494693
  nanos: 641893863
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_356de59be2934db9a051458d58367b8e does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494694
  nanos: 996229171
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_956e0aa5-f_1636494694_420\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_956e0aa5-f_1636494694_420"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494695
  nanos: 122036933
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494700
  nanos: 357110023
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494700
  nanos: 613743782
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_956e0aa5-f_1636494700_496\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_956e0aa5-f_1636494700_496"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494700
  nanos: 702662944
}
message: "Job status: RUNNING"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494705
  nanos: 898760557
}
message: "Job status: DONE"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494705
  nanos: 910364866
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494706
  nanos: 75305223
}
message: "Finished listing 1 files in 0.16501688957214355 seconds."
instruction_id: "bundle_1"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636494720
  nanos: 287349224
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_4"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636494720
  nanos: 463906049
}
message: "Finished listing 1 files in 0.17653703689575195 seconds."
instruction_id: "bundle_4"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:severity: INFO
timestamp {
  seconds: 1636498133
  nanos: 478657960
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498133
  nanos: 479772329
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498133
  nanos: 480294227
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498133
  nanos: 480665683
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498133
  nanos: 482636451
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498133
  nanos: 483080625
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

6dcf6d2f1aa2adb86ad217ad429ec9604bbe5fbc7d9d6994e2d62b3e27d9a860
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 3450.998395204544 seconds.
INFO:root:Successfully completed job in 3450.998395204544 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:root:Using provided Python SDK container image: gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Python SDK container image set to "gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest" for Docker environment
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f8a6119e560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f8a6119e5f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f8a6119ed40> ====================
WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--source_bucket_name=src_fake_news_bs']
INFO:apache_beam.runners.portability.abstract_job_service:Running job 'job-38db1309-5ec5-471d-9117-5612b0c0afa3'
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.33.0
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f8a6119bf80> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f8a6119e0e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f8a6119e560> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f8a6119e5f0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f8a6119e7a0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f8a6119e830> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f8a6119e950> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f8a6119e9e0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f8a6119ea70> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f8a6119eb00> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f8a6119ed40> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f8a6119ecb0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f8a6119edd0> ====================
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting control server on port 51209
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting data server on port 51210
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting state server on port 51211
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:starting logging server on port 51212
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.
INFO:root:==================== <function read_to_impulse at 0x7f8a6119ea70> ====================
INFO:root:==================== <function impulse_to_input at 0x7f8a6119eb00> ====================
INFO:root:==================== <function sort_stages at 0x7f8a6119ed40> ====================
INFO:root:==================== <function setup_timer_mapping at 0x7f8a6119ecb0> ====================
INFO:root:==================== <function populate_data_channel_coders at 0x7f8a6119edd0> ====================
INFO:root:starting control server on port 51209
INFO:root:starting data server on port 51210
INFO:root:starting state server on port 51211
INFO:root:starting logging server on port 51212
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STOPPED
INFO:apache_beam.runners.portability.portable_runner:Job state changed to STARTING
INFO:apache_beam.runners.portability.portable_runner:Job state changed to RUNNING
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7f8a5026d410> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
INFO:root:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.DockerSdkWorkerHandler object at 0x7f8a5026d410> for environment ref_Environment_default_environment_2 (beam:env:docker:v1, b'\nngcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest')
INFO:root:Attempting to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest
Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
INFO:root:Unable to pull image gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest, defaulting to local image if it exists
WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Waiting for docker to start up. Current status is running
INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Docker container is running. container_id = b'edff7944f3eeff60a712cfa94349d1da962330c0205933926c8e06f6f0d64f22', worker_id = worker_1
INFO:root:Waiting for docker to start up. Current status is running
INFO:root:Docker container is running. container_id = b'edff7944f3eeff60a712cfa94349d1da962330c0205933926c8e06f6f0d64f22', worker_id = worker_1
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-Read-Impulse_10)+(ref_AppliedPTransform_Read-from-BigQuery-Read-Map-lambda-at-iobase-py-898-_11))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)
INFO:root:severity: INFO
timestamp {
  seconds: 1636498141
  nanos: 874706029
}
message: "Logging handler created."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:64"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498141
  nanos: 889188528
}
message: "semi_persistent_directory: /tmp"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:85"
thread: "MainThread"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636498142
  nanos: 108794212
}
message: "Discarding unparseable args: [\'--direct_runner_use_stacked_bundle\', \'--job_server_timeout=60\', \'--pipeline_type_check\']"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/options/pipeline_options.py:309"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498142
  nanos: 118596076
}
message: "Pipeline_options: {\'project\': \'fake-news-bs-detector\', \'staging_location\': \'gs://src_fake_news_bs/staging_02/\', \'temp_location\': \'gs://src_fake_news_bs/tmp/\', \'experiments\': [\'beam_fn_api\'], \'save_main_session\': True, \'sdk_location\': \'container\', \'job_endpoint\': \'embed\', \'environment_type\': \'DOCKER\', \'environment_config\': \'gcr.io/fake-news-bs-detector/europe-west2-docker.pkg.dev/fake-news-bs-detector/dataflow-docker-stage-02:latest\', \'sdk_worker_parallelism\': \'1\', \'environment_cache_millis\': \'0\', \'output_executable_path\': \'gs://src_fake_news_bs/added_ttl_json_10000\'}"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:102"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498142
  nanos: 129429101
}
message: "Creating state cache with size 0"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/statecache.py:172"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498142
  nanos: 130669832
}
message: "Creating insecure control channel for host.docker.internal:51209."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:181"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498142
  nanos: 140123605
}
message: "Control channel established."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:189"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498142
  nanos: 143857479
}
message: "Initializing SDKHarness with unbounded number of workers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:232"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498142
  nanos: 154967784
}
message: "Python sdk harness starting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:152"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498142
  nanos: 171951055
}
message: "Creating insecure state channel for host.docker.internal:51211."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:888"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498142
  nanos: 172782897
}
message: "State channel established."
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:895"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498142
  nanos: 180399894
}
message: "Creating client data channel for host.docker.internal:51210"
instruction_id: "bundle_6"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:758"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498142
  nanos: 386861085
}
message: "Setting socket default timeout to 60 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:106"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498142
  nanos: 387435674
}
message: "socket default timeout is 60.0 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/internal/gcp/auth.py:109"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498142
  nanos: 450797319
}
message: "Attempting refresh to obtain initial access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/transport.py:157"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498142
  nanos: 536363840
}
message: "Refreshing access_token"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/oauth2client/client.py:777"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498143
  nanos: 172320365
}
message: "Started BigQuery job: <JobReference\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector None"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498144
  nanos: 259738922
}
message: "Using location \'europe-west2\' from table <TableReference\n datasetId: \'fake_news\'\n projectId: \'fake-news-bs-detector\'\n tableId: \'src_fake_news_10000\'> referenced by query \n        WITH LATEST_TIMESTAMP_CTE AS (\n            SELECT  \n            article_id\n            ,url\n            ,MAX(etl_timestamp) AS etl_timestamp\n            FROM `fake-news-bs-detector.fake_news.src_fake_news_10000`\n            GROUP BY article_id, url\n        )\n        SELECT \n        orig.*\n        FROM LATEST_TIMESTAMP_CTE AS latest\n        INNER JOIN `fake-news-bs-detector.fake_news.src_fake_news_10000` AS orig\n        ON orig.article_id = latest.article_id\n        AND orig.url = latest.url\n        AND orig.etl_timestamp = latest.etl_timestamp;\n\n    "
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:397"
thread: "Thread-14"

WARNING:root:severity: WARN
timestamp {
  seconds: 1636498144
  nanos: 549115896
}
message: "Dataset fake-news-bs-detector:beam_temp_dataset_cd47f777d07d44268fbfe90bcb9455c0 does not exist so we will create it as temporary with location=europe-west2"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:836"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498145
  nanos: 990131855
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_QUERY_BQ_EXPORT_JOB_956e0aa5-f_1636498145_810\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_QUERY_BQ_EXPORT_JOB_956e0aa5-f_1636498145_810"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498146
  nanos: 144118547
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498151
  nanos: 342598438
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498151
  nanos: 646310091
}
message: "Started BigQuery job: <JobReference\n jobId: \'beam_bq_job_EXPORT_BQ_EXPORT_JOB_956e0aa5-f_1636498151_276\'\n location: \'europe-west2\'\n projectId: \'fake-news-bs-detector\'>\n bq show -j --format=prettyjson --project_id=fake-news-bs-detector beam_bq_job_EXPORT_BQ_EXPORT_JOB_956e0aa5-f_1636498151_276"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:515"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498151
  nanos: 802758455
}
message: "Job status: RUNNING"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498156
  nanos: 985886812
}
message: "Job status: DONE"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_tools.py:581"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498156
  nanos: 997018575
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636498157
  nanos: 168815374
}
message: "Finished listing 1 files in 0.17184662818908691 seconds."
instruction_id: "bundle_6"
transform_id: "Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:root:Running ((((((ref_PCollection_PCollection_6_split/Read)+(Read from BigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Shuffle-AddRandomKeys_24))+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-Map-reify_timestamps-_26))+(Shuffle/ReshufflePerKey/GroupByKey/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:root:Running (((((Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_28))+(ref_AppliedPTransform_Shuffle-RemoveRandomKeys_29))+(ref_AppliedPTransform_Make-RDFLib-graph_30))+(ref_AppliedPTransform_Serialise-RDFLib-graph_31))+(ref_AppliedPTransform_Push-to-GraphDB_32)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:root:Running ((((ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-2968-_5))+(ref_AppliedPTransform_Read-from-BigQuery-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-from-BigQuery-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)
INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:Running (((ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-29_19))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-from-BigQuery-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)
INFO:root:severity: INFO
timestamp {
  seconds: 1636501624
  nanos: 362748384
}
message: "Starting the size estimation of the input"
instruction_id: "bundle_10"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:557"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636501624
  nanos: 516318798
}
message: "Finished listing 1 files in 0.15359234809875488 seconds."
instruction_id: "bundle_10"
transform_id: "Read from BigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/io/gcp/gcsio.py:573"
thread: "Thread-14"

INFO:root:severity: INFO
timestamp {
  seconds: 1636501624
  nanos: 777378320
}
message: "No more requests from control plane"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:261"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636501624
  nanos: 778127908
}
message: "SDK Harness waiting for in-flight requests to complete"
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:262"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636501624
  nanos: 778679370
}
message: "Closing all cached grpc data channels."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:790"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636501624
  nanos: 779146671
}
message: "Closing all cached gRPC state handlers."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:907"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636501624
  nanos: 783769845
}
message: "Done consuming work."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:274"
thread: "MainThread"

INFO:root:severity: INFO
timestamp {
  seconds: 1636501624
  nanos: 784345626
}
message: "Python sdk harness exiting."
log_location: "/usr/local/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker_main.py:154"
thread: "MainThread"

edff7944f3eeff60a712cfa94349d1da962330c0205933926c8e06f6f0d64f22
Completed at: 2021-11-09 23:47:05.507848
Duration: 1:55:42.740897
END
INFO:apache_beam.runners.portability.local_job_service:Successfully completed job in 3491.0073120594025 seconds.
INFO:root:Successfully completed job in 3491.0073120594025 seconds.
INFO:apache_beam.runners.portability.portable_runner:Job state changed to DONE
END OF BENCHMARKING

Process finished with exit code 0
